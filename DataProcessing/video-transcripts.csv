video_id,datetime,title,transcript
SXwC9fSwct8,2024-01-11T15:10:59Z,Ultimate Docker Compose Tutorial,"in this video you will learn everything you need to know to get started with using Docker compose we'll go over what it is exactly what problems Docker compose was designed to solve its common use cases and of course we will do some Hands-On demos to learn actually using Docker compose in practice I am super excited to teach you all this so let's jump into it now in order to understand Docker compos you need to First understand docker and have some basic experience with it if you don't I recommend you pause and watch my Docker crash course first and then continue with this one because Docker compost is essentially a tool that is supposed to manage and work with Docker containers so you need to understand that part first so that you understand the context for learning Docker compost so in the docker video I break down what the containers are what images are what problems Docker solves and what use case it it has dockerizing your application with Docker file and all the concepts you need to understand Docker itself so based on that knowledge we can Now understand why Docker compos was created along with Docker and when we want to use it now applications are composed of many different parts you can have apis databases any Services your application depends on and even within the application you may have a microservice application which is basically an application broken down into multip micro applications or microservices and when you're creating containerized applications all of these different application components must be deployed and run together because they have dependencies on each other so basically you have a set of containers which are running different services and applications within them that need to run together that need to talk to each other and so on so Docker compose is basically a tool that allows you to Define and run multiple services and applications that belong together in one environment so simply put if you want to deploy multiple Docker containers where each container may have its different configuration options you can use Docker compose to do this to manage these containers way more easily now this is just a general definition to give you some idea of what Docker compose is but of course we want to understand this with specific examples and specific demonstration so that you really understand these Concepts and the actual use cases of using doer compose and not just a general abstract explanation of what it is and because of that we're going to jump right into that demo where I'm going to explain the concepts the use cases using those demonstrations so let's get started as a first step we're going to start two Services as Docker containers using just the docker command so we're not going to use Docker compos as a first step so we can see and compare the before after States first we're going to create a Docker Network where these two containers will run and talk to each other using just the container name and then we're going to start two containers one is going to be a mongodb container and another one is going to be Express container which is basically a UI for the mongodb database very simple use case and we're going to run both containers using Docker run commands so that's our first very simple demonstration let's go ahead and do that so I'm going to switch to a terminal because we're going to execute those docket run commands on the terminal and you probably see this is a fancy fun looking terminal that I have been using since recently and this is an application or a terminal app called warp which is actually a sponsor of this video I actually played around with warp and love using it it's free it's easy to install on your computer so I will be using warp throughout the entire day demo because it also helps highlight some of the commands and stuff better so it's going to be easier for you guys to follow what I'm showing you however if you want to install warp yourself on your computer you can go ahead and check out the link to get started in the video description where I'm going to provide all the relevant links for this crash course including the warp installation so to run our Docker containers of course we need to have Docker installed and running so I'm going to start up Docker and then we can start the containers so the docker service is up and running let's go ahead and create the docker Network first so I'm going to do Docker Network and since we're going to run mongodb and Express containers we can call this network  Network and let's create and now if I do Docker Network LS so basically list all the networks available these are the default ones basically that you get out of the box when you install Docker and this is the Network that we just created awesome so the network is there now let's run our two containers and if you know Docker if you followed my Docker crash course basically you know all this stuff Docker run and we're going to execute this in the background and we have the and Express image documentation so we can actually reference this so first I'm going to define the port uh mongodb's default Port is $27 07 so we're going to map that to the same port so we we're going to bind that to the same port on the host then we're going to define those two environment variables to basically set the admin or the root user and password so we're going to copy those and we're going to call this admin and this is some password we're going to set this to super secret so all these should be actually be a refresher from Docker we also want to specify that it should run in this network network so we're going to do Network run in this one we're also going to name our container instead of having Docker just create a random container name so we're going to call this DB and finally we need to specify the image right and this is the name of the image and that's basically our Docker command so I'm going to execute and this will fetch or pull the latest image from dockerhub repository and run it in a detached mode perfect so we should have have our mongodb container running and now let's start Express container and I can actually bring up my previous command and we're going to adjust it for the  Express right here we see that  Express is running on port 8080 so that's what we're going to set here there you go we also have different environment variables so basically Express is just a UI for mongodb and in order for us to use it it needs to connect and authenticate with mongodb so we need to provide it the credentials as well that we set for mongodb database and we're passing those also as environment variables but in this case the environment variables are named differently so that's what we're using referring to the official documentation which you always should do to get the most up toate data and you also see the default values for those environment variables the port is correct because that's what we binded it to on our host and mongod to be server which is going to be the mongod to be container name in our case it's different because we called our container mongod beam so we're going to set this environment variable as well so right here I'm going to add this and we're going to set these to mongodb let's not forget the backwards slash here so the ports are correct the environment variables are correct we are going to run it also in the Network we're going to name this Express so that's going to be the name of the container and let's see what the actual name of the image is just going to copy that so that I don't make spelling mistake and that's it let's execute this as well and seems like it started without any problems let's see perfect it's running and now to test that it was actually able to connect without any issues to the mongodb database container we're going to access it in our browser so we opened it on Port 881 on our host and it is asking for basic authentication in the browser and we can actually get those in the locks let's do that do logs of Express and here we have the credentials so admin pass should work and there you go so that's a test and a proof that it was able to connect to our database since we didn't have any connection errors here and we're able to access the application here so this was basically just to demonstrate how you would start containers that belong to each other so Express container actually depends on mongodb because we don't need it without the database in the background so kind of start containers that belong together that should run together using just plain Docker and also starting them in the same network so they can talk to each other in that isolated virtual Network now obviously these are just two containers but if we have microservice application with 10 different services that has a messaging service maybe two databases that it belongs to maybe those databases have their own UI services that we want to run in addition so now these are lots of containers that we need to start and manage using just plain Docker commands and now imagine if you need to stop those containers because you don't want to have them running all the time or you want to make changes and restart them again this is going to be a lot of manual tedious work and you don't want to execute these commands all the time on the command line terminal especially when you have tens of containers so you want an easier way to manage to stop start configure containers that you want to start together and that's exactly where Docker compose comes into the picture so Docker compos basically makes running multiple Docker containers with all this configuration that we just defined on those containers so you have the environment variables you have ports maybe you have multiple ports on the same container same application that you want to open maybe you want to configure additional volumes for example for data persistence so that's the main use case of Docker compose so with Docker compose basically you have a file a yaml file where you define all this configuration a list of contain ERS or services that you want to start together and all their configuration in one central place in a file that you can modify configure and use to start and stop those containers so let's see how the file looks like and how these Docker run commands actually map to the docker compost so how can we migrate or map all of these and write a Docker compost file that starts those two containers with exactly the same configuration that we defined here so this is a Docker run command of the mongod beam that we executed previously so basically with Docker compos file what we can do is we take the whole command with this configuration and map it into a file so we have that command defined in a structured way so if you have let's say 10 20 Docker containers that you want to run for your application and they all need to talk to each other and interact with each other you can basically write all the Run commands for each container in a structured way in Docker compose and Define the entire configuration there and this is how the structure in Docker compose will actually look like so the first two lines are required attributes of Docker compose file with the first line we basically Define the version of Docker compose which is the latest version that should be compatible with the docker compose that you have installed locally so the latest Docker compose tool installed on your computer will be able to read the latest Docker compose file version and then we have the services and Docker compose is super simple Services is basically an attribute where you can list all the services or all the containers that you want to run as part of this doer compos file so in this case the first service we want to Define is mongodb and that Maps actually to The Container name or rather this is going to be part of the container name when the services are created as Docker containers and for each service like Mong TB we have all the configuration for that specific container so the first one is obviously image because we are building the container from the image so we need to know which image that container is going to be built from and of course you can specify version Tech here next to the name the next one is the list of ports because you can open multiple ports on a container if there are multiple processes running inside the container but mostly you would just have one so this is where we Define the port mappings so mapping a container port to the host so just like in Docker command the first Port refers to the host the second one refers to the port inside container then we have the environment variables listed under an environment attribute like this and this is actually how the structure of Docker compose looks like for one specific command now let's actually add the second container command for Express and how that Maps into our Docker compost file so again we have the service which we can call Express and by the way the service names are completely up to you you can call them whatever you want just like the container names you can call the containers whatever you want and under that Express we have the same exact configuration options we have the image which refers to Express image again you can have a TCH here if you want to have a specific one then we have the port and all the environment variables that we defined with Docker run command under the environment attribute and this is how Docker compos will look like with multiple Services defined inside so basically Docker compos is just a structured way to contain very normal common Docker commands and of course it's going to be easier for you to edit this file if you want to change some variables or if you want to change the ports or if you want to add more services with those services and as part of everything as code Trend dock compose is basically a code that defines how your services should run within a file that you can check in to a code repository and multiple people can work on it together compared to a command that you just execute manually on your computer with individual Docker run commands the final thing here which you may already noticed is the network configuration is not defined in the docker compost so we didn't map that part from the docker run commands so this Monga Network that we created we don't have to explicitly create or Define it in Docker compost because Docker compose will actually take care of creating a shared network for all the containers from from the services list that it's going to run when we execute this file so we don't have to create the network specifically and then specify that all the containers run in that Network Docker compose will automatically take care of it and we're actually going to see that in action right away so now let's actually go and create a Docker compos file in a code editor so in this projects directory so basically where I'm in the terminal I created this mongos services. yl file which is my Docker compos file with those two Services defined here so exactly the same code that you just saw we have our credentials all our environment variables defined and since this is a yl format please make sure that your indentations are correct because yl is a very simple language but it's very strict on indentation so the services need to be on the same level and then inside that service you need to have correct indentation for the configuration attributes so now compared to the docker commands it's going to be easier for me to go here to this file first of all see what services I'm running with what configuration edit those make any changes add any new services that I want to run and now let's actually execute this Docker compos file and see how it works back to my warp terminal I'm actually going to stop all the containers because we want to start them using Docker compost so that's the first one let's stop them we can actually remove them and we can also remove the dock Network and there you go so we have a clean State no containers running and now how do we execute a Docker compost file with Docker compost good news is if you have Docker installed on your computer that means you automatically also have Docker compose installed so you don't have to do that separately that means we should have Docker compose command already available as part of Docker and Docker compos takes one attribute which is the file name Services there you go and the command which is up which basically means go through the docker compost file provided here and Run start all the services configured right here so let's execute this and we're going to see the result awesome so now there are a couple of interesting things that I want to point out and highlight in the output that we got and also explain some of the interesting Concepts behind so let's go through them one by one I'm going to scroll all the way up to the beginning of the output which is right here when we executed Docker compose command the first one is I mentioned that Docker compose takes care of creating a dedicated Network for all the containers and here we see in the output that it actually created Network called projects uncore default so this is the name of the network and it's going to run those two containers in that Network so if I open another terminal and if I do Docker Network LS we're going to see projects default network was created another interesting thing to point out is the container names for those two containers so in the docker compose we actually called those services mongodb and Express however as you see Docker compose actually added a prefix projects and a suffix at the end to each service so this is basically the folder that contains the docker compos file where the docker compos file is located as you see right here so Docker compose always takes the name of the folder where the docker compose file is executed and it uses it as a prefix of the container and then you have one as a suffix so we have one instance of each container and that's how the containers are called and we can also check our containers and here you see the names projects mongodb 1 another interesting thing to point out is that you see that the logs of those two containers are actually mixed so we have the mongod be logs Express then mongod be again and so on because we're starting both containers at the same time so if you had 20 Services defined here they will all start at the same time and you will see the logs basically just mixed together on Startup however when you have multiple Services where some Services actually depend on the others in our case Express depends on mongodb because it cannot establish a connection the initial connection with the service until mongodb is fully up and running so we may have such dependencies or we may have an application our custom web application that also needs to connect to the database when we actually start the application to fetch some initial data and so on however if the database is not up and running when the application starts the application will fail with an error because it won't be able to connect to the database because it's not ready for the connection yet and you may have lots of such dependencies when you're running multiple Services as part of one application and this is something that you can Define in Docker compose with a depends on attribute so you can explicitly say this service actually needs to wait for another service or container to be fully up and running until this container is created with a very simple dependson attribute so basically we can say the express service depends on and we can have multiple dependencies so for example we can say an application depends on two different databases to start plus an authentication Service so all of those should be up and running until we start the application because otherwise it's not going to be able to connect to those on the initial startup so dependon takes a list of the services and it basically says wait until all the dependent services are fully up and run running before you start this service so we can fix it very easily using this attribute and now since we have both Services up and running again I'm going to refresh here and we should see Express accessible from the browser and we can actually do something here so we can change something in the database so for example I can create a mydb database and inside that I can create my collection collection I'm very bad with with names and not very creative so that's all we got we have my DB and my collection and this actually creates those in the actual mongodb database cool and if I go back to the terminal we should actually see all these change logs from Express and in mongodb basically logs new entries in the database that it created cool now what do we do if we want to stop those containers or maybe we want to change some configuration in do compose and restart those containers right now since we have the dock compos process running in the terminal we're going to need to do contrl c to basically break out of the process and this is going to stop both of the containers however just like with Docker run commands we have the detached mode we can actually run Docker compose in the detached mode like this so we'll start the containers in the background however now if we want to stop the containers we could stop them using Docker stop commands and providing the ID of the container however again if we have 20 containers running this is not going to be a efficient way to do it and with do compose it's also very simple actually instead of up we do down and what this will do if we have 20 Services defined here that are running as containers it's going to go through all of those and it will actually not only stop those containers but also remove them so now if I do Docker PS a so this shows running and stopped containers so all the containers in any state you see that we have no containers because they have been removed completely and you also see the network itself was removed so basically with Docker composed down you have a very easy way to clean up the entire state so you don't have any leftovers of containers and networks that you created previously everything will be completely removed however when you're running containers and when you make changes like we did in the database for testing you may want to retain those changes the state or the data in those containers so you don't want to completely remove the containers you just want to stop them and then restart them and as you've learned in the docker crash course containers are ephemeral they have no persistence so all the data is gone when you remove the container because by default it doesn't have any persistence unless you configure that persistence with volumes however if you just stop the containers and restart them you will still have the state and data because the container itself was not removed it actually stayed locally so to demonstrate that let's do up again and with Docker compos you can execute stop command which simply stops the containers and if I do docker PSA you see that the containers are still available locally they're just not running they're in an exited status and we can start them again using Docker compose start command and if we refresh our mydb database and collection are gone we can create them again like this we can restart using Docker compose and the data should still be there so that's basically the difference between up and down commands compared to start and stop and obviously both have their different use cases and one more thing since we are executing Docker compose commands very often like this one for example we can actually go ahead and bookmark this like this so if we have too many commands in the history for example and if we are scrolling around which basically creates this visual marker and you can just click inside and it jumps directly to that command we can then copy that command and execute here perfect so now before we move on to the next part of this demo where we connect our own custom application to the mongodb database and run it also as part of Docker compos service let's go to the database and in our new collection let's actually create a new document that our application is going to need it's going to be a very simple document let's add two more attributes here so we're going to have let's call this my ID again as you see I'm very uncreative with names so this is going to be an ID that we can reference in addition to this generated ID and then we're going to have the actual data which is going to be a string and we're just going to write here some Dynamic data loaded from DB so when we load this from our application we know that it's coming from the database so I'm going to save this document in the collection you see it was created here here are the values the generated ID my ID literally my ID and this data um text okay and we're going to make this a little bit more interesting so we're going to use a custom JavaScript application which is a super simple application with just one file that simply connects to the mongodb database and displays the data in the browser so we can see some of the concepts in action and we're going to containerize our JavaScript application and run it as part of the docker compos services and of course I'm going to provide the link to the git repository where this JavaScript application is hosted in the the video description so you can just clone it locally to follow along and by the way you will also find the docker compost file in that repository so all the code that we write in this demo will be there so I have cloned my own application locally in the projects I've called it Docker compos crash course so let's switch inside and to show you how simple the application is I have opened it in the visual studio code so I don't have the docker compos here yet this is the entire application we basically have the server JS which is a node.js backend and index.html which has the style the JavaScript code which is basically just one function and the HTML code in one file so the simplest app ever created so first of all you don't need to understand any part of this code we're just going to concentrate on the configuration and the dockerization part part of this app so even if it's simple app you don't need to understand the code but just to run through the logic on a high level this backand basically connects to the database logic we have this index.html file which shows two lines of data we have some static data which is hardcoded in the file itself and then we have data that is supposed to come from a database so this is empty so we're going to set it dynamically from the data that we get from the database which is going to be this data right here and the way we do that is in this JavaScript section when we load this index HTML page it basically the front end basically sends a request to our server JS backand and it says fetch the data and in server JS we accept that request right here we connect to the database base using this logic right here and now that we are using my DB and my collection as the database and collection name that's why we created them in the database and it connects to this collection and it basically grabs the element that has this key value pair inside my ID one so it's going to get this data here from the collection and it's going to send that the whole object back to the front end as a response and then we're going to grab the data attribute from that response that's the data this is the value of the data and we're going to set it as the value for this second line so that's how the whole thing is going to work and in order to connect to the database because remember we actually set username and password on mongodb so our application needs to have that same username and password just like the  Express container had to have those credentials so we are providing those also as environment variables so just like Express had to receive those values as environment variables our application is also going to receive those as environment variables with these names so Mong to be username M Tob password and we use those to connect to the database that's the entire logic so now our goal is to take this application to build a Docker container out of it using the docker file blueprint which is right here also very simple simple because it's a nodejs application we use node as a base image uh we basically take the code that we have here in the app folder we copy it into the image we run npm install to download the dependencies inside the image and then we just start the application using node command which comes from here and server.js file which is this file right here so it starts the application on Port 3000 and logs this as a first log of the application so we want to use Docker file and again you learn Docker file in the docker crash course how to use it so all this should be familiar to you so we want to build our custom JavaScript application as a container and run it as part of Docker compose along with mongodb and Express Services that's the goal how do we do that first of all we need to copy that Docker compos that we created into this application code and remember another interesting point to highlight here Docker compose is part of the application code so developers work on Docker compose just like they work on Docker file and other parts of the application code which is the best practice to have all this logic together in one repository instead of scripts and commands spread on laptops and computers of different developers everything is in a central place so we created the this Docker compose file in the projects folder and we want to copy this into this folder so let's do a simple copy there you go and here we have our Services yl. compost file and as I said we want to add our application as a third service which is going to run as a container service but we don't have the image yet so we need to build the image as well in do compost what you can actually do you can Define both in one configuration so we can build and then start the container with Docker compose so right here I'm going to add the service for our nodejs application and let's call this my app because great with names and instead of image because we want to build that image first we're going to Simply provide build attribute and the build context which is current directory and this basically points to where the docker file is located as well as the entire build context for that image and the rest of the configuration is going to be the same as for other services so we have the ports in our case we're starting this application on Port 3000 so that's what we're going to Define right here so Port 3000 inside the container we're going to bind it on 3,000 on our host and as we saw we have the environment variables defined here as well so we need to set those so that our application will be able to connect to the database using those credentials and that's basically the entire configuration this will build our node.js application image using Docker file as a blueprint for the image and it will start it as a container on this port and pass in those environment variables that our application will read here and use it to connect to the database now we don't have to configure depend on here because the application doesn't actually connect to the database when it starts up so this is the startup logic so here we don't have any connection it only connects to the database when we load the application in the front end in the browser this function gets executed or this script gets executed and because of that we don't need to do depends on here and now let's go back to the terminal let's first of all see whether we have containers running let's get our Command to stop those containers so we're not going to remove them because we need the database collection and the data inside for our application and now I'm going to go into the docker compose crash course folder where we have the new Docker compose and I'm going to execute Docker compose up and let's execute and as you see it is actually building the new Docker image from the Noe base image and that was actually pretty fast and now we should have all three containers running let's check that and we have really bad names for our containers because the name of the folder is very descriptive large name which was used as a prefix for containers but that's fine and this were created from scratch so our previous containers with this prefix are not actually running instead it created the new ones and that brings me to another concept which is you can actually over IDE the value that is used as a prefix so maybe you want to reuse the same containers but you have moved the docker compost file to another location so let's actually remove those containers that we just started like this so now we only have those two and the way we can override is use using a flag on Docker compose so we can add an additional flag here minus P or also project name so essentially the name of the folder is assumed to be the project name so we can overwrite that project name value using this flag and we can call this projects which was the previous one like this and let's start the container let's do drps as you see our old instances of mongodb and Express were restarted instead of new ones being created plus the network called projects default that was already there and that means if I refresh this we still have our MB and my collection and the data inside for our application which means if I visit the application on Local Host 3000 which should see our awesome application and Let me refresh this once again so we can see the network traffic here we refresh so this was the fetch data request which we execute right here that basically returns this object that we created here from in the database back to the front end so if we go to preview or our response we see this object with my ID one this is the ID from the database and the data some Dynamic data loaded from DB and we're using that to set this line right here so if I actually went there and changed this like this and let save I'm going to refresh again you see that now we get this updated data from the database so the entire connection Works our application is connected to the database and displays that information right here now I mentioned that dock compose is part of the application code which means it gets committed and stored in a git repository so that everyone can work on it it's available for the entire team if a new engineer joins the team and they download the code they have docu compos so they know know exactly what services are running as part of that application and they can easily start that locally however that also means that it's really bad that we are hardcoding our secret credentials in the docker compost file because the best practice for security is that you shouldn't hardcode any sensitive data in the application code so it doesn't end up in the git repository and even if you remove it later if you accidentally checked it in and removed it it's still going to in the commit history so you shouldn't have any hardcoded values here so how do we solve this because we need those credentials to be passed on to Services well for that we can actually use variables or placeholders in Docker compost instead of the actual values and we can set the values of those variables as environment variables on the operating system so let's see how it works first of all we're going to remove all those hard-coded values and instead we're going to define the variables in Docker compos which has a syntax of dollar sign and then curly braces and inside that we can name the variable whatever we want I'm going to call this  admin user because it's the admin user in mongod to be and by the way this could be lowercase you can call this really what you want but I'm using a standard environment variable name convention here with all upper cases so we have the admin user let's call this admin pass for password and we're going to reuse use those everywhere which is another advantage of using variables because if you change those values like if you change the password value for example you just have to change it or set it once and it automatically gets updated everywhere so now this do compost does not have any hardcoded sensitive data and it's safe to check it in the G repository however we still need to set those actual values so to test that let's go back to the terminal first of all I'm going to stop those stop the containers so we can test that everything works and on the first Docker compos command execution we get a warning that says the variables are not set the containers were stopped however we need to set those as variables in our terminal session so we set them here export admin user let's set the other one like this and the same way as we did with up command we actually need to specify which containers we're stopping so by default it's going to look for containers that start with this name the name of the folder so we need to overwrite that projects tag again and there you go and I'm actually going to bookmark this one as well like this and if we refresh we should see that the pages are not working because the containers are stopped and then let's start them again and if we start them again with those environment variables set everything should work same as before let's wait there you go now I want to mention here that Docker compos actually has a concept of Secrets which is another functionality to manage the secrets especially when you're running do compos in a production environment which is exactly for this use case where you need to pass in credentials or any sensitive data to the services defined in Docker compose so you can use Docker compose Secrets as an alternative to this method basically awesome we have just learned the fundamentals of docker compose and more importantly you understand its core use case and by the way I want to highlight the importance of learning tools like Docker and Docker compos or generally cloud and devops Technologies because nowadays it is becoming more and more needed for software developers to learn those tools to become more valuable in their roles especially in the current tense job market where we have layoffs and companies hiring less as more and more companies are adopting devops it is a great way to Stand Out Among developers who only focus on programming and are not interested to learn new Concepts and tools that are being adopted in the industry so I think it's definitely more important than ever to educate yourself keep learning and with devops or Cloud engineering skills you will definitely be ahead of 90% of developers in fact most of our devop boot camp students are actually software developers or software Engineers since many companies do not have a separate devops engineer role but often their responsibility lies on senior developers to set up the devops processes like release pipelines for example so even if you are a junior software engineer learning devops and Cloud skills and technologies will absolutely accelerate your career to a senior engineer so if you want to get a complete education on devops to take over devops tasks at your work then definitely check out our devops boot camp you will learn various Technologies from zero to an advanced level to be able to build real world Davos processes at your job and if you need some guidance before you can also contact us there with your questions so check out the information below and let's move on to the next part so now we are building and running our JavaScript application as a container along with mongodb service and the mongodb UI but usually that's a testing environment as you learn in the docker crash course eventually you want the JavaScript application your custom application container to be stored centrally in a Docker registry or rather the image to be stored in Docker registry so we can start and deploy it as a container on the end environment on a actual deployment server where end users will access it so we need to build the image and push that to the repository like a dockerhub repository or whatever other Docker repository want and now the interesting question is after we push the image to private Docker repository how do we reference our Custom Image from our private Docker repository in Docker compose and also note that when we run this on an actual deployment server we're going to copy the docker compose on that server where we have Docker and Docker compose installed and when we run this Docker compost file or execute this it will go through all the services and it will pull all the images defined here and run them as containers with all this configuration so we pull the official images from docker H public repository and any custom images from the private repositories so let's actually see how it works it's actually very very easy and again building image pushing it to the repository the whole thing you should already know it from the docker course so this should be a refresher for you so let's actually see that in action right here so first of all I'm going to log into my dockerhub account and I'm actually going to create a new private repository in dockerhub like this let's call this my app because it's generic I may use it for some other demonstration later so there you go and now we're going to build our image using the docker file and we're going to push that image to this specific private repository so let's execute those commands I'm going to build using Docker build command I'm going to tag this and this is again a refresher from Docker course we need to take the image with the name of the repository so that Docker knows on push command which repository to push that image to so that's going to be the entire name so the image name itself includes the repository name and we're just going to tag it with a simple 1.0 and we need to provide the build context which is the current directory where Docker file is located and that's basically it let's execute again super first let's list all the images so we can see what we have built locally and there you go this is our image with an image tag and this is exactly the image we want to push now to the private reposer and you know when we want to push or pull from a private repository we need to be logged in to that repository so we need to do Docker login and the username is actually your dockerhub username name and dockerhub user password and this is actually different for other Docker repositories so if you have a ECR or some other Docker repository the process may be different with dockerhub it's very simple that's why I use it for the demos mostly so we are now logged in to dockerhub and what I can do now is basically push that image that we just created using simple Docker push and the image full name with the tag and there you go and if I refresh we should see one tag here 1.0 for our my app image repository perfect and one thing I wanted to show you here is that whenever you are building and pushing an image you basically have the same commands all the time for the specific action you build the image you log in if you're not logged in already you push the image and so on and I have actually used a feature called workflows in workp for these kind of use cases which can be really helpful if you want to if you have a set of commands that you always need for the same type of workflow you can basically Define that as one unit so you can group those commands in one unit called the workflow and you can save it here and whenever you need that you can just execute all those commands with one click which I personally found super cool so in warp drive you have some options here you create a new workflow and I'm going to call this Docker push and here you can list the different commands basically so let's do Docker login and we have username which was this one right here and obviously we don't want to provide a password hardcoded here so we're going to pass that as very variable or argument and we're going to read that from the standard input again this is a refresher from Docker this is how should use Docker login so you don't type in the password directly and with worp you can actually use arguments like this so whenever you run this command or list of commands before it runs it will actually tell you how you should fill out this argument so we're going to use an AR arent here and then after Docker login we're going to do Docker build like we did with an image tag we can also make this an argument let's make this an argument number two and then push that like this you can also set default values let's actually do 1.0 here and this way you don't have to type out all the commands for the same workflow so let me check all these commands so we need a build context at the end and the rest looks pretty good and let's save the workflow and the way it works is now I have the workflow right here and whenever I need that workflow to execute I just click on it and it fills out the terminal basically with all these commands and it highlights the the arguments that I need to set so I'm actually going to put my password here as an argument and then let's say we want 1.1 as a second argument and we can execute all the commands like this and this will actually have pushed another tag with 1.1 so this is a cool feature that you can use on warp to make your life a little bit more convenient and that means now we have our image with two different Texs in a private Repository and now if we go back to the dock compost we don't need to build it locally we can again this is convenient for testing because when you're testing on a local environment as a software developer as an engineer you may want to just do very quick local changes in a Docker file or in application test it quickly and you don't want to be building and pushing and pulling image all the time so this is a very good functionality for local testing however on an end environment obviously we need to Define an image that comes from a repository maybe we have scanned that image already and made sure that it's secure and properly configured and so on and now we want to actually use it and how do we reference our Custom Image from a private repository in Docker compose very simple basically do it just like any other image in dockerhub or any other repository like this with a specific image tag that is available let's do 1.0 and how will Docker compos be able to pull that image from a private repository or basically authenticate with the private repository to pull the image it actually uses the same Docker login that we use to do Docker push so Docker login basically creates after successful login authentication with the docker Hub it actually creates a Docker Json file locally that creates the authentication credentials or tokens in that file and Docker compos in the background is using Docker to run those containers so it's going to be the exact same process pulling or pushing the images from Docker compose so that means if you have done Docker login already to that repository then you should be able to pull any images defined in Docker compose from that repository that means that's the configuration this should work now in order to test that let's actually stop our containers so they're all stopped and I'm actually going to remove the application container because we want to simulate that the container is recreated from the new image that we pull and now if we do up again in DET mode and as you see my app image was pulled and the container my app was started from that if we check the running containers we see that this is the image that was used to create this container awesome and again we can check that our application still works and there you go that's how we can reference our Custom Image from a Docker repository in the docker compose and in case when you're executing those commands so let's say we're doing Docker build and you forget one of the arguments or use a wrong flag and so on first of all you get an error but you can also do a troubleshooting feature within the terminal to actually give you a pretty good tips and notes on what the error actually is because sometimes we make spelling mistakes sometimes we forget an argument or whatever so it could be helpful for a tool to actually tell you what the actual problem is so you can fix it and warp has this AI assistant which is pretty cool so for this specific error if I open this warp AI which you can see on every command block so you have this bookmark and you have this warp AI so if I click on this it actually autog generates the question question because it knows that this is an error and you can ask it how to fix it so you can modify your question or example and if I hit enter here it gives me an answer that the docker build command requires an argument which should be the path to the docker file and gives me an example with the correct one so change the directory that has Docker file and then execute this command which has dot at the end so I found this feature also pretty cool which means if any of the commands give you an error while you're following this demo you can actually use this to find out what the error is about and and ideally how to fix it so this is going to help you troubleshoot your issues and finally I want to actually add a few very interesting and important Concepts regarding docu compose and kind of what the next steps are so this final small section may be really really interesting for you so basically as you see the main use case of Docker composed was to have a central place to manage containers that were supposed to run together like your application and all the services that it depends on and we configure all the environment variables or any other configuration for those services in that one file and also start them in one isolated Docker Network and it makes it super easy for us to clean up all the resources so Engineers took Docker and they containerized their applications to a whole new scale that was not a standard before and Docker was especially perfect to use as a host for microservice applications where you have even more applications and more containers now running in one environment and again if you don't know about microservices I have a separate video about them but essentially it's when you have all the services needed to run one application but split into separate micro applications or services and they can be scaled independently and run independently as independent containers so Docker was a perfect host for that so we ended up with lots of applications lots of microservices applications with hundreds or thousands or tens of thousands of containers that is pretty much a standard nowadays such a scale actually led to Docker compost actually not being able to handle such large scale of containers and more importantly Engineers will have to still manually manage running and operating those containers with do compose like if containers die or crash or have connectivity issues ETC you have to manually detect and then debug and restart the services now Docker compose actually made some improvements on that there tags like restart and so on but it's still a lot of operational effort to run the containers with this kind of scale where you have thousands of them using Docker compose and that's where kubernetes kind of came into the picture to solve exactly these two main issues initially scaling to thousands or tens of thousands of containers with kubernetes you can basically merge hundreds of servers into one huge server to deploy all the containers that belong to the same application in that environment they will all run as if they were running on the same server so it naturally makes it easier to scale your applications and to run thousands of instances and the second one was the automatic operations or making the operations of applications easier or also called kubernetes Auto healing feature which basically manages starting and restarting containers if they crash and has mechanisms to manage operations of a large number of containers in an automated way when manual effort isn't it's just impossible or not feasible anymore and that led to C is becoming so popular so Docker compose is kind of like a intermediary step if you have smaller set of containers but with today's standards when you want to work with very complex applications with a large scale then Docker compose has its limits so that's where kubernetes basically comes into the picture so if you're learning this containerized containerization and container orchestration Concepts then I would actually recommend to use this road map of learning the docker using the docker crash course then learning the docker compose with this course like you did and then you can move on to the kubernetes and if you want to learn kubernetes I also very conveniently have a kubernetes crash course to get you started in kubernetes very easily so if you want to get started with that you can check out any of the many videos that I have on my YouTube channel but I would recommend to start with the kubernetes crash course so I hope you learned a lot of new Concepts you obviously learned Docker compose as a new tool new technology thank you for watching till the end let me know in the comments how this video actually helped you in your work or maybe in your job application I'm always happy to hear and read that feedback from our viewers to know that my videos are helpful in actual job environment you can also share any other tips and learnings about dock compose that you have from your practical experience so that other viewers can read and benefit from it as well and with that thank you for watching and see you in the next video"
gLJdrXPn0ns,2023-12-07T15:01:28Z,DevSecOps Tutorial for Beginners | CI Pipeline with GitHub Actions and Docker Scout,"Welcome to this
DevSecOps Crash course. After an extremely successful
launch of our complete DevSecOps bootcamp, which so many of you
were interested in and so many companies
are already using to upskill their engineers. After seeing this immense
interest in this topic, I wanted to create a DevSecOps
crash course for those who want to get an idea of what that is, to get a basic understanding
of DevSecOps concepts, DevSecOps tools, and get their very first hands
on experience with actual practical implementation
of DevSecOps. So in this crash course, we're going to go through
the fundamentals of what DevSecOps is, as well as see some hands
on examples with a demo project. So you get an understanding
of it. And then if it spikes
your interest, you can decide if you want
to actually enroll and do a full DevSecOps bootcamp
to learn this extremely demanded skill set
and basically just get way ahead in your career.
So let's get started. Security is important at all levels of software development lifecycle.
In the application itself, the application's
runtime environment and underlying infrastructure, which could be on premise
or cloud platform. And it's important to the level
that when companies fail to properly secure things
and they get hacked or some data gets leaked,
et cetera. Where their user data or company
data gets compromised, or their systems get attacked
and aren't accessible anymore, the price they pay
for that is way more expensive than actually
implementing the security. And it's expensive, both financially but also
reputation wise. And of course, that means all companies
should implement security. However, it is pretty difficult
and there are two biggest challenges companies
have in terms of security and what may be
the reason why they fail to actually implement
the security. First of all, often feature development
and providing business value is more incentivized
because that's what brings in customers. That's what provides the direct
value for users. And very simply, that's what brings in money
for the business. So security is like
a necessary evil. You don't get so much
reward and pat on the back for implementing great security. But if a security
incident happens, you get real punishment. So security stays
an afterthought in application development
or even infrastructure configuration
process. The second issue is, even if you and your
team are dedicated to implementing great security, you still have a challenge
because the application systems themselves are becoming
more and more complex. Think about the modern
tech stack. In our application systems, we have a large
containerized microservices application that is running
in Kubernetes cluster on cloud platform, using tons of different
services with data persistence in ten different
types of databases. You may have like a
primary database, a SQL database, NoSQL database,
a caching or memory database, and so on, and tens of external
services that your application may be talking to.
Additionally, we have a streamlined CI CD
pipeline that deploys to the cluster. Imagine how many entry
points and how large of an attack surface such
a complex system has that may allow different
types of attacks. These levels could be within
the application itself. So your own code or third party
applications and libraries that allow for SQL injection,
for example, or cross-site scripting
or forging requests from clients or even worse,
from servers, then you may have security
issues within your application container
image like the image operating system layer,
the image configuration, all these different third
party operating system packages that you may need
in that container environment that may have
security vulnerabilities. Now that container will
have to run somewhere like Kubernetes cluster. So here we have
the security challenges. Like is the access to the
cluster security. Is server publicly accessible
or only from within the internal network? Have you opened any unneeded
ports on worker nodes that allow access into the
cluster nodes directly? Now that's just outside
the cluster. What about inside the cluster?
Once an attacker is inside, do they have wide open
network where thousands of pods can all
talk to each other freely? Can the control plane
processes be easily accessed from within
the cluster? Is the pod two pod
communication encrypted, and so on. Now Kubernetes is not just
floating around on the air, right? It's running
on actual infrastructure. Let's say it's a cloud
infrastructure on AWS. So now the security continues
over to the servers. Then the underlying
infrastructure. Are people able to ssh into the
worker nodes directly. If they can do that, they could potentially access
the Kubernetes processes on that server directly, or the container processes
or even cloud processes running
on those servers. Or what if access is generally
are badly managed, like permissions are not
strict enough, and credentials are spread
around the company on different platforms
and developers machines, so attacker may easily access
them on other internal systems. Continuing with the CI CD
pipeline itself, what about CD accessing your
cluster to make deployment updates? What permissions does your CD
tool have? Is it able to delete
components in all Kubernetes namespaces? So basically if an attacker
hacked into one system like ci CD platform, will they attacker then get
access to credential? Stored in your CD platform
to your private repositories. Kubernetes cluster. Account basically all
the platforms that it connects to. And if yes. What permissions do
those credentials have? Are they restricted or can
they do a lot more damage. And we can go on and on with
these security questions around different
tools and platforms and so on. We'd like secret
management tools, credential rotation
certificates and so on. But I think you got the point. Security is complex
because the systems have become complex. Security is. Afterthought means
that those potential security issues get analyzed
after the main work is done. And there are two problems
with this approach. First of all, this creates long iterations
and slows down the release process compared
to if we checked and found security issues earlier during
the development process itself. And second, when you're checking all
security at once of 50 new features and bug fixes and 50
configuration changes, you may more easily oversee
stuff because you have way more things to test, and more issues may slip
into production as a result. Also, naturally, you have high chance of human
error when this kind of checks are done manually and less
frequently compared to the automated approach. Now you remember my simplified definition of DevOps.
Basically, what it really comes down
to eventually is anything, any tool or concept use
to remove any bottlenecks on the way of releasing
and delivering changes to the end user fast
and with minimal bugs. And this applies whether
it's application or infrastructure changes.
So naturally, if security is a bottleneck
in that release process, that should become part
of DevOps issue that we have to eliminate this showstopper. So DevOps naturally
should include security. But as I often say, reality in theory or how
it's supposed to be are two different scenarios.
So in practice, it's so happened that DevOps
left out the security, it focused on development
and even bug fixes and efficiency and speed
in those areas. But security teams and external
pen tests stayed in later steps, not streamline, not automated,
and still done mostly manually. So as a reminder to kind
of highlight the importance or bring back
the importance of security in DevOps,
the DevSecOps concept emerged. And as you know, DevOps affects entire software
development lifecycle too. So DevSecOps is naturally
taking that overarching security and integrating
it in all DevOps steps from start
to finish, along with application tests,
build steps and so on. So the responsibility of fixing
security issues and secure implementation still lies
with individual teams and different engineering
roles who have the expertise in those specific areas. But DevSecOps creates
an overstretching process and automated steps
that measure what's called the security posture
across your systems, basically giving us a visibility
of how secure our systems are. So how does DevSecOps do this? Automation is the key
here as well, just like it is in DevOps.
So with DevSecOps, we automate checking
and validating all these layers of security
in different parts of the software
development lifecycle. And there are tools
and technologies to run those automated tests. So what are those automated
checks and where in the release pipeline
are they? Edit first we want to check
security of our code. Do we allow for SQL
injection because we're sanitizing user input? Are we using weak
or outdated encryption algorithms to encrypt
user passwords. So all these checks that we're
doing in our code to validate for any such
security vulnerabilities is called
static application security testing, or Sast, where various SAS tools
will validate the static code for any of these issues. So it basically scans
the code base for known patterns of allowing
SQL injection, cross-site scripting, and so on, and common coding mistakes
that could lead to such security issues. And in the DevSecOps
bootcamp itself, we cover the individual security
issue types in detail. So you actually understand
what a SQL injection looks like, what it is, exactly what a cross-site
scripting is, what client or server
side request forgery is, and so on. And we even learn how to fix
some of those issues in code. So instead of just having
an abstract idea, just in theory, you actually see hands on how
it looks like and how it can be fixed in the code itself. Now we also want to check our
code for any hard coded secrets. And this happens way too
often that developers forget to remove API keys
that they use for testing, or hardcoded passwords
for database connection, maybe. And they basically end
up in git repository in the commit history, and secret scanning tools can
be used to go through the code and identify any hard
coded secrets like access tokens, API keys,
or various platforms, any credentials, certificates,
and so on. Again, in the bootcamp, we go into detail
and learn various use cases of when this happen, as well as how to use
these tools as pre-commit hooks so they don't even
end up in the code repository. Commit history because they get
validated before the developer can commit the changes. Now,
apart from our own code, we also want to check whether
the code from other people that we use in our
application has any such issues, like libraries, frameworks that we're
using as dependencies. They are code as well, right?
That other engineers wrote. So those engineers may
also write insecure code just like our engineers. And this is called software
composition. And. Allergies or SCA. So we use SCA tools to scan
all our application dependencies for any
publicly known, already discovered
vulnerabilities. And we identify whether
we're using any outdated versions of third party software
with security issues. And again, here we have a whole
section in the bootcamp. Or explain how these
publicly known vulnerabilities are documented
and where are they accessible, how the SCA tools actually
go through these dependencies and identify any
issues, how to analyze them. Once you found that you
have such vulnerabilities and more
importantly, how to actually
fix those issues. Now these are all static checks.
So we're checking the code base. But there are some security
issues that can only be caught
when the application is actually running. And this is called dynamic
application security testing or Dest, which is a testing method
that basically evaluates a running application
to identify different vulnerabilities.
Again, this could be SQL injection
or manipulating URLs with different parameters
to get data that you are not
authorized to see. So the Does tools basically
send various requests to the application
and they observe how the application responds, what data it returns
to those requests. And this way they can identify
any potential security weaknesses in the application. We also want to validate
the image artifacts that we're producing.
Again, there are tools that scan
the image layers to find any security issues on the
container runtime level. For example, are we using
a root user? Are we using a deprecated
vulnerable operating system package? Are we using a bloated
image with lots of tools that we don't actually need. So we are increasing the attack
surface and risk unnecessarily. So there are all these tools
out there that help us automate these type of security checks.
And again, in DevSecOps bootcamp, we basically go into details
and very importantly, the practical application
of introducing and implementing those tools. One of those concepts
important in the practical usage of the tools
is managing what's called the false positives, as well as how to visualize
the scan reports and analyze them
in vulnerability management tool. We basically combine all
the reports from different tools and see what issues
you have in your application, with what severity levels, where exactly in the application
are those issues, and some recommended options
of how to fix those. Understanding what's called
the quiz and CVEs that are a big part
of analyzing and fixing the discovered issues. And also what's very
important to me is to reference the real world
projects and understanding wherever relevant, whether there is a
difference between how the things should work,
the theoretical part, and how these tools are actually
used in real life scenarios. Things like how to balance
the additional checks that increase
the pipeline duration, and when to run separate
nightly builds for full scans, for example. So I go into detail in this kind
of examples in the boot camp. So these automated security
checks are done in multiple phases of release and can
start as early as pre commit, even before the developer
has committed the code. And the CI CD pipeline
was triggered. So it gives us fast feedback
on any security issues we may be introducing in our systems, through changes in code or
in infrastructure configuration. This is called shifting security
to the left, because another important fact
is that the later in the release stage we discover
security issues, the more expensive it is
to fix it. So instead of reactively
fixing issues in production and patching them, we are proactively reducing
the possibility that they end up
in the production in the first place. Now, talking about reactively
checking for security issues in your systems, I want to give a shout
out to chef, the sponsor of this video, which is one of the important
tools for security and compliance automation
as part of DevSecOps. If you've been in DevOps
long enough, you probably already know
that chef is a well-established
and widely used technology in the industry. Chef compliance provides
packaged CIS benchmark profiles, and these profiles can be easily
customized to support your organization's specific security
and compliance requirements. You can schedule those
compliance scans for single or multiple environments, and you can run them
regularly or on demand to basically automatically
detect and notify about any configuration, drift,
or errors in your environments. For example, running a profile that checks
that 44 controls are set properly and two
of the controls fail because of a misconfigured.
API server dot Yaml. To remediate
this misconfiguration, chef can automatically reset
the controls to the proper configuration based
on the profile. Chef uses the concept
of cookbooks, which provide flexible recipes
with template and attributes files to specify the correct
values for the cube API server Yaml. Using a simple chef
knife command, we can confirm the managed
Kubernetes control plane has the correct cookbook
and recipe in the run list. When the chef compliant skin
is run again, the Kubernetes system meets all
of the necessary requirements. And by the way, the whole Kubernetes security
compliance checks and CIS benchmarks themselves
are super interesting topics which you also
learn with practical use cases in our DevSecOps bootcamp.
So as you see, DevSecOps is a huge exciting
topic where on top of the DevOps, which is already
a huge thing, you explicitly integrate
security implementation in your engineering processes. So large
processes automated. That means lots of tools
and concepts involved. So 1 or 2 hours is really just
a drop in this large DevSecOps ocean to learn
all about it. So I try to take out
this part from the entire DevSecOps bootcamp to teach
about the fundamentals, and I have carefully
created the demo to give you the basics to get started
and see the benefits of DevSecOps, and get the understanding
of how the entire DevSecOps can be implemented
in an organization. So now enough with the theory.
Let's get to the practical part. We're going to be working with one project for the entire demo, and that is an open source
project from OS Foundation, which is a Python based project. And this application
is intentionally vulnerable so that it can serve as a demo
for various security scans. So we can actually see
the security vulnerabilities discovered
by those scanning results. So that's the project
that we're going to use. Since it is Python specific, we're going to see how
to select and then use various scanning tools based
on the language or tech stack that an application is using. And in order to work
with this project, I actually forked the project
and made my own copy. So we can start from a clean
state without any skins whatsoever. So I removed all
the pipeline code, I made a couple of adjustments
and we can build the demo step by step from the start. And I'm going to link
both of these repositories in the video description.
So you can easily follow along. So in this demo we're going
to build a pipeline a release pipeline
that is going to have security checks
for this application. So we're going to build
a DevSecOps pipeline. And we're going to do
that with GitHub actions. Since we are on GitHub. If you don't know
GitHub actions, I actually already have a crash
course on GitHub actions. So you can watch this video
first to learn the basics and get some
foundational knowledge. Of course, I'm going to explain some
details as well in this video, but that should give
you a starting point. And as I said, I do not have any pipeline code
in this project, so we're going to build
it from scratch. So here you see we have a tab called actions. So if I go here and you actually
learn this in the crash course, you have some templates
that you can start with. So instead of writing the GitHub
actions file from scratch, you can just go with one
of the templates. And templates are based
on the tech stack of your application. And as you see
it actually detected what we are using
in this application. And it is suggesting us
to use either Python template or Docker image
template and so on. And we're actually going
to build this pipeline from scratch. However, I still want to show you how
the template will look like. So for example, if we choose a continuous
integration template with Pylint since we're
going to be building continuous integration pipeline. So CI pipeline actually
and if I click on configure this will do two things. First one is in my
project it will automatically create
a dot GitHub folder. So this was not there before. And inside that it will
create workflows folder. And then pylint dot Yaml file. So this path will be
automatically created. This is the location where
GitHub actions detects a pipeline code automatically. So we can automatically
trigger it and run it. And the second thing
is that it generates a boilerplate code
for the continuous integration pipeline. And this
is what it looks like. Again, if you go through my
GitHub actions tutorial you will understand the syntax as well. So basically we could take
over this template code. But I want to show how
to build it from scratch. So first of all I'm going
to rename this to main dot Yaml. So we're going to build multiple
steps in that pipeline. And second of all I'm
just going to mark all of these and just remove. So we're going to start
from scratch. And as you learn in the GitHub
actions course the application
release pipeline, whether it's a CI or CI, CD pipeline is one of the
GitHub workflows. That's why we have these
workflows folder. We can name our pipeline
workflow in our main dot Yaml file. So I'm going
to call it CI. And then we want to configure
when this pipeline will get triggered. And we wanted to get
triggered on push. And you can actually specify
which branches you want to trigger this pipeline for.
So for example, if I had multiple
other branches, except for the main, I can say I only want these
pipeline to trigger. For. A list of specific branches,
whatever that is. However, because this is a continuous
integration pipeline, it makes sense to always run it, no matter whether it's a main
branch or feature branch. So that means we can. Basically say whenever there
is a push in the repository, no matter which branch that is, we always want to run
this pipeline. And now we can start writing
or adding our jobs. And this is going to be
a list of security scan jobs that we're going to run
against our application. And the first one we're
going to be adding is going to be assessed job. So we're going to run static
application security tests on our Python application.
So let's call it Sast skin. And we're going to use
a tool called bandit, which is a popular tool
specifically for Python applications to run static
application security tests. Now, as you know already, I always repeat that the tools
are not as necessary as knowing
the concepts. So you could theoretically
use whatever tool you want. However, of course, when you compare the tools
and evaluate them, you have to consider
a couple of criteria. So first of all, the adoption,
right? If it's largely used
by a community, there are a lot of people
who are contributing to this project. If it's an open
source project, for example, then that is definitely
a plus for the project, because you don't want to be
one of the few engineers who is using a tool that nobody
else is using or knows about. Another one is, of course, how easy it is to integrate
to use the tool. Is there already official
Docker image for the tool? So basically this simple
criteria should be enough to decide what tool to use,
because beyond that, like the specific features
and so on do not matter as much because most of the
tools are pretty similar. They can be configured
in a very similar way. So for most common use
cases they should work pretty much the same. So bandit is a very popular
tool for Python specifically. And it's also pretty
easy to use and that's why I chose that one. But again you can choose
whatever you want. You can also choose multiple
tools for the same job. So you can actually
have 2 or 3 different tools that do sass scanning. And basically you can compare
the results and see maybe one of the tools
finds vulnerabilities that others were not
able to detect. That is a common
practice as well. So let's go ahead and write
our script to use bandit. Again, you learn in the course
that within a job you have multiple steps or actions
that you want to execute during the job. So let's configure all
those steps. So first of all let's
add a description or name. We are running bandit skin. We also want to specify that we want to run it on an ubuntu machine. Because then the installation of the tool Cetera will depend on which operating system we are executing the job or the steps on. So we want an Ubuntu Runtime
environment for our job, and now we can write
those steps. We're going to start by checking out the code, obviously.
So again, as you learn in the course, the jobs get executed on fresh
new environments, on GitHub hosted machines, and you can choose
what operating system that machine should have. And that means it's a fresh
new machine. It doesn't know anything
about your application. It doesn't have any tools
that you need pre-installed on them. So you have to explicitly
install things on it. Plus check out your application
code so you have that available. And obviously we want to scan
our application code. So we want to have the code
on that machine where the job is going
to execute. So check out the code and we're
going to use an action here. Called check out. Version two, and this will take care
of checking out our repository code.
The second step will be. To set up Python on this job
environment. As I said, no tools are pre-installed, so we have to explicitly install
anything that we need for the job. And we need Python because
bandit is a Python package. So we're going to install
it using Python's package manager called pip. So we have to install
Python first. Or basically prepare the Python
installation and setup. And again, for this kind of common
regular use cases they are actions. So we're going to use
one of those actions. That is called setup Python. With this version
for the action, and we can specify a version
of the Python that we want to set up use,
which is logical, because whenever we are
installing a tool, obviously we want to have
an option to specify which version of the tool
we want, right. So we're going to define
Python version 3.8. That's the version
we want to use. And by the way we can find
those actions here as well to see what attributes
and parameters you can use. So if I search for setup
Python there we go. This is the action. And you can see all
the attributes that you can set here. So Python installation
is done here. Now we want to actually
install bend it so we can execute the bended skin right. So as I said it's a
Python package. So we're going to install
it with Python's package manager. So it's going to be
install bend it. And for this we're going
to actually run a command directly on our ubuntu machine
where the job is executed using pip install
bandit command. Super easy
and straightforward right. And finally we want to run
the bandit command or bandit skin. With a command called bandit.
Minus R dot. So this basically scans
everything in the current folder. So this is a location where
we are pointing bendit to. We're saying everything
in this current folder. All the files that it contains
should be scanned recursively. So whatever folders
we have here, subfolders that contain
Python files, all of that should be scanned. Very simple and straightforward
as you see. And the folders or the code
that we have available has been checked out with the first step. So that means the application
code will be on the machine. And after installing bandit
we can just run bandit scan against that entire
application code. So it will check and scan
every single file in the application code
and give us the results. And this is how you set
up and run a security scan. And now we want to commit
those changes. And as I said because we have
this Yaml file in GitHub or GitHub slash
workflows folder, GitHub will automatically
detect this location. And it will know there is a
workflow to automatically execute on code push. So this will trigger our zest
scan job. So let's do that. Commit the change I'm going
to work directly in the main branch for simplicity
for our demo. So let's go ahead and do that. And if I switch to actions
as you see the workflow is already running.
It's in progress. So let's wait for its execution. So run bandit scan
job was executed. And if I go inside, we're going to see
the execution results. And as you see, the job failed. And that is good
because it means the bandit scan actually
found security vulnerabilities in our
Python application. And it failed the job marking
our application is not releasable, which is the purpose
of security scans. Right. So let's go ahead and check
out the results. And right here in the run
bandit scan logs you see the test results listed
with some detailed information. If I scroll all the way down, you see all those are possible
security issues that it detected. And right here we have a summary
that says how many lines of code it scanned and how
many issues it detected. And one helpful thing that the
tool also gives us is it doesn't only tell us, hey, there is a security issue
or possible security issue here, but it also marks it with
the severity level because not all issues
are equally important or equally risky. And that's why we need
to differentiate between them. So we have the severity
level that basically says these are some issues, but they are low severity
so they won't cause as much damage. And they are high
severity issues. So this could be a more risky, highly exploitable
security issue. And this is an important
metadata about the findings because as you see, we have way more low severity
issues than high severity issues. And in practice this creates
a lot of noise and distraction from the actual severe issues. So usually in DevSecOps
we want to configure the scanning tools
to only focus on high severity or medium
severity issues, especially when we are first
introducing these scans to the team, because we imagine we're going
to the developers and saying, now we're going to start
scanning the application, and if there are any
security issues, you have to fix them. And the tool finds hundreds
of security issues, most of them low severity level, so developers don't have
much value from the scan, and they don't know
how to handle these hundreds of security issues.
Right. So this may create a lot
of unneeded effort and just destruction
without bringing much value to the team. You won't be too popular
with developers if you do that. Instead. If you show hey, we ran this and it detected
two severe issues that's manageable for the developer
team and proves the usefulness of the skin
to the developers that aren't fully bought
in the DevSecOps concept yet, and the tools can be tweaked
to teach them or to configure them to only focus on what's
important and mature those tools to the level that we 100%
can rely on their findings. Another metadata
that we are getting here along with the severity
level is confidence. So confidence is basically
the tools level of confidence about the discovery itself. So it found a high issue
with low confidence means it may be an issue. But the tool itself
is not 100% sure that it detected
the issue properly. So it could be a false positive. So for example here we have
an issue that is medium severity.
But the confidence is low, which means the tool
is not actually confident that the issue is and actually
issue with medium severity. So that could be a false
positive. And as I said, we can configure the tool
to ignore everything that is low severity or low confidence
and just concentrate on the important findings. And all security scanning tools
have that configuration option. As I said, most of them work
in a similar way. So let's configure bandit
to ignore and not display any low severity issues as well
as issues with low confidence. And for all these tools,
of course, you have the official
documentation pages where you can see the command
options and how to tweak and configure those
tools to your specific needs for your application.
And right here, as you see, it has an option to configure
what level of severity we want to focus on and
what confidence level want to focus on, and let the tool ignore
anything else. So we're going to use
these options to tweak our bandit configuration. So going back to the repository
this is our workflows folder. And let's. Edit or bend it command
with this configuration. So basically we're going
to tell bend it to only focus on medium and high
level issues and only medium and high confidence findings. So we're going to add
those two options. So we're going to copy them. Add them here. Take this one and that's it. Basically that's
the configuration. Let's commit the changes again. Let the pipeline run. The bandits again failed again.
However, now let's see how many issues
it actually printed out so the summary stays the same. So we get the information
about how many issues we have. However, the logs themselves, you see we only have medium
and high severity issues with high or medium confidence.
Right. So our list of. Findings have. Decreased, which means this is more
manageable for the developers now. So they can actually go
through this and analyze those issues one by one, because they're just a handful
of them. And again, if you're just
starting out. So this is the very first
introduction of DevSecOps tools to your project team. You can even start with only
high security issues. And once you have those fixed,
then move on to medium findings. As the next step, we're going to configure
our bandit scan to produce a reports file and to
provide all the findings of the scans
in the reports file, instead of just displaying them
in the job logs. Again, we can find
that configuration here we have two options for that. First
of all is the output file. So that's going to be the name
of the report file. We can name it whatever we want. And the second one is the format
of that report. So you can produce reports
in multiple formats. This could be CSV, HTML, Json,
XML, whatever. And note that all of these
formats are meant for machine consumption
and not human consumption, which means there are actually
tools where you can upload these scanning reports, where you can visualize in a
nice UI and analyze your findings there in one place. And I'm going to explain
that in detail later in the demo. But for now,
let's produce those reports. So we have the findings
in the reports file. So we're going to export
it in Json format. So let's go ahead and do that. It's going to be
EF or format Json. And we're going to produce
an output. Or output file. And let's call this benefit
report. Dot Json. And as I said, the purpose of generating
a report file is so that we can take
that report's file that has the findings inside
and we can fit it or upload it to a vulnerability
management tool such as Defect Dojo, that will then consume
that file and display all the contents of it in a nice
UI and a nice list with all the information
about the finding, the description,
the fix recommendations, and so on. Whatever the tool
basically provides, which makes the analyzing
and fixing of those issues way easier.
And as I said, if you run the pipeline
multiple times a day, if you have multiple tools, you have to have a central
place where you can view and manage all those findings, or the team of developers can
view all those findings in one place, as well as compare
the findings between the pipeline runs
in a vulnerability management tool
because you can't manage them through the logs.
So this is a very central, very important part
how to upload reports and how to consume them, how to analyze those
issues in Defect Dojo, which is one of the most
popular vulnerability management tools for DevSecOps. And you learn all of these
in the DevSecOps bootcamp. So right now we are generating
the report. However, to make it possible to download
that report, we have to create an artifact
or the job artifact that will be uploaded for the pipeline at
the end of the pipeline execution.
And for that, we want to have another step
to upload that artifact and make
it available for download for us. So let's call
this upload artifact. And there is an action for that,
of course, because it's a very common use
case or common step. It's called upload. Artifact.
Again, you can check what the latest
version is. And we can provide the name of the artifact. So how it should be called
when it's exported. Let's call it banded findings. And of course we need to tell
it which artifact to export. Now, as I said, the jobs in GitHub actions run
on isolated, fresh new machines that spin
up for that specific job. All the steps get executed there
on that machine, and when the job is done,
the machine gets thrown away. Everything that we
generated there, including the reports file,
everything is gone. Right? So that means
we upload artifact. We're taking the file
that we generated on that machine. It's going to be thrown
away after the job is done. And we're going to say you take
that file and upload it as an artifact so we can have
it available after the pipeline has run. So even when the machine
is gone, we still have
that file available. So the path points to the actual
file or folder on that machine. And this is just what we want
to call it. And they will make this file
available after the pipeline run. However, there is one more thing we need
to do for this to work, which is the way GitHub
actions works, is that whenever any
step in the job fails, the next steps will be skipped,
right? So for example, if the Python installation
didn't go through because of whatever reason
or the bandit installation, so this command failed, the next
steps will be skipped. Which makes sense, because usually this is an order
of executing the tasks where the next step
kind of relies on the previous one. And that means when the bandit
command fails, which it will fail
because we have security issues in the job,
this will not get executed. However, we want the findings
or the reports file to be uploaded with those findings. So even if it fails
and that means we have to explicitly tell GitHub
to execute this step always, which means whether the previous
step fails or succeeds, it doesn't matter. Always
execute this last step. So let's commit this. And now,
when the pipeline runs, we should have the bandit
report dot Json file available there. So let's
go back to actions. There you go. So the job failed.
And if I scroll down here. You see the artifact section? And now we have this bandit
findings artifact. I can show that it was not
available for the other jobs. So here we don't
have the artifacts. And this is the Json file. So in the zip file there
is bandit report dot json file. And here we have all
the information about those findings. Additionally to was displayed
in the logs. So that's how it looks like. As I said this is
for machine consumption or for those vulnerability
management tools where you can upload the reports file
and you can display those results in a UI. Awesome. So we have scanning
for our application code using bandit, which does
sass scanning. But we also learned that not
only the application code or the dependency code, but also the application
runtime environment, may have security
vulnerabilities that will allow the hackers
to hack into our systems. And since in the modern
application development, Docker and containers
have become a standard, the artifacts that we
are producing in our release pipelines
is Docker images and Docker image. As you have probably already
learned from my Docker videos, you know that it's built
with layers, so every single image
layer may actually have a security vulnerability.
An image layers, just as a reminder, are basically whenever
we're using a base image. Could be a Linux Alpine
base image, which is a lightweight
operating system layer. And then on top of that, we install a bunch of other
stuff like Python for our Python application
image or various operating system packages and tools. All of that basically add
as layers on top of each other. And just like in
application code, we have libraries
and dependencies with vulnerabilities. We may have operating system
packages and software with vulnerabilities. So we may actually be using
outdated images like base images or operating system
tools with security issues. And the same way we want
to scan the image to understand how secure
is the Docker image that we are building
for our application. And we have various tools
for scanning Docker images. And in this demo, I actually chose a Docker
native tool that actually part of Docker itself that is
called Docker Scout, that goes through the image
layers and scans for security issues. And it does that actually
on multiple levels. So let's add a job for Docker
Scout and see how vulnerable or how
secure our Docker images. And of course, to be able to scan a Docker
image we need to first build a Docker image. So we're going to extend
our current CI pipeline to build the Docker image. And then we're going to scan
that newly built Docker image. So let's go ahead and do that. So right here we're
going to add a new job. And this job will contain
the steps for building the image and then scanning that image. Now this could be
two separate jobs. So for example we can push
that image to a Docker repository. And in a separate job
we can pull that image from the repository and scan it.
However, to make this simpler, we're going to have one job
where the image is built on that job execution machine.
So we have the image available. We don't have to pull
it from anywhere. And then on the same machine
we're going to run scans against that image. So I'm going to call
this job image scan. And I can actually just copy this.
Configuration from here. Let's call these build.
Image and run image scan. We're going to run
on ubuntu latest. And here we're going
to define our steps. The first one is again going
to be checkout because we need our application
code with the Docker file to build the Docker image.
So we have the checkout step. Now for this job we actually
don't need Python. Instead we need Docker installed
on the machine where the job is executing
because we're going to execute docker build
command to build the image and later Docker scout command. So the same way we set up Python
in this machine for this job here we're going
to set up Docker. So let's create a step.
Let's call it set up Docker. And let's actually search
our marketplace for the action to install Docker. So. I'm going to look for set up
Docker and let's see what comes out. And we have
this one here. This is the location
of the action. So basically just
going to copy that. And paste it here.
And as I said, for any actions or any
steps that are very common, like installing
Python or Docker, they are prepackaged or ready
actions that you can just reference from the marketplace, which just makes the creation
of the pipeline easier. But of course, alternatively, you can just run a command
for installing Docker as well. I prefer to use this action
for the setups, just easier cleaner code, especially if something changes
in the installation of the tool. You don't have to worry
about that. And then of course you have
these parameters that allows you to specify additional stuff. So for example I want
to define Docker version for our installation. And I'm going to do that with
the width attribute. And. This is the Docker version. And I'm going to set it to one
of the latest Docker versions. Let's do 2010 seven. And that makes Docker
available on our job environment, which means we can now
execute Docker commands. And the first command will be
to build the image. And for that we're going to simply run docker build command.
And you know the drill. We need to specify the Docker
file as a blueprint for building the image. We can also specify
the name of the image so we can reference it later
when we want to scan the image. So we can call this my
app or Pi goat or whatever. And we can tag it with
the latest tag. And then we have to specify
the build context, which is the location
that Docker will take as a build context. And this is going to be
the current directory where the application code is. And that's our Docker build
command. Awesome. The next step will be
to scan that build image for any security issues. And as I said we're going to use
Docker Scout from Docker itself, which actually does a very
thorough scanning of the images on multiple
layers to find any security issues. And we can actually use two
different commands of Docker scout. One of them is called
quick view. Quick view command will
basically show you that your base image is outdated, and it will give
you a recommendation of how to update it to make
your image more secure. And then there is a more
extensive or more thorough scanning that you can do
with the Docker Scout CVS command. It basically gives
you a complete view of all the vulnerabilities
that your image contains. And just like the other
tools that we've used, you can tweak it and configure
it to add additional flags
to basically limit that you are only
interested in certain severity level and so on. So let's go ahead and add a step
for Docker Scout commands. So we're going to add
a step here. Let's call this Docker scout.
Skin. And we're going to run a command
here that basically is a multi-line command. And this syntax basically
allows us to write multiple commands one after another
instead of just single command. And here we're going to first
install the Docker scout command line tool. And then we're going to execute
Docker scout commands. And I'm going to copy the URL
which points to the installation. So this will basically
download the installer for Docker scout cli. So install scout dot s h
file will be created locally. Then we can execute
that installer file that shell script to actually
install Docker scout. And here we can then execute
Docker scout commands. As I said we can use both
commands for a quick view as well as to scan the complete
image for any vulnerabilities. So this is actually the main
command that scans the image. And that's what we
are executing. Now let's actually try to run
this and see what happens. So I'm going to commit
the changes and let's actually see the execution result. Again. You see that by default these
two jobs will actually be executed on two
different machines. That saves time because these
jobs can be executed in parallel
instead of waiting for the previous
jobs to execute. So your pipeline
is overall faster. So the job is running.
Let's wait for that. The image is being built. And we can also check the docker file. So this is a pretty simple
Docker file actually. We just have a couple
of commands. So each one of those commands
basically creates a new layer. And we may be configuring
the Docker environment or installing tools
that are vulnerable because they're outdated. And those things will be scanned
with the scanning tool. And the Docker scout scan
failed. And you see, the reason for that is because
we need to log in to Docker to execute
Docker scout. So we need to authenticate
with the Docker ID and email address. So that's what we need to set
to authenticate with Docker. So we can execute Docker
scout commands. And that leads to another
interesting concept in GitHub actions which is project secrets
or projects environment variables that you can
use to basically store secret or sensitive data.
As you know, in release pipelines, CI
pipeline or CI CD pipeline, you have to integrate
with multiple tools, right? So you are maybe pushing to a
Docker registry, maybe you are deploying
to an environment and you have to connect to these
platforms with credentials. Right? So you need a proper
way of storing those credentials. And obviously you don't want
to hardcode them in the code. Right. So in the settings
of the project you have security section. And in that security section
you have secrets and variables. So if I open that and
open actions. So these are where you can
create secrets and variables for GitHub actions workflows. And we're going to create
a new repository secret. So basically whatever secrets
and variables we create here will be available
as environment variables in the GitHub workflows. So you may have multiple
workflows for GitHub actions. And you can use these
secret values or variables in all your workflows. So here I'm going to create
variables secret variables for my Docker
user and Docker password. So basically these are the ones
that I use to log into Docker Hub. So if you don't have an account
you can just sign up here and you get your
Docker ID and Docker password. And I'm going to call
this repo user. You can call this whatever
you want. And this is my Docker ID. And now I'm going to create
repo password. And with the value
of my Docker password. So those two values are here, which means I can now reference
them in the pipeline or in my workflow. So going back
let's edit. So before we execute
the Docker scout commands we need to first
log in to Docker. And you probably already
know docker login command from various of my previous
tutorials where I have showed this. So a safer login
is not to provide password directly with password, but rather read it from
the command line. So we're going to do echo and. Our password variable
and the syntax for referencing environment
variables or project variables, repository secrets or repository
variables in GitHub. Actions is very simply
dollar sign and double curly braces like this. And inside that you have
secrets object that contains all the secrets
that you have defined here. Like this. So this is referencing
the value of the password. And then we are piping that to
Docker login command. So we have the username
which again we're going to reference like this. Repo user. And we're going to read
this password using. Password. Stdin standard input. So this is going to read
whatever we echoed here and that's it. We don't have to provide
the repository for Docker login because by default
it is Docker or docker.io. So this login command will
automatically go to Docker itself. And that's it. We are authenticating
with Docker. And after that we can execute
Docker scout commands. So now we're going to commit
those changes. And we're going to see
the results of Docker image scan our pipeline ran. So now let's actually check
the logs to see the findings. First of all you see
that the job is green. Let's see what it means. So we have the build Docker
image a new image was built and then we have
Docker Scout results. This is the login part. And this is basically
an output of Docker Scout quick view command
which basically gives you an overview of what image
you're using. What is the base image. That we have defined
in our Docker file that we are building on top of. And then it also tells us
whether our base image is outdated,
the size of the image. As you know from security
best practices of Docker, we don't want to use bloated, unnecessarily large images
because it just increases the attack
surface unnecessarily, especially if we don't use
or if we don't need most of the tools in that image
for our application and so on. So this is like a
quick overview. And this is a more detailed
overview of analyzing the image. And this is the Docker
scaled CVS command. And here you see a whole
list of the things it found which are a lot of issues.
So basically. For different tools that we're
installing or using in our Docker image.
For example, this one here, the curl package. Or this package with this specific version have all these vulnerabilities. And this is actually similar
to the dependency scan, because just like you
have libraries in your code that have
dependencies on other libraries and so on. So you have these
transient dependencies. Here we have a couple of tools
that we're installing. However, since we're using
this as a base image, which depends on another
base image that has some tools installed, Docker Scout basically goes
through all these image layers, including whatever base
image this one is built on, and it looks at the tools
that not only the tools that we're installing
on top of this image, but also whatever tools
this image itself comes with. And that's why we have
so many issues here. We have a pretty large list
of vulnerabilities that we found, because it basically went
through multiple layers all the way to the initial image, and we can actually check
that ourselves as well. So if we look for Python
image in Docker Hub. So this is the official Python image. And in text we basically look
for this text specifically. This one right here. As you see, Docker Hub itself shows
you this vulnerability scan results for the base image, and this one is also supported
by Docker Scout. And here you see the exact
breakdown of which packages in this specific image
are included and what vulnerabilities
those packages actually have. And as you see we have
curl Python open SSL. All of these are basically
part of this. And the thing is if we don't
need curl for example, or git in that image, then there's no need to use
this larger image as a base. Instead, we can use a slimmer, lightweight image with a less
libraries and less libraries automatically means less risk
for finding vulnerabilities, right? And this is
the Docker file that is used to create
this image that installs all those tools and so on. And again you have
this differentiation with critical high
level medium and low severity vulnerabilities. So you can kind of prioritize
and see if you have critical issues
which libraries are affected by those critical
issues and so on. So that's one thing using just
smaller images. But also this is one
of the older versions. The newer version
is already at 3.13. So of course upgrading
to a new version often would mean that some
of those issues and vulnerabilities
were actually fixed. But you can also
introduce new ones. So we're not going to go
into the remediation part. But this kind of scan basically
gives you a really good overview of whether
you are using an outdated image or whether
your image is too large. So you have lots of libraries
and packages inside which also have vulnerabilities. So you end up with a huge
list of security issues in your Docker image scan.
And also as we saw, we have this severity level
for each security finding. And we have 23 critical
issues and 267 low level issues or low severity issues, which means again this creates
a lot of noise. Look how large the list is.
That means, especially at the beginning
when it's the first time running an image scan for your
application your engineers probably don't want to deal
with, in this case, hundreds and hundreds
of vulnerabilities and just fix them so you can focus
on the critical and high ones. And then basically step by step, move on to the less critical
ones. So in this case, again, it makes sense
to configure Docker scout command to only print out. Those two severity level issues
and basically ignore the other ones.
And as you see here, you can even use
the Docker Scout recommendations command to give
you suggestions of how to fix those issues
that were discovered. And one more interesting
thing that I want to draw your attention to is that for
the issues that are discovered.
So for example, this one right here,
apart from the CV link, you also have the fixed
version attribute. So it tells you which version
you are using which is less than or smaller
than this version. And it tells you that there
is a version that has this vulnerability
fixed already, so you can upgrade to that one
to basically get rid of this security issue. And you also see that for some
issues the fix has not been done yet. So there is no safer
version for that for now. But again, many of these apply to the low
severity issues, which means we can now go
back to our workflow and configure Docker Scout
to basically just ignore all of those. So we don't have
this overwhelming list of issues, but we can kind of filter out
and just focus on the real issues and have them in the
logs. However, before we add these
configuration options directly here, I want to show you an
alternative option of running Docker Scout
commands with a ready GitHub action from the marketplace. And this is another good example
to show using the ready actions from the marketplace.
And of course, the main advantage of using
actions from here is always that it's high level,
it's more abstract, and it's just easier
to configure than directly working with the tool.
Less flexible sometimes, but if you just want to run
the tool with a couple of parameters and configuration, it's basically the easiest
way to get started. And if I look for Docker scout. You'll actually see that there
is one from Docker itself. So this is an official action, which is always good to use
the official ones. Now obviously the installation
of Docker Scout itself was pretty simple, as well as running the Docker
scout commands, so there is no requirement
or need to use action instead for simplicity
because it's pretty simple already.
But generally speaking, these ready actions make
it easier to use any tool with a high level configuration
so you don't have to worry about installing the tool.
You know, making sure that these curl
link is up to date and so on. So I just want to switch
to this one for demonstration. But to make sure you guys
still see this code snippet in the repository
when you follow along the demo, I'm actually just going
to comment this out. And I'm just going to create a new step. And this is going to use
the action. There you go. So this is Docker scout
action with this version. And then we have a couple
of configuration options. Obviously we need to configure
the login data just like we did here. So for that we have Docker Hub
user and Docker Hub password. So we're going to do
with and then we're going to set all those
parameters that we need. So Docker Hub user is going
to be. Referenced like this. Then we have Docker
Hub password, and I'm just copying this stuff
so that I don't make any spelling mistake. There you go. So we have
the login data. And finally we have the command
because we actually have to execute some kind of command.
And this is a list of commands. So we can basically execute
multiple Docker scout commands. We just need two of them.
So I'm just going to. List them here like this
separated by comma. And that's it. So this is basically exactly
the same as this part here. Looks a little bit
cleaner nicer. That's the only difference
in terms of what they do. However when I execute this we
will actually see one more difference of using
this action instead of running the Docker
scout commands like this, which is an improvement. So let's commit the changes
and let's wait for the job execution.
Okay. So our pipeline executed. So this is the one with Docker
scout action. And this is without. And let's
compare those two. So this is the workflow
with Docker scout action. And this is without. So if I scroll down here
we basically have our artifacts and some information
like annotations. And when we execute
it the build image and scan image with the official
Docker scout action. If I scroll down we see these. Visualization of results
in the user interface view itself. So instead of having to go
and check the logs, we can basically see
the entire thing. Here we see the breakdown
of what libraries got scanned, how many critical, high, etcetera issues
each library had, as well as the base
image and total summary of the entire image scan, which is actually pretty nice
because this makes it way easier to analyze and kind
of dig in into your image and what libraries you're using
and what you may need to update to fix those issues and so on.
So for example, you see all these libraries
actually do not have any critical issues,
so you can just ignore them, etcetera. So it gives
you this nice overview. So that could be another
advantage of using the Scout action. And finally
with that configuration let's now tweak our Docker Scout
action to only report critical and high level issues.
And also in addition to that, we want to create this report
file that we have generated for other tools.
So let's do those two changes. I'm going to edit
this again and again. If we check out the official
documentation to see what configuration
options we have. But this time for the Scout
action itself. And again if I bring
up the Scout action. We're going to see all the configuration options here.
However, if you want, for example a more detailed
overview with examples and so on, we can also search
it online. So this GitHub repository
for example that has Scout action gives you a more detailed
description of the inputs for different commands.
So this one for example. And as I said we only
want to focus on certain level of severities. And this is option
to configure that. So we have only severities. And we can basically just
choose or provide a comma separated list of what severity
levels we want to focus on. So I'm going to copy this.
And edit here as a parameter. And I'm going to choose
critical and high. So that takes care of ignoring
all the low and medium level issues. And we also want to configure
the report file. And we have this sorry file
which is a specific format. And again if I check this here
it basically expects a file name. So we can set this parameter
as well. And we can call this Scout
report. Dot. Serif and. Of course, we have to upload
that artifact as well so that it's available after
the workflow runs. And. Let's call these Docker Scout findings. And of course the name
of the report. And that's our configuration. Now with this we're going
to commit the changes and run the workflow again. And now up to this configuration, we see in the scan results
that only critical and high level vulnerabilities
were displayed by the tool and the list
of libraries that were scanned. We see that only those
that have either critical or high vulnerabilities
are displayed here. So we don't have this huge
list anymore, but we have rather manageable
list right now. So we can even limit that only
to critical issues. So we would basically be
working on fixing and updating these libraries here. And again, we still see the overview on all
levels of security issues, including low and medium.
However, the specific issues are only
limited to those two, so it's not
overwhelming anymore. And additionally, we also have these Docker Scout
findings which were exported as an artifact. And again we can download
it and import it in a vulnerability
management tool along with other reports. One more optimization we can
do with Docker Scout is to basically fail
the job when security vulnerabilities are found.
And again, if we go back
to the documentation, we see there is this exit
code parameter that is by default set to false. And we can set this exit
code to non zero value which will be unsuccessful
or error result which will fail the job. So again let's adjust
our configuration. And instead of default false,
I'm going to set it to true. And let's commit the change. And wait for the result. And if we check
our pipeline run, you see that our image
scan job is also red because we have security
issue findings in the result. So we optimized that as well.
Awesome. So our pipeline is doing
all the static code checks, is scanning the image artifact
that we are building for any vulnerabilities. And we're producing these
two different scan reports. And when you are implementing
DevSecOps in your organization you have to use
vulnerability management tools, because otherwise
it will be really inconvenient and hard to analyze
and fix the security issues, or to basically just see
the security posture of your application
and of your systems based on the security
scans that you are doing. So it kind of unnecessarily
making your work harder. And as I said, there are different
vulnerability management tools. One of the popular ones,
which is an open source project, is Defect Dojo, which is the tool I teach
in the DevSecOps bootcamp. And of course, every time we run
this workflow, it will produce new reports. That means you don't want to be
manually downloading these reports from each
workflow execution, and then importing that manually
in the dojo. Either you want to automate
that process because it just happens to often
and it's a repetitive task. So in the DevOps bootcamp,
of course, we want to learn things as they
are done in production in a proper way. So actually show how to write
a Python automation script that takes the reports
from each pipeline execution and automatically
connects to the Defect Dojo API, and automatically uploads
and imports those reports in the defect dojo.
And again, you can group that per
application version. So we have a history of scan
results and see whether your issues are increasing over time
or as you are fixing the issues, or they decreasing and getting
less over time. The second important
point is now that we've discovered the issues, those
issues need to be fixed, right. So first of all,
who fixes those security issues? Is it a DevSecOps engineer?
Is it a DevOps engineer? Security engineer?
The application team itself. So who is responsible for fixing
the issues in the code, fixing the issues
in the libraries, or upgrading the library
version so we don't use vulnerable, unsafe
or insecure libraries? Who fixes the Docker
image issues and so on. And this is important
to understand. To know how DevSecOps
is implemented, how the responsibilities
are divided among the team members in a practical, actual,
real time project. So in the DevSecOps bootcamp, we go into the topic
of dividing the roles and responsibilities
with DevSecOps principles, and also how
to pragmatically approach this in an organization
to implement DevSecOps and involve all of these
other roles in the implementation
process as well, and to kind of motivate
them to join in and not can resist against it. What's also super important
is we see different issue types like SQL injection,
vulnerable code, vulnerable third
party libraries, and how to fix those including
the transient dependencies. Same with the Docker
image scanning. How to fix security issues
found in your Docker image. And here it's important
to understand the CVEs in dependency scanning
and image scanning. So we go into detail in all
of those areas and learn how to analyze and find such issues. And then of course
fix those issues. And we actually use
completely different project and completely different
application in the bootcamp. So these Python application
on GitHub actions is just to demonstrate the basic
principles in DevSecOps. So we are actually not
repeating anything from this crash course
in the DevSecOps bootcamp. So this should already give
you a pretty good basis and understanding of DevSecOps. So you can actually
go ahead and start implementing this already.
However, you may want to know what the
next steps would be that DevSecOps also
encompasses and also more advanced scenarios. Diving deeper and really
getting to the production grade DevSecOps processes.
So of course, the obvious one is that
the continuous deployment part comes after
that we're deploying to servers on an
infrastructure will open up another world
of security concepts like cloud security,
infrastructure security, server administration, secure deployment to the servers
and so on. And as you know, in today's world, no DevOps topic is complete
without Kubernetes, which again, is its own separate world
of various different concepts that are security
relevant and security related. So starting from security,
handling, data encryption, network security within
the Kubernetes cluster, access control management, and. And DevOps and ops is anyways
about automation. So policy is code, cloud
infrastructure is code, compliance is code and so on. So there is a ton
of concepts and tools and topics involved
in DevSecOps that takes this whole thing to a completely
new next level. And that's exactly why
we have a complete bootcamp to teach
you all of this, because it's a huge subject. It's a very interesting
but very complex skill set. So you need a proper guide
with easy explanations, with real life production use
cases and examples to become really good at this subject.
So all these advanced topics, plus the monitoring and logging
on cloud level, on application level,
on Kubernetes, cluster level
for security specifically. So all of that is in
the bootcamp. That means if you need
this for your career, for your position, or if you work at a company
and your company or your projects actually need this, then DevSecOps bootcamp
that we created gives you complete picture
and complete skill set of everything you need to learn
and know about DevSecOps. We worked on this for almost
two years and there is way more content, and the topics and projects
are way more comprehensive than anything
that you can find out there. So as I said, if this is a topic
of interest for you, then I definitely recommend
our DevSecOps would come as a next step to completely
uplevel your career for just a fraction
of the price of what an engineer with this skill set will earn. So definitely check out
the information about the bootcamp in
the video description. But if you just needed
to get the conceptual understanding of DevSecOps,
understand what it is, and get your first practical
experience with DevSecOps, then I hope I was able
to give you exactly that. And this will help you in
your job, in your project. I'll be very happy
about that as well. And if it did, please let me know in the
comments what you liked about this, what value you get
out of this, and as well as share
it with other engineers who you think will also
benefit from this knowledge. And with that, thank you for watching
Till the End and see you in the next video."
pg19Z8LL06w,2023-02-15T15:08:45Z,Docker Crash Course for Absolute Beginners [NEW],in this video I will teach you all the main concepts of Docker including getting your first hands-on experience with it so if you have to use Docker at work or if you need to learn Docker to level up your engineering skills and need to get started fast and understand all the main Concepts and learn basics of how to work with Docker this crash course is exactly right for you first we'll start by explaining what Docker is why was it even created basically what problems it solves in engineering and how it helps in software development and deployment process so you will understand exactly why Docker is such a big deal and why it has become so popular and widely used in IT projects and as part of a virtualization solution Docker being an improvement over virtual machines or the next Evolution step I will also explain the difference between virtual machine and Docker and what are the advantages of Docker in this comparison after we've understood why we want to use Docker in the first place we will install Docker and learn how to actually work with it we will learn the concepts of Docker images containers Docker registry public and private Registries and we will run containers locally based on some of the images available on Dockers public registry called Docker Hub we will also learn the concept of creating your own images and learning about a Docker image blueprint called Docker file and of course we will see all these in action and learn all the docker commands for pulling images running containers building your own Docker image Etc we will also learn about versioning images with image text and finally after you've learned how to work with Docker I will also explain with graphical animations how Docker fits in the big picture of software development and deployment process so by the end of this video you will feel way more confident about your knowledge and understanding in Docker and and can easily build on that Foundation knowledge to become a Docker power user if you want to and under the video description I will provide some resources to learn even more about Docker and become more advanced in it but before we jump right in it seems like many of you watching the videos on our channel are still not subscribed so if you're getting some value out of the free tutorials I put out regularly on this channel be sure to subscribe not to miss any future videos or tutorials I would also be happy to connect with you on my other social media accounts where I post behind the scenes content weekly updates and so on so hope to connect to you there as well well I'm super excited to teach you all these so let's get into it let's start with the most important question what is Docker why was it even created and what problem does it solve in simple words Docker is a virtualization software that makes developing and deploying applications very easy much easier compared to how it was done before Docker was introduced and Docker does that by packaging an application into something called a container that has everything the application needs to run like the application code itself its libraries and dependencies but also the runtime and environment configuration so application and its running environment are both packaged in a single Docker package which you can easily share and distribute now why is this a big deal and how are applications actually developed and deployed before Docker was introduced let's see that to understand the benefits of Docker more clearly so how did we develop applications before containers usually when you have a team of developers working on some application they would have to install all the services that application depends on or needs like database Services Etc directly on their operating system right for example if you're developing a JavaScript application and you need a postgresql database maybe you need a redis for caching mosquito for messaging like you have a microservices application now you need all these Services locally on your development environment so you can actually develop and test the application right and every developer in the team would then have to go and install all those Services configure and run them on their local development environment and depending on which operating system they're using the installation process will be different because installing postgresql database on Mac OS is different from installing it on a Windows machine for example another thing with installing Services directly on an operating system following some installation guide is that you usually have multiple steps of installation and then configuration of the service so with multiple commands that you have to execute to install configure and set up the service the chances of something going wrong and error happening is actually pretty high and this approach or this process of setting up a development environment for a developer can actually be pretty tedious depending on how complex your application is for example if you have 10 services that your application is using then you would have to do that installation 10 times for each service and again it will differ within the team based on what operating system each developer is using now let's see how containers solve some of these problems with containers you actually do not have to install any of the services directly on your operating system because with Docker you have that service packaged in one isolated environment so you have postgresql with a specific version packaged with its whole configuration inside of a container so as a developer you don't have to go and look for some binaries to download and install on your machine but rather you just go ahead and start that service as a Docker container using a single Docker command which fetches the container package from internet and starts it on your computer and the docker command will be the same regardless of which operating system you're on and it will also be the same regardless of which service you are installing so if you have 10 services that your JavaScript application depends on you would just have to run 10 Docker commands for each container and that will be it so as you see Docker standardizes the process of running any service on your development environment and makes the whole process much easier so you can basically focus and work more on development instead of trying to install and configure services on your machine and this obviously makes setting up your local development environment much faster and easier than the option without containers plus with the docker you can even have different versions of the same application running on your local environment without having any conflict which is very difficult to do if you are installing that same application with different versions directly on your operating system and we will actually see all of this in action in the demo part of this video now let's see how containers can improve the application deployment process before containers a traditional deployment process would look like this development team would produce an application artifact or a package together with a set of instructions of how to actually install and configure that application package on the server so you would have something like a jar file for Java application or something similar depending on the programming language used and in addition of course you would have some kind of database service or some other services that your application needed also with a set of instructions of how to configure and set it up on the server so that application could connect to it and use it so development team would give that application artifact or package over to the operations team and the operations team would handle installing and configuring the application and all its dependent services like database for example now the problem with this kind of approach is that first of all you need to configure everything and install everything again indirectly on the operating system which I I mentioned in the development context that is actually very error prone and you can have various different problems during the setup process you can also have conflicts with dependency versions where two services are depending on the same library for example but with different versions and when that happens it's going to make the setup process way more difficult and complex so basically a lot of things that can go wrong when operations team is installing and setting up application any services on a server another problem that could arise from this kind of process is when there is a miscommunication between the development team and operations team because since everything is in a textual guide like an instruction list of how to configure and run the application or maybe some kind of checklist there could be cases where developers forget to mention some important step about configuration and when that part fails the operations team have to go back to developers and ask for more details and input and this could lead to some back and forth communication until the application is successfully deployed on the server so basically you have this additional communication overhead where developers have to communicate in some kind of textual graphical whatever format how the application should run and as I mentioned this could lead to issues and miscommunications with containers this process is actually simplified because now developers create an application package that doesn't only include the code itself but also all the dependencies and the configuration for the application so instead of having to write that in some textual format and document they basically just package all of that inside the application artifact and since it's already encapsulated in one environment the operations people don't have to configure any of this stuff directly on the server so it makes the whole process way easier and there is less room for issues that I mentioned previously so the only thing now that operations team need to do in this case is to run a Docker command that gets the container package that developers created and runs it on the server the same way operations team will run any services that application needs also as Docker containers and that makes the deployment process way easier on the operation side now of course the operations team will have to install all and set up the docker runtime on the server before they will be able to run containers but that's just one-time effort for one service or one technology and once you have Docker runtime installed you can simply run Docker containers on that server now at the beginning I mentioned that Docker is a virtualization tool just like a virtual machine and virtual machines have been around for a long time so why did Docker become so widely adopted what advantage is it has over virtual machines and what is the difference between the two for that we need to see a little bit of how Docker works on a technical level I also said that with Docker you don't need to install Services directly on operating system but in that case how does Docker run its containers on an operating system now in order to understand all this let's first look at how an operating system is made up operating systems have two main layers you have the operating system kernel and the operating system Apple locations layer and kernel is the part that communicates with the hardware components like CPU memory storage Etc so when you have a physical machine with all these resources and you install operating system on that physical machine the kernel of the operating system will actually be the one talking to the hardware components to allocate resources like CPU memory storage Etc to the applications then running on that operating system and those applications are part of the applications layer and they run on top of the kernel layer so kernel is kind of a middleman between the applications that you see when you interact with your computer and the underlying Hardware of your computer and now since Docker and virtual machine are both virtualization tools the question is what part of the operating system they actually virtualize and that's where the main difference between Docker and virtual machines actually lie so Docker virtualizes the applications layer this means when you run a Docker container it actually contains the applications layer of the operating system and some other applications installed on top of that application layer this could be a Java runtime or python or whatever and it uses the kernel of the host because it doesn't have its own kernel the virtual machine on the other hand has the applications layer and its own kernel so it virtualizes the complete operating system which means that when you download a virtual machine image on your host it doesn't use the host kernel it actually puts up its own so what is this difference between Docker and virtual machine actually mean first of all the size of the docker packages or images are much smaller because they just have to implement one layer of the operating system so Docker images are usually a couple of megabytes large virtual machine images on the other hand can be a couple of gigabytes this means when working with Docker you actually save a lot of disk space you can run and start Docker containers much faster than virtual machines because virtual machine has to put up a kernel every time it starts while Docker container just reuses the host kernel and you just start the application layer on top of it so while virtual machine needs a couple of minutes to start up Docker containers usually start up in a few milliseconds the third difference is compatibility so you can run virtual image of any operating system on any other operating system host so on a Windows machine you can run a Linux virtual machine for example but you can't do that with Docker at least not directly so what is the problem here let's say you have a Windows operating system with Windows kernel and its application layer and you want to run a Linux based Docker image directly on that Windows host the problem here is that Linux based Docker image cannot use the windows kernel it wouldn't need a Linux kernel to run because you can run a Linux application layer on a Windows kernel so that's kind of an issue with Docker however when you're developing on Windows or Mac OS you want to run various Services because most containers for the popular services are actually Linux based also interesting to note that Docker was originally written and built for Linux but later Docker actually made an update and developed what's called Docker desktop for Windows and Mac which made it possible to run Linux based containers on Windows and Mac computers as well so the way it works is that Docker desktop uses a hypervisor layer with a lightweight Linux Distribution on top of it to provide the needed Linux kernel and this way make running Linux based containers possible on Windows and Mac operating systems and by the way if you want to understand more about virtualization and how virtual machines work and what a hypervisor for example is you can watch my other video where I explain all of that in detail so this means for local development as an engineer you would install Docker desktop on your Windows or Mac OS computer to run Linux based images which as I mentioned most of the popular Services databases Etc are mostly Linux based so you would need that and that brings us to the installation of Docker in order to do some demos and learn Docker in practice you would first need to install it so in order to install Docker you just go to their official page for installation guide and follow the steps because Docker gets updated all the time the installation changes so instead of me just giving you some comments that may work now but we'll get updated in the future you should always refer to the latest documentation for installation guide for any tool so if we search for Docker desktop installation click on one of those links like install on windows so that's the docker desktop the tool that I mentioned that solves this problem of running Linux based images on a different operating system but it actually includes a lot of other things when you install it so what are you exactly installing with Docker desktop and you see exactly what's included in there so basically get the docker service itself it's called Docker engine that's the main part of the docker that makes this virtualization possible but when we have a service we need to communicate with that right so we need a client that can talk to that service so Docker desktop actually comes with a command line interface client which means we can execute Docker commands on a command line to start containers to create containers start stop them remove them Etc and do all kinds of things and it also comes with a graphical user interface client so if you're not comfortable working with command line you can actually use the graphical using interface where you can do all these things but in a nice user-friendly UI so you get all these things when you install Docker desktop basically everything that you need to get started with Docker and of course depending on which operating system you're on you're going to choose that one Mac windows on Linux so let's click on one of those and you basically just follow the instructions you have some system requirements you have to check things like the the version of your Mac OS how much resources you're going to need and you also have the options for Mac with Intel or Mac with apple silicon so you can toggle between those and basically just choose the guide that matches your computer specifications and once you have that check the system requirements go ahead and click on one of those in my case I have mac with Intel chip so I would click on this one and that's actually the docker desktop installer so if I click it's going to download this DMG image and once it's downloaded you basically just follow the steps described here right you double click on it open the application and so on and same for Windows if your windows you basically click on this one and download Docker desktop for Windows and make sure to check the system requirements and kind of prepare everything you need for starting Docker generally for latest versions of Windows Mac or whatever operating system it should be pretty easy and straightforward to install Docker so go ahead and do that once you're done with installation you can simply start the service by searching Docker and if I click on it you will see right here that it's actually starting up Docker service for Docker engine and there you go it's running and this view here that you're seeing this window is actually the graphical user interface of Docker that I mentioned so that's the client that you can use to interact with the docker engine so you have a list of containers running currently so there's no list same with images if I switched images I have cleaned up my environment so I'm starting with scratch with empty State just like you so we're ready to start using Docker But first you may be wondering what are images and that's what I'm gonna explain next because it's a very important Concept in docker now mentioned that Docker allows to package the application with its environment configuration in this package that you can share and distribute easily so just like an application artifact file like when we create a zip or tar file or a jar file which you can upload to a artifact storage and then download on the server or locally whenever you need it and then package or artifact that we produce with Docker is called a Docker image so it's basically an application artifact but different from jar file or from other application artifacts it not only has the compiled application code inside but additionally has information about the environment configuration it has the operating system application layer as I mentioned plus the tools like node npm or Java runtime installed on that depending on what programming language your application was written in for example you have a JavaScript application you would need node.js and npm to run your application right so in the docker image you would actually have node and npm installed already you can also add environment variables that your application needs for example you can create directories you can create files or any other environment configuration whatever you need around your application so all of the information is packaged in the docker image together with the application code and that's the great advantage of Docker that we talked about and as I said the package is called an image so if that's an image what is a container then well we need to start that application package somewhere right so when we take that package or image and download it to server or your local computer laptop we want to run it on that computer the application has to actually run and when we run that image on an operating system and the application inside starts in the pre-configured environment that gives us a container so a running instance of an image is a container so a container is basically a running instance of an image and from the same image from one image you can run multiple containers which is a legitimate use case if you need to run multiple instances of this same application for increased performance for example and that's exactly what we were seeing here so we have the images these are the application packages basically and then from those images we can start containers which we will see listed right here which are running instances of those images and I also said that in addition to the graphical user interface we get a command line interface client Docker client that can talk to Docker engine and since we installed Docker desktop we should have that Docker CLI also available locally which means if you open your terminal you should be able to execute Docker commits and Doc recommends we can do anything for example we can check what images we have available locally so if I do Docker images that will give me a list of images that I have locally which in this case I don't have any which we saw in the graphical user interface and I can also check the containers using a command docker occur PS and again I don't have any running containers yet now before moving on I want to give a shout out to Ned Hopper net Hopper's Cloud platform called kubernetes application operations offers an easy way for devops teams to deliver manage upgrade connect secure and monitor applications in one or more kubernetes clusters with this platform they basically create this virtual Network layer that connects multiple environments for example if you have multiple Cloud platforms and multiple kubernetes clusters even your own on-premise data center where your application gets deployed you can connect all these in one virtual Network so you can deploy and operate your kubernetes workloads as if it was in one cluster or one infrastructure environment and the GitHub Centric approach they use offers the visibility to know who did what and when for both your infrastructure and application so with net Hopper Enterprises can automate their operations and instead of building an own platform devops teams can focus on what matters the most which is releasing more application features faster so check them out you can actually sign up for a free account and take it for a spin to see if net Hopper is the right solution for you now it's clear that we get containers by running images but how do we get images to run containers from let's say we want to run a database container or redis or some log collector service container how do we get their Docker images well that's where Docker Registries come in so there are ready Docker images available online in image storage or registry so basically this is a storage specifically for Docker image type of artifacts and usually the company is developing those services like redis mongodb Etc as well as Docker Community itself will create what's called official images so you know this mongodb image was actually created by mongodb itself or the docker community so you know it's an official verified image from Docker itself and Docker itself offers the biggest Docker registry called Docker Hub where you can find any of these official images and many other images that different companies or individual developers have created and uploaded there so if we search for Docker hub right here you see Docker Hub container image Library and that's how it looks like and you don't actually have to register or sign up on Docker Hub to find those official images so anyone can go on this website and basically browse the container images and here in search bar you can type any service that you're looking for for example redis that I mentioned and if I hit enter you will basically see a list of various radius related images as well as the ready service itself as a Docker image and here you have this batch or label that says Docker official image for example for the reddish image that we are going to choose here you see that it is actually maintained by Docker Community the way it works is that Docker has a dedicated team that is responsible for reviewing and Publishing all content in the docker official images and this team works in the collaboration with the technology creators or maintainers as well as security expert words to create and manage those official Docker images so this way it is ensured that not only the technology creators are involved in the official image creation but also all the docker security best practices and production best practices are also considered in the image creation and that's basically the description page with all the information about how to use this Docker image what it includes Etc and again as I said Docker Hub is the largest Docker image registry so you can find images for any service that you want to use on Docker Hub now of course technology changes and there are updates to Applications those Technologies so you have a new version of redis or mongodb and in that case a new Docker image will be created so images are versioned as well and these are called image tags and on the page of each image you actually have the list of versions or tags of that image listed right here so this is for redis and if I search for postgres for example foreign you will see different image tags for postgres image also listed here so when you're using a technology and you need a specific version you can choose a Docker image that has that version of the technology and there is a special tag that all images have called latest so right here you see this latest tag or here as well in the recent text so latest tag is basically the latest the last image that was built so if you don't specify or choose a version explicitly you basically get the latest image from the docker Hub so now we've seen what images are and where you can get them so now the question is how do we actually get the image from Docker Hub and download it locally on our computer so we can start a container from that image so first we locate the image that we want to run as a container locally for our demo I'm going to use an nginx image so go ahead and search for nginx which is basically a simple web server and it has a UI so we will be able to access our container from the browser to validate the container has started successfully that's why I'm choosing nginx and here you have a bunch of image tags that you can choose from so the second step after locating the image is to pick a specific image tag and note that selecting a specific version of image is the best practice in most cases and let's say we choose version 1.23 so we're choosing this tag right here and to download an image we go back to our terminal and we execute docker pull comment and we specify the name of the image which is nginx so you have that whole command here as well so that's basically the name of the image that you have written here so that's nginx and then we specify the image tag by separating it with a column and then the version 1.23 that's what we chose that's the whole command so Docker client will contact Docker Hub and it will say I want to grab the nginx image with this specific tag and download it locally so let's execute and here we see that it's pulling the image from the image registry Docker Hub and the reason why we don't have to tell Docker to find that image on Docker Hub is because Docker Hub is actually the default location where Docker will look for any images that we specify right here so it's automatically configured as a location for downloading the images from and the download happened and now if we execute Docker images command again as we did here we should actually see one image now locally which is nginx with an image tag 1.23 and some other information like the size of the image which is usually in megabytes as I mentioned so we have an image now locally and if we pull an image without any specific tag so we do this basically Docker pull name of the image if I execute this you see that it is pulling the latest image automatically and now if I do Docker images again we're going to see two images of nginx with two different texts right so these are actually two separate images with different versions cool now we have images locally but obviously they're only useful when we run them in a container environment how can we do that also super easy we pick the image we already have available locally with the tag so let's say we want to run this image as a container and we execute Docker run command and with the name of the image and the tag super easy and let's execute and that command actually starts the container based on the image and we know the container started because we see the logs of nginx service starting up inside the container so these are actually container logs that we see in the console so it's launching a couple of scripts and right here we have start worker processes and the container is running so now if I open a new terminal session like this and to Docker PS I should actually see one container this one here in the running container list and we have some information about the container we have the ID we have the image that the container is based on including the tag when it was created and also the name of the container so we have the ID and name of the container this is the name which Docker actually automatically generates and assigns to a container when it's created so it's a random generated name now if I go back here you see that these locks the container logs actually are blocking the terminal so if I want to get the terminal back and do Ctrl C exit the container exits and the process actually dies so now if I do Docker PS you will see that there is no container running but we can start a container in the background without it blocking the terminal by adding a flag called minus D which stands for detached so it detaches the docker process from terminal if I execute this you see that it's not blocking the terminal anymore and instead of showing the logs from nginx starting up inside the container it just locks out the full ID of the container so now if I do Docker PS here in the same terminal I should see that container running again and that's basically the ID or the part of the this full ID string shown here but when we start a container in the background in a detached mode you may still want to see the application logs inside the container so you may want to see how did nginx start up what did it log actually so for that you can use another Docker command called Docker locks with the container ID like this and it will print out the application logs from the container now in order to create the container the nginx container we first pull the image and then we created a container from that image but we can actually save ourselves the pull command and execute run command directly even if the image is not available locally so right now we have these two images available locally but in the docker run command you can actually provide any image that exists on Docker Hub it doesn't necessarily have to exist locally on your computer so you don't have to pull that first so if I go back we can actually choose a different image version let's choose 1.22 Dash Alpine so this image tag which we don't have locally or of course this can be completely different service it doesn't matter so basically any image that we don't have locally you can run directly using Docker run command so what it does is first it will try to locate that image locally and if it doesn't find it it will go to Docker Hub by default and pull the image from there automatically which is very convenient so it does both in one command basically so it downloaded the image with this tag and started the container and now if we do Docker PS we should have two containers running with different nginx versions and remember I said Docker solves the problem of running different versions of the same application at once so that's how simple it is to do that with Docker so we can actually quit this container and now again we have that one nginx container with this version now the important question is how do we access this container well we can't right now because the container is running in the closed Docker Network so we can't access it from our local computer browser for example we need to First expose the container to our local network which may sound a little bit difficult but it's super easy so basically we're going to do what's called a port binding the container is running on some Port right and each application has some standard port on which it's running like nginx application always runs on Port 80 radius runs on Port 6379 so these are standard ports for these applications so that's the port where container is running on and for nginx we see the ports under the list of ports here application is running on Port 80 inside the container so now if I try to access nginx container on this port on Port 80 from the browser and let's try to do that we're eating and hit enter you see that nothing is available on this port on localhost so now we can tell Docker hey you know what bind that container Port 80 to our local host on any port that I tell you on some specific Port like 8080 or 9000 it doesn't actually matter so that I can access the container or whatever is running inside the container as if it was running on my Local Host Port 9000 and we do that with an additional flag when creating a Docker container so what we're going to do is first we're going to stop this container and create a new one so we're going to do Docker stop which basically stops this running container and we're going to create a new container so we're going to do Docker run nginx the same version and we're going to find it in the background in detached mode now we're going to do the port binding with an additional flag minus p and it's super easy we're telling Docker the nginx application Port inside container which is 80. please take that and find that on a host localhost on Port whatever 9000 for example right that's the port I'm choosing so this flag here will actually expose the container to our local network or localhost so these nginx process running in container will be accessible for us on Port 9000. so now if I execute this let's see that container is running and in the port section we see a different value so instead of just having 80 we have this port binding information so if you forgot which Port you chose or if you have 10 different containers with Docker PS you can actually see on which Port each container is accessible on your Local Host so this will be the port so now if I go back to the browser and instead of localhost 80 we're going to type in localhost 9000. and hit enter there you go we have the welcome to nginx page so it means we are actually accessing our application and we can see that in the logs as well Docker locks container ID and there you go this is the log uh that nginx application produced that it got a request from MEC or Mac OS machine Chrome browser so we see that our request actually reached the nginx application running inside the container so that's how easy it is to run a service inside container and then access it locally now as I said you can choose whatever Port you want but it's also pretty much a standard to use the same port on your host machine as the container is using so if I was running a MySQL container which started at Port 3306. I would bind it on localhost 3306. so that's kind of a standard now there's one thing I want to point out here which is that Docker run command actually creates a new container every time it doesn't reuse the container that we created previously which means since we executed Docker run command a couple of times already we should actually have multiple containers on our laptop however if I do Docker PS I only see the running container I don't see the ones that I created but stopped but those containers actually still exist so if I do Docker PS with a Fleck a and execute this gives you actually a list of all containers whether they are running or stopped so this is the active container that is still running and these ones are the stopped ones it even says exited 10 minutes ago six minutes ago whatever so we have four containers with different configuration and previously I showed you Docker stop command which basically stops an actively running container so we can stop this one and now it will show it as a stopped container as well exited one second ago but the same way you can also restart a container that you created before without having to create a new one with Docker run command so for that we have a Docker start and that takes the ID of the container and starts the container again and again you can start multiple containers at once if you want like this and they have two containers running now you saw that we use ID of the container in various Docker commands so to start the container to restart it to check the logs Etc but ID is hard to remember and you have to look it up all the time so as an alternative you can also use container name for all these commands instead of the ID which gets auto-generated by Docker but we can actually rewrite that and we can give our containers a more meaningful names when we create them so we can stop those two containers using the ID or the name like this so these are two different containers one with the ID one with name and we're going to stop both of them there you go now when we create a new container we can actually give it a specific name and there is another flag for that which is dash dash name and then we provide the name that we want to give our container let's say this is a web app so that's what we're going to call our container and let's execute if I do Docker PS you see that the name is not some auto-generated random thing but instead our container is called web app so now we can do Docker locks and name of our container like this now we've learned about Docker Hub which is actually what's called a Public Image registry which means those images that we used are visible and available for public but when a company creates their own images of their own applications of course they don't want it to be available publicly so for that there are what's called private Docker Registries and there are many of them almost all Cloud providers have a service for private Docker registry for example AWS is ECR or elastic container registry service Google Azure they all have their own Docker Registries Nexus which is a popular artifact storage service has Docker registry even Docker Hub has a private Docker registry so on the landing page of Docker Hub you saw this get started form so basically if you want to store your private Docker images on Docker Hub you can actually create a private registry on Docker Hub or even create a public registry and upload your images there so that's why I actually have an account because I have uploaded a couple of images on Docker Hub that my students can download for different courses and there is one more concept I want to mention related to registry which is something called a repository which you also often hear Docker repository Docker registry so what is the difference between them very simply explained AWS ECR is a registry so basically that's a service that provides storage for images and inside that registry you can have multiple repositories for all your different application images so each application gets its own repository and in that repository you can store different image versions or tags of that same application the same way dockerhub is a registry it's a service for storing images and on Docker Hub you can have your public repositories for storing images that will be accessible publicly or you can have private repositories for different applications and again you can have repository dedicated for each application so that's a side note there if you hear these terms and Concepts and you know what is the difference between them now I mentioned that companies would want to create their own custom images for their applications so how does that actually work how can I create my own Docker image for my application and the use case for that is when I'm done with development the application is ready it has some features and we want to release it to the end users so we want to run it on a deployment server and to make the deployment process easier once you deploy our application as a Docker container along with the database and other services that are also going to run as Docker containers so how can we take our created deployed application code and package it into a Docker image for that we need to create a definition of how to build an image from our application and that definition is written in a file called a Docker file so that's how it should be called creating a simple Docker file is very easy and in this part we're going to take a super simple node.js application that I prepared and we're going to write a Docker file for that application to create a Docker image out of it and as I said it's very easy to do so this is the application it is extremely simple I just have one server.js file which basically just starts the application on Port 3000 and then it just says welcome when you access it from the browser and we have one package of Json file which contains this dependency but the express library that we use here to start the application super lean and simple and that's the application from which we're going to create a Docker image and start it as a Docker container so let's go ahead and do that so in the root of the application we're going to create a new file called Docker file so that's the name and you see that most code editors actually detect Docker file and we get this Docker icon here so in this Docker file we're going to write a definition of how the image should be built from this application so what does our application need it needs a node installed because node should run our application right so if I wanted to start this application luckily for my terminal I would execute node SRC so the source folder and server.js command to start the application so we need that node command available inside the image and that's where the concept of Base image comes in so each Docker image is actually based on this base image which is mostly a lightweight Linux operating system image that has the node npm or whatever tool you need for your application installed on top of it so for a JavaScript application you would have node base image if you have Java application we will use an image that has Java runtime installed again Linux operating system with Java installed on top of it and that's the base image and we Define the base image using a directive in Docker file called from we're saying build this image from the base image and if I go back to Docker Hub and search for node you will see that we have an image which has node and npm installed inside and base images are just like other images so basically you can pile and build on top of the images in Docker so they're just like any other image that we saw and they also have text or image versions so we're going to choose node image and a specific version and let's actually go for 19-alpine so that's our base image and our first directive in the docker file so again this will just make sure that when our node.js application starts in a container it will have a node and npm commands available inside to run our application now if we start our application with this command we will see that we get an error because we need to First install dependencies of an application we just have one dependency which is press Library which means we would have to execute npm install command which will check the package.json file read all the dependencies defined inside and install them locally in node modules folder so basically we're mapping the same thing that we would do to run the application locally we're making that inside the container so we would have to run npm install command also inside the container so as I mentioned before most of the docker images are Linux based Alpine is a Linux a lightweight Linux operating system distribution so so in Docker file you can write any Linux commands that you want to execute inside the container and whenever we want to run any command inside the container whether it's a Linux command or node command npm command whatever we executed using a run directive so that's another directive and you see that directives are written in all caps and then comes the command so npm install which will download dependencies inside the container and create a node modules folder inside the container before the application gets started so again think of a container as its own isolated environment it has a simple Linux operating system with node and npm installed and we're executing npm install however we need application code inside the container as well right so we need the server.js inside and we need the package.json because that's what npm command will need to actually read the dependencies and that's another directive where where we take the files from our local computer and we paste them copy them into the container and that's a directive called copy and you can copy individual files like package.json from here into the container and we can say where in container on which location in the file system it should be copied to and let's say it should be copied into a folder called slash app inside the container so this is on our machine right we have package.json here this is inside the container it's a completely isolated system from our local environment so we can copy individual files and we can also copy the complete directories so we also need our application code inside obviously to run the application so we can copy this whole Source directory so we have multiple files inside we can copy the whole directory into the Container again in slash app location and the slash at the end is also very important so the docker knows to create this folder if it doesn't exist in the container yet so the roots of Linux file system app folder inside and then slash so now all the relevant application files like package.json and the whole Source directory are copied into the container on this location the next thing we want to do before we can execute npm install command is to actually change into that directory right so in Linux we have this CD right to change into a directory in order to execute the following commands inside the directory in Docker file we have a directive for that called work dear it's a working directory which is an equivalent of changing into a directory to execute all the following commands in that directory so we can do slash app here so it sets this path as the default location for whatever comes afterwards okay so we're copying everything into the Container then we are setting the working directory or the default directory inside the container and then we're executing npm install again within the container to download all the dependencies that application needs that are defined here and finally we need to run the application right so after npm install the node command should be executed and we learned to execute commands we use the Run directive however if this is the last command in the docker file so something that actually starts the process itself the application inside we have a different directive for that called CMD so that's basically the last command in the docker file and that starts the application and the Syntax for that is the command which is node and the parameter gserver.js so we copied everything into slash app so we have the server.js inside the app directory and we're starting it or running it using node commit that's it that is the complete Docker file which will create a Docker image for our node.js application which we can then start as a container so now we have the definition in Docker file it's time to actually build the image from this definition I'm going to clear this up and without changing to the terminal we can actually reuse this one we can execute a Docker command to build a Docker image which is super easy we just do Docker build then we have a couple of options that we can provide the first one is the name of the image so just like all those images have names right like node release Etc and the text we can also name our image and give it some specific tag and we do that using this Dash T option and we can call our application node app maybe with Dash doesn't matter and we can give it a specific tag like 1.0 for example and the last parameter is the location of dockerfile so we're telling Docker build an image with this name with this tag from the definition in this specific Docker file right so this is a location of Docker file in this case we are in the directory where Docker file is located so it's going to be the current directory so this dot basically refers to the current folder where Docker file is located so now if we execute this as you see Docker is actually building the image from our Docker file and it looks like it succeeded where it started building the image you see those steps those directives that we defined here so we have the first one from directive got executed then we have the copy as a second step then we have copy The Source folder setting work directory and running npm install and then the last one just started the application so now if I do Docker images in addition to those nginx images we downloaded previously from Docker Hub we should actually see the image that we just created this is the node app image with tag 1.0 and some other information so that's our image and now we can start this image and work with it just like we work with any other image downloaded from Docker Hub so we're going to go ahead and run container from this node app image and make sure that the application inside is actually working so we're going to do Docker run node app image with 1.010 and we're going to pass in parameter to start in detach mode and also we want to expose the port right we want to be able to access the application the node application from localhost and we know that the application inside the container will start on Port 3000 because that's what we have defined here so the application itself will be running on Port 3000 so that's inside container and we can bind it to whatever Port we want on localhost and we can do 3000 the same as in the container so this is the host port and this is container port and now if I execute command and do Docker PS we should see our node app running on Port 3000 and now the moment of truth going back to the browser and opening localhost 3000 there is our welcome to my awesome app message from our application and we can even check the logs by grabbing the ID of our not app and doing Docker blocks with the ID and that's the output of our application inside the container so that's how easy it is to take your application package it into a Docker image using Docker file and then run it as a container and finally going back to this graphical user interface client that Docker desktop actually provides us with now we are able to see other containers and images here as well and that's how this UI actually looks like it gives you a pretty good overview of what containers you have which ones are currently running which ones are stopped with their names and so on and you even have some controls here to start a stop container like this or even stop it again restart container deleted whatever and the same way you have a list of images including our own image and you can also create containers directly from here using some controls so I personally prefer the command line interface to interact with Docker but some feel more comfortable using the visual UI so whichever you prefer you can actually choose to work with either now we've learned a lot of basic building blocks of Docker however it's also interesting to see how Docker actually fits in in the complete software development and deployment process with lots of other Technologies as well so in which steps throughout this whole process is Docker relevant so in this final part of the crash course we're gonna see Docker in big picture view of software development life cycle so let's consider a simplified scenario where you're developing a JavaScript application on your laptop right on your local development environment your JavaScript application uses a mongodb database and instead of installing it on your laptop you download a Docker container from the docker hub so you connect your JavaScript application with the mongodb and you start developing so now let's say you developed the application first version of the application locally and now you want to test it or you want to deploy it on the development environment where a tester in your team is gonna test it so you commit your JavaScript application in git or in some other version control system that will trigger a continuous integration a Jenkins build or whatever you have configured and Jenkins build will produce artifacts from your application so first you will build your JavaScript application and then create a Docker image out of that JavaScript artifact right so what happens to this Docker image once it gets created by Jenkins build it gets pushed to a private Docker repository so usually in a company you would have a private repository because you don't want other people to have access to your images so you push it there and now is the next step could be configured on Jenkins or some other scripts or tools that Docker image has to be deployed on a development server so you have a development server that pulls the image from the private repository your JavaScript application image and then pulls the mongodb that your JavaScript application depends on from a Docker Hub and now you have two containers one your custom container and a publicly available mongodb container running on dev server and they talk to each other you have to configure it of course they talk and communicate to each other and run as an app so now if a tester for example or another developer logs in to a Dev server they will be able to test the application so this is a simplified workflow how Docker will work in a real life development process so in a short time we actually learn all the basic building blocks the most important parts of Docker so you understand what images are how to start containers how they work and how to access them as well as how to actually create your own Docker image and run it as a container but if you want to learn more about Docker and practice your skills even more like how to connect your application to a Docker container learn about Docker compose Docker volumes Etc you can actually watch my full Docker tutorial and if you want to learn Docker in the context of devops and really really Master it with things like private Registries using Docker to run Jenkins integrate Docker in cicd pipelines and use it with various other Technologies like terraform ansible Etc you can check out our complete devops bootcamp where you learn all these and much more
F7WMRXLUQRM,2022-05-15T13:25:55Z,GitLab CI/CD Full Course released - CI/CD with Docker | K8s | Microservices!,hi there i am super excited to welcome you to the gitlab ci city course getlab csd has become a very popular platform for creating release pipelines for your application so i created this course to teach you all the concepts and features to build real-life and modern cicd pipelines on gitlab so by the end of this course you'll be able to confidently use gitlab to build real devops pipelines in your projects since it's a cicd platform naturally it means that you need to know how to integrate it with other technologies so in this course i teach you how to use gitlab ci with technologies like docker docker compose kubernetes aws and so on because as an engineer if you get into a project where gitlab ci is used and you need to work with it you need a course where you can learn not only the basics and fundamentals of gitlab cicd itself like in most other courses but you need to know how to actually do real tasks of integrating it with deployment environments container registries using secret data et cetera and building pipelines not only for a simple application but for microservices applications which has its own specifics and challenges but you don't want to just learn to do it in any possible way like an inexperienced engineer just googling things and putting together something that just works instead you want to learn how to do things in the correct way with actual best practices like properly handling the sensitive data reusing pipeline configuration to avoid code duplication using a dedicated user to deploy to kubernetes environment parameterizing and using dynamic values in the pipeline to make them more flexible and reusable and so on if you have taken any of my other courses you know that you always get the best combination of theory and practice you learn every single concept and feature with engaging easy to follow demo projects but before each demo i explain the concept so you really understand exactly why a certain feature or concept is important and what purpose it has and again you learn not only gitlab specific things but also a lot of additional valuable information about docker docker compose kubernetes microservices etc so it's a well-rounded course with a lot of valuable knowledge for you as an engineer throughout the course i make sure to explain and highlight best practices and industry insights were relevant and this is an essential addition to teach you not only how to do stuff in any way but in the correct way making your knowledge even more valuable so equipped with this knowledge by the end of this course you will be absolutely confident that you really understand how gitlab cia city works and know exactly how to apply it in your real projects no knowledge gaps no insecurities so let's dive into the course curriculum to see what you will learn exactly we'll start with the basics of what ci cd is in general and comparison of git lapsity with other cicd tools then we'll go on to learn a very interesting topic of gitlab architecture and how it all works we'll learn about runners and executors and we will actually create and configure our own runners for the gitlab instance we will look at different ways of operating gitlab and see managed versus self-managed gitlab platform once we have the infrastructure set up we will move on to learn the core concepts of gitlab cicd like jobs stages the pipeline syntax in general how to use conditionals regular and secret variables and reusing configuration code to avoid duplication we will learn the concept of artifacts and how to generate test reports as well as share data within the pipeline using the artifacts we will see how workflow rules can be used to define when the pipelines should be triggered and we will also learn about gitlab's built-in docker registry and how to use it in the pipeline after that we'll move on to more advanced topics and learn how to speed up the pipeline using cache how to use the include feature and use gitlab's job templates in your pipeline and how to extract common pipeline code and build a job hierarchy with extends feature to make your pipeline configuration more maintainable and you will learn all these while building complete ci cd pipelines for demo applications in the first demo project we will build a real-life pipeline deploying a node.js application testing the application with simple unit tests and security tests incrementing and dynamically setting version for the docker image building and pushing image to gitlab's docker registry doing a multi-stage deployment to a deployment server with docker compose and promoting from development staging and production environments after that you will work through a demo of building a csd pipeline for a micro services application in a monorepo and then do a demo and compare building cicd pipeline for a polyrepo so you learn how to do both and throughout these demos we will use best practices of extracting and reusing configuration code for multiple services by creating a gitlab ci templates library here we use docker compose to deploy the services but in the final demo we will take this one step further and configure continuous deployment of microservices to a kubernetes cluster so with all this knowledge and practice you will be ready to implement real ci cd pipelines on gitlab at your work we took time and put a lot of effort in making this course as high quality as possible making all the concepts easy to understand and all the demos easy to follow as well as structure the course in a way that it stays engaging and interesting till the end so i really hope you enjoy it so if you want to learn all these check the video description for the link to the course where you get more information and you can get enrolled i'm very excited to welcome you there
8vXoMqWgbQQ,2021-11-11T15:07:56Z,Top 8 Docker Best Practices for using Docker in Production,in this video we're gonna talk about eight best practices for using docker in production docker is obviously a technology that became a standard and a technology that everyone is familiar with however not everyone is using docker according to the best practices so in this video i want to show you eight ways you can use docker in a right way in your projects to improve security optimize the image size and take advantage of some of the useful darker features and also write cleaner and more maintainable docker files the first best practice is to use an official and verified docker image whenever available let's say you are developing a node.js application and want to build it and run it as a docker image instead of taking a base operating system image and installing node.js npm and whatever other tools you need for your application use the official node image for your application this will not only make your docker file cleaner but also let you use an official and verified image which is already built with the best practices okay so we have selected the base image but now when we build our applications image from this docker file it will always use the latest tag of the node image now why is this a problem because this means you might get a different image version as in the previous build and the new image version may break stuff or cause an unexpected behavior so latest tag is basically unpredictable you don't know exactly which image you are getting so instead of a random latest image tag you want to fixate the version and just like you deploy your own application with a specific version you want to use the official image with a specific version and the rule here is the more specific the better this also gives you and your team a transparency to know exactly what version of the base image you're using in your docker file now looking at all the image tags or versions here you see that for node.js there are multiple official images not only with different version numbers but also with different operating system distributions so the question is which one do you choose and that's an important point if the image is based on a full-blown operating system distribution like ubuntu or centos which has a bunch of tools already packaged in the image size will be larger right but you don't need most of these tools in your application images in contrast having smaller images means you need less storage space in image repository as well as on a deployment server and of course you can transfer the images faster when pulling or pushing them from the repository now in addition to the size there is another issue with images based on full-blown operating systems with lots of tools installed inside and that is a security issue because such base images usually contain hundreds of known vulnerabilities and basically create a larger attack surface to your application image and this way you basically end up introducing unnecessary security issues from the beginning to your image in comparison by using smaller images with leaner operating system distributions which only bundle the necessary system tools and libraries you're also minimizing the attack surface and making sure that you build more secure images so the best practice here would be to select an image with a specific version based on a leaner operating system distribution like alpine for example alpine has everything you need to start your application in a container but is much more lightweight and for most of the images that you look on a docker hub you will see a version tag with alpine distribution inside it is one of the most common and popular base images for docker containers so the best practice here is that if your application does not require any specific system utilities or libraries make sure to choose the leaner and smaller images from the selection the next best practice with docker is optimizing caching for image layers when building an image so what are image layers and what does caching and image layer mean it's very simple actually now docker image is built based on a docker file right in docker file each command or instruction creates an image layer let's look at a simple docker file based on a node alpine image so every docker image is made up of layers this means when we use a base image of node alpine like in this example it already has layers because it was already built using its own docker file and you can see that actually in the documentation when you go to the image page in docker hub and click in one of the text you will see the layers that make up the image so this is how the image was built and plus in our docker file on top of that we have a couple of other comments that each will add a new layer to this image and again this is how every docker image is created using this multiple layers and once you build your own application you can also see all the image layers of the final application image on a command line using docker image history command with the image name so this will display you the image layers with the corresponding commands that created this layer okay now what about caching well each layer will get cached by docker so when you rebuild your image if your docker file hasn't changed a docker will just use the cached layers to build the image this of course makes building the image much faster caching is also useful and important when pulling and pushing an image so if i pull a new image version of the same application and let's say two new layers have been edited in the new version the whole image doesn't need to be downloaded only the newly edited layers will be downloaded the rest are already locally cached by docker so they will be reused from the cache so going back to our simple docker file example what we're doing here is that we're copying all the files from the project into the image using the copy command and then we are executing npm install to install all the project dependencies inside now what happens if we make some code changes in our application since we are copying everything into the image this means copy command will be executed again because it needs to copy the change files right all the changes that we made in the code but the next line of npm install is also re-executed even though we didn't change anything in the dependencies now why is that why isn't it used from cache well that's because once a layer changes all the following or downstream layers have to be recreated as well in other words when you change the contents of one line in docker file caches of all the following lines or layers will be busted and invalidated so each layer from that point will be rebuilt however instead we want to take advantage of docker cache and we want things that haven't changed to be reused from cache again giving us an advantage that the image will be built fast so in our case we don't want to rerun npm install dependencies every time some file in the project changes we only want to run it when package.json file contents have changed which includes all the application dependencies so let's restructure the docker file to only copy package.json file and then run npm install and only after that run the copy all the files comment in this case if we add or remove a dependency in package.json file it will be copied and npm install will be executed if we change any other file in project these two layers will be reused from cache so npm install will not get re-executed and you can see that in the output of docker build command as well where you have the information that a layer has been reused from cache or a layer has been rebuilt so the rule here and the best practice is that you should order your commands in the docker file from the least to the most frequently changing commands to take advantage of caching and this way optimize how fast the image gets built before moving on i want to give a shout out to castin who made this video possible castings k10 is the data management platform for kubernetes k10 basically takes off most of the load of doing backup and restore in kubernetes from the cluster administrators it has a very simple ui so it's super easy to work with and has an intelligent logic which does all the heavy lifting for you and with my link you can download k10 for free and get 10 notes free forever to do your kubernetes backups so make sure to check out the link in the video description and now let's continue now usually when we build the image we don't need everything we have in the project to run the application inside we don't need the auto-generated folders like targets or build folder we don't need the readme file etc so how do we exclude such content from ending up in our application image in order to reduce the image size and that's our next best practice to use a dot docker ignore file and it's pretty straightforward we basically just create this docker ignore file we list all the files and folders that we want to be ignored and when building the image docker will look at the contents and ignore anything specified inside but now let's say there are some contents in your project that you need for building the image so during the build process but you don't need them in the final image itself to run the application the way it works is that while you build an image from a docker file many artifacts actually get created which are required only during the build time and this could be development tools and libraries needed for compiling the application or this could be dependencies needed to run unit tests could also be some temporary files and so on if you keep these artifacts in your final image even though they're absolutely unnecessary for running the application it will again result in an increased image size and increased attack surface a specific example for this is a package.json or palm.xml or any other dependencies file which specifies all the dependencies for the project and are needed to install those dependencies however once the dependencies are installed we don't need these files in the image itself to run the application another more interesting use case is when building java based applications for example we need jdk to compile the java source code but jdk is not needed to run the java application itself in addition to that you might be using tools like maven or gradle to build your java application those are also not needed in the final image so how do we separate the build stage from the runtime stage in other words how do we exclude the build dependencies from the image while still having them available while building the image well for that you can use what's called multi-stage builds the multi-stage builds feature allows you to use multiple temporary images during the build process but keep only the latest image as the final artifact let's see how that works this is an example docker file with two build stages the first stage which we call build specified like this is used to build the java application using maven tool and in the second stage which starts from here with directive from tomcat we use the files generated in the previous build stage and copy them in the final image so the final application image is created only in the last stage using these two lines and all the files and tools that we used in the first stage will be discarded once it's completed and also we talked about layers in our case the final two commands of this docker file will create the layers of the final image again all these previous steps will be discarded so this basically helps us separate the build tools and dependencies from what's needed for a runtime and gives us images which have way less dependencies and are much smaller in size now when we create this image and eventually run it as a container which operating system user will be used to start the application inside well by default when a docker file does not specify a user it uses a root user but in reality there is mostly no reason to run containers with root privileges and there is also a bad security practice this basically introduces a security issue because when container starts on the host it will potentially have root access on the docker host so running an application inside the container with a root user will make it easier for an attacker to escalate privileges on the host and basically get hold of the underlying host and its processes not only the container itself especially if the application inside the container is vulnerable to exploitation to avoid this the best practice is to simply create a dedicated user and a dedicated group in the docker image to run the application to create the user in its group you can simply run user add and group ad linux commands like this and once you have that user to also run the application inside the container with that user you can use a directive called user with the username and then start the application conveniently some images already have a generic user bundled in which you can use so you don't have to create a new one for example node.js image already bundles a generic user called node which you can simply use to run the application inside the container finally how do you make sure and validate the image you build has a few or no security vulnerabilities and so the next and final best practice is once you build the image to scan it for security vulnerabilities using the docker scan command here note that you have to be logged into docker hub to be able to scan your images so you can do a simple docker login on your command line and then execute docker scan command with image name as a parameter and in the background docker actually uses a service called sneak to do the vulnerability scanning of the images the scan uses a database of vulnerabilities which gets constantly updated so new ones get discovered and edit all the time for different images and this is basically an example output of docker scan command where you see the type of vulnerability a url for more information but also what's very useful and interesting you see which version of the relevant library actually fixes that vulnerability so you can update your libraries to get rid of these issues and in addition to scanning your images with docker scan command on a command line interface you can also configure docker hub to scan the images automatically when they get pushed to the repository and you can see the results of that scanning in docker hub or docker desktop and of course you can integrate this check in your ci cd pipeline when building your docker images so these are eight best practices that you can apply today to make your docker images leaner and more secure of course there are many more best practices related to docker but applying these will already give you great results when using docker in production do you know some other best practices which you think are super important and have to be mentioned please share them in the comments for others as well and finally make sure to check out the video description for more learning resources and with that thank you for watching and see you in the next video
7KUdmFyefSA,2020-12-10T16:55:01Z,Kubernetes is dropping Docker support - What does it mean for YOU?,a couple of days ago an update was released that kubernetes will no longer support docker so what does that actually mean why did this change happen and how does that impact you as a developer or devops engineer who is working with docker and kubernetes as you probably already know kubernetes supports multiple container runtimes one of them being docker which is also one of the most popular container tools out there which is used in different projects and docker was also originally the container technology that made containers popular in the first place and also caused need for orchestration tools like kubernetes itself so let's say we deployed docker engine on a kubernetes worker node docker engine actually comes with three components you have docker server then you have the docker api which is basically for interacting with the server and we have the command line interface these are the docker commands that you can execute against the server and docker server component itself has a couple of components and features it has the container runtime which is responsible for starting and stopping containers basically managing the whole life cycle it has the volumes part which is for persisting data in docker it has the network interface for docker containers and also you can build images with docker and the fact is that the only part that kubernetes needs in order to run the containers inside the cluster is the container runtime of docker so kubernetes doesn't actually need all of those features that docker offers because either it has its own features for example for creating volumes or command line interface as well as container network interface or in case of docker feature of building images kubernetes doesn't actually need that because you're not building images inside kubernetes cluster or with kubernetes and for kubernetes to actually directly talk to and use this container runtime component it needs to interact with docker first and for this interaction with docker kubernetes uses docker shim and docker shim is basically part of or has been part of kubernetes code so kubernetes developers have been maintaining and updating dokashim till now and this docker shim this part of the code in kubernetes that talks to docker is actually what kubernetes is deprecating and eventually removing from code and as you see from this diagram it is logical not to deploy the whole docker with all the features that kubernetes doesn't need and instead have just the runtime component so that containers can run in kubernetes cluster this first of all will save a lot of resources like ram and cpu and storage because your installation is much smaller and also it will reduce the security risk because the less components you have the less security risk you are exposed to and the container runtime that docker uses is container d and container d component was actually originally part of the docker daemon code and docker has extracted it as a separate component so that it can be deployed as a standalone container runtime and used in kubernetes cluster and container d is actually now part of cncf and is being developed and maintained as a separate project and container d is the second most popular alternative to using docker as a runtime and in fact container d is already being used by major cloud platforms in kubernetes cluster for example aws eks or google cloud's kubernetes service all use container d already as a container runtime so container d is a mature and also very popular container runtime used in kubernetes clusters another alternative container runtime is cryo which is used by openshift so probably now the most interesting and important question is what does this change mean for you as a kubernetes user or kubernetes administrator now first let's start with kubernetes users and these are mostly developers maybe also devops engineers who are just installing resources on an already existing and configured kubernetes cluster so basically the cluster is running already and you're deploying your applications inside in this case the change or substituting the container runtime underneath kubernetes doesn't actually affect any of your workflows as kubernetes user because that layer is completely abstracted away from you're not interacting with containers so whatever container runtime is running underneath even if it changes you won't probably even notice that the second part is kubernetes administrators or operators and these are actually people who are setting up kubernetes cluster right they're creating them from scratch and maybe configuring some network configuration on kubernetes cluster etc and these are either devops engineers or system administrators or cloud engineers and as i mentioned if you have created your kubernetes cluster on cloud on one of those kubernetes services like on aws or google cloud etc you didn't actually have to install any binaries on the servers you didn't have to install and configure masternodes you didn't have to install container runtime and the kubernetes processes on worker nodes and that means that it is not your responsibility or worry what container runtime is running on those servers so basically the cloud providers who are managing your cluster for you are actually taking care of all that for you which is again one of the advantages of using managed community service because you don't have to worry about installing all these binaries and then updating the versions etc and as i said aws google cloud and many other platforms already use container d as a container runtime now there is one specific case where you as a kubernetes administrator have to take some action and that is if you have installed kubernetes maybe on bare metal or some just virtual machines using cube admin for example so you basically spin up some servers and you installed all those binaries the kubernetes processes container runtime the network interface one by one on those servers in order to create your own kubernetes cluster and this actually might be a case for companies that are using on-premise servers maybe because of some security issues and who are installing the whole kubernetes cluster on those servers from scratch including the container runtime and if it was docker then you have two alternatives first of all you can either substitute the container runtime and use container d or cryo or some other container runtime or as a second option if you for some reason still want to use docker as a container runtime in your cluster mirantis and docker actually announced that they will maintain and support dokoshim as a standalone open source component that you can install separately inside your kubernetes cluster to be able to run docker as a container runtime now in this case when do you need to make this change you won't have to do it immediately obviously because the first step is that docker support just got deprecated so you're just gonna have a warning if you update to the latest version of kubernetes and you will still have some time to make that change from docker to another container runtime so that means that even in this specific scenario there is basically not such a huge change and not an urgent one that you have to do inside your cluster and when kubernetes administrators actually substitute docker with another container runtime developers who are using kubernetes cluster and deploying applications inside probably won't even notice that change there's still two more questions that i want to address the first one being what about running kubernetes locally using mini cube or docker desktop do i have to change anything or does this change actually impact running mini cube cluster or running kubernetes cluster with docker desktop the answer to that is no it doesn't impact at all you as a community's user again will just install these tools and they actually already make the decision about which container runtime is used in the background because you don't have to install that separately it comes with the tool so in this case minicube has support for docker and container d so if it gets removed from mini cube as well you're just gonna install mini cube and you're gonna start using kubernetes cluster without even worrying or knowing what container runtime is running behind one of the frequent questions that i see is should i still learn docker if i'm gonna use kubernetes and what about building docker images in a cicd pipeline well one of the features of docker is actually building images so before the image can run as a container inside kubernetes cluster you first have to build that image right and this is where you will still be using docker in your cicd pipeline to build images from your application let's say we have this complete ci cd pipeline that builds docker images and then deploys to a kubernetes cluster how will this work because we're building docker images we're pushing them to docker repository and how are they going to run in kubernetes cluster where there's no docker installed and the answer to that is very simple every docker image can run in every container runtime so basically you can be building docker images pushing them to docker repository and deploying it to kubernetes cluster that runs container d as a container runtime and it will run perfectly in fact it is already happening if you're using eks and the reason for this compatibility and why all the docker images can run as containers in any container runtime is that these container tools are all compatible with each other because they're all using standards defined by open container initiative open container initiative or oci is basically a project of linux foundation that standardizes how container technologies should work and how they should be implemented and because docker and container d as well as cryo all comply to these open container initiative standards they're actually compatible with each other so you can run docker images on cryo runtime on container d runtime without any problem and that means you will not have to make any changes to your existing infrastructure to your existing ci cd setup even when kubernetes does not use docker as a container runtime so to wrap this up in most of the cases there's no action required especially if you're a kubernetes user as a kubernetes administrator if you have created cluster from scratch by installing everything yourself then either you can substitute docker with another container runtime or just use docker shim as a standalone component
3c-iBn73dDE,2020-10-21T19:26:53Z,Docker Tutorial for Beginners [FULL COURSE in 3 Hours],"hello and welcome to this complete Docker course by the end of this course you'll have a deep understanding of all the main Concepts and also a great big picture overview of how Docker is used in the whole software development process the course is a mix of animated theoretic explanations but also Hands-On demos for you to follow along so get your first hands-on experience and confidence using Docker in your projects so let's quickly go through the topics I'll cover in this course we will start with the basic concepts of what Docker actually is and what problems it solves also we'll understand the difference between Docker and virtual machine and after installing Docker we will go through all the main Docker commands to start and stop containers debug containers Etc after that we'll see how to use Docker in practice by going through a complete workflow with a demo project so first we'll see how to develop locally with containers then we'll run multiple containers or services with Docker compos we'll build our own Docker image with Docker file and we'll push that built image into a private Docker repository on AWS and finally we'll deploy our containerized application last but not least we'll look at how to persist data in Docker learning the different volume types and afterwards configure persistence for our demo project if you get stuck anywhere just comment under the video and I will try my best to answer your questions also you can join the private Tech worldwi community group on Facebook which is there to exchange your knowledge with others and connect with them if you like the course by the end of the video be sure to subscribe to my channel for more related content so let's get started so we'll talk about what a container is and what problems it solves we will also look at a container repository which is basically a storage for containers we'll see how a container can actually make the development process much easier and more efficient and also how they solve some of the problems that we have in the deployment process of applications so let's dive right into it what a container is a container is a way to package applications with everything they need inside of that package including the dependencies and all the configuration necessary and that package is portable just like any other artifact is and that package can be easily shared and moved around between a development team or development and operations Steam and that portability of containers plus everything packaged in one isolated environment gives it some of the advantages that makes development and deployment process more efficient and we'll see some of the examples of how that works in later slides so as I mentioned containers are portable so there must be some kind of a storage for those containers so that you can share them and move them around so containers live in a container repository this is a special type of storage for containers many companies have their own private repositories where they host or the where they store all the containers and this will look something like this where you you can push all of the containers that you have but there is also a public repository for Docker containers where you can browse and probably find any application container that you want so let's head over to the browser and see how that looks like so if I here search for a dockerhub which is the name of the public repository for Tucker I will see this official website so here if you scroll down you see that there are more than 100,000 container images of different applications hosted or stored in this Docker repository so here you see just some of the examples and for every application there's this official Docker container or Docker container image um but if you are looking for something else you can search it here and I see there's an official image for let's say Jenkins uh but there's also a lot of non-official images or container images that developers or or even from Jenkins itself that they actually store it here so public repository is where you usually get started when you're using or when you're starting to use the containers where you can find any application image so now let's see how containers improve the development process by specific examples how did we develop applications before the containers usually when you have a team of developers working on some application you would have to install most of the services on your operating system directly right for example you are developing some JavaScript application and you need a postp ql and you need red for messaging and every developer in the team would then have to go and install the binaries of those services and configure them and run them on their local development environment and depending on which operating system they're using the installation process will look actually different also another thing with installing services like this is that you have multiple steps of installation so you have a couple of commands that you have to execute and the chances of something going wrong and error happening is actually pretty high because of the number of steps required to install each service and this approach or this process of setting up a new environment can actually be pretty tedious depending on how complex your application is for example if you have 10 services that your application is using then you would have to do that 10 times on each operating system environment so now let's see how containers solve some of these problems with containers you actually do not have to install any of the services directly on your operating system because the container is its own isolated operating system layer with Linux based image as we saw in the previous slides you have everything packaged in one isolated environment so you have the posis ql with a specific version packaged with the configuration and the start script inside of one container so as a developer you don't have to go and look for the binaries to download on your machine but rather you just go ahead and check out the container repository to find that specific container and download on your local machine and the download step is just one Docker command which fetches the container and starts it at the same time and regardless of which operating system you're on the command the docker command for starting the container will not be different it will be exactly the same so if you have 10 applications that your JavaScript application uses and depends on you would just have to run 10 Docker commands for each container and that will be it which makes the setting up your local development environment actually much easier and much more efficient than the previous version also as we saw in the demonstration before you can actually have different versions of the same application running on your local environment without having any conflict so now let's see how containers can improve the deployment process before the containers a traditional deployment process will look like this development team will produce artifacts together with a set of instructions of how to actually install and configure those artifacts on the server so you would have a jar file or something similar for your application and in addition you would have some kind of a database service or some other service also with a set of instructions of how to configure and set it up on the server so development team would give those artifacts over to the operations team and the operations team will handle setting up the environment to deploy those applications now the problem with this kind of approach is that first first of all you need to configure everything and install everything directly on the operating system which we saw in the previous example that could actually lead to conflicts with dependency versions and multiple Services running on the same host another problem that could arise from this kind of process is when there is misunderstanding between the development team and operations because everything is in a textual guide as instructions so there could be cases where developers forget to mention some important point about configuration or maybe when operations team misinterpret some of those instructions and when that fails the operations team has to go back to the developers and ask for more details and this could lead to some back and forth communication until the application is successfully deployed on the server with containers this process is actually simplified because now you have the developers and operations working in one team to package the whole configuration dependencies inside the application just as we saw previously and since it's already encapsulated in one single environment and you don't have to configure any of this directly on the server so the only thing you need to do is run a Docker command that pulls that container image that you've stored somewhere in the repos repository and then run it this is of course a simplified version but that makes exactly the problem that we saw on the previous slide much more easier no environmental configuration needed on the server the only thing of course you need to do is you have to install and set up the docker runtime on the server before you will be able to run containers there but that's just onetime effort now that you know what a container concept is let's look at what a container is technically so technically container is made up of images so we have layers of stacked images on top of each other and at the base of most of the containers you would would have a Linux based image which is either Alpine with a specific version or it could be some other Linux distribution and it's important for those base images to be small that's why most of them are actually Alpine because that will make make sure that the containers stay small in size which is one of the advantages of using container so on top of the base image you would have application image and this is a simplified diagram usually you would have these intermediate images that will lead up to the actual application image that is going to run in the container and of course on top of that you will have all this configuration data so now I think it's time to dive into a practical example of how you you can actually use a Docker container and how it looks like when you install it and download it and run it on your local machine so to give you a bit of an idea of how this works let's head over to dockerhub and search for postris ql so here which is a Docker official image I can see some of the versions and let's say say I'm looking specifically for older version I don't know 96 something so I'm going to pull that one so this is a dock repository so that I can actually go ahead and pull the containers from that repository directly and because it's a public repository I don't have to log into it I don't have to provide any authentication credentials or anything I can just get started with simple Docker command without doing or configuring anything to access dockerhub so on my terminal I can just do Docker Pole or I can even do Docker run and then just copy the the image name and if I don't specify any version it will just give me the latest but I want a specific version so I'm just I'm going to go with 9.6 actually just to demonstrate so I can provide the version like this with a column and I can start run so as you see the first line says unable to find image locally so it knows that it has to go to dockerhub and pull it from there and the next line says pulling from library postgress and here you see a lot of hashes that says downloading and the this is what I mentioned earlier which is Docker containers or any containers are made up of layers right you have the Linux image layer you have the application layers and so on so what what you see here are actually all those layers that are separately downloading from the dockerhub on my machine right and the advantage of splitting those applications and layers is that actually for example if the image changes or I have to download a newer version of pogress what happens is that the layers they're the same between those two applications two versions of posis will not be downloaded again but only those layers that are different so for example now it's going to take around 10 or 15 minutes to download this one image because I don't have any pogress locally but if I were to download the next version it will take a little bit less time because some layers already exist on my local machine so now you see that it's already logging because it this command that I ran here the docker run with the container name and version it fetches or it pulls the the container but it also starts it so it executes the start script right away as soon as it downloads it and here you see the output of the starting of the application so it just gives some output about um starting the server and doing some configuration stuff and here you see database system is ready to accept connections and launcher started so now let's open the new tab and see with Docker PS command you can actually see all the running containers so here you see that postgress 96 is running and it actually says image so this is another important thing to understand when we're talking about containers there are two technical terms image and a container and a lot of people confuse those two I think and there is actually a very easy distinction between the two so image is the actual package that we saw in one of those previous slides so the application package together with the configuration and the dependencies and all these things this is actually the artifact that is movable around is is actually the image container is when I pull that image on my local machine and I actually started so the application inside actually starts that creates the container environment so if it's not running basically it's an image it's just an artifact that's lying around if I start it and actually run it on my machine it is a container so that is the distinction so here it says the active running containers with a container ID image that it's running from and some entry commands that it executed and some other status information so this means that poql is now running on my local machine simple as that if I were now to uh need let's say another version of pogress to run at the same time on my local machine I could just go ahead and say let's go back and let's say I want want to have 9.6 and 10.10 running at the same time on my local machine I just do run postgress and run again it doesn't find it locally so it pushes and this is what I actually explained to you earlier because it's the same application but with just a different version version some of the layers of the image are the same so I don't have to fetch those again because they are already on my machine and it just fetches the layers that are different so that saves a little bit of uh time and I think it's it could be actually good Advantage so now we'll wait for other image layers to load so that we have the second postgress version running and now you see I have postgress 9.6 running in this uh command line tab and I have postgress version 10.10 running in the next one so I have two postes with different versions running and I can actually output them here have both of them running and there's no conflict between those two like I can actually run any number of applications with different versions maybe of the same application with no problem at all and we going to go through how to use those containers in your application and the port configuration and some of the other configuration stuff later in this tutorial when we do a deep dive but this is just for you to get the first visual image of how Docker containers actually work how they look like and how easily you can actually start them on your local machine without having to implement lement a specific version of postgress application and do all the configuration yourself when I first started learning Docker after understanding some of the main Concepts my first question was okay so what is the difference between Docker and an Oracle uh virtual box for example and the difference is quite simple I think and in this short video I'm I'm going to cover exactly that and I'm going to show you the difference by explaining how docu works on an operating system level and then comparing it to how virtual machine works so let's get started in order to understand how Docker works on the operating system level let's first look at how operating system is made up so operating systems have two layers operating system kernel and the applications layer so so as you see in this diagram the kernel is the part that communicates with the hardware components like CPU and memory Etc and the applications run on the Kernel layer so they are based on the Kernel uh so for example we you all know Linux operating system and there are lots of distributions of Linux out there there's buonto and Debian and there's Linux Mint Etc there are hundreds of distributions they all look different so the graphical user the interface is different the file system is maybe different so a lot of applications that you use are different because even though they use the same Linux kernel they use different or they Implement different applications on top of the kernel so as you know Docker and virtual machine they're both virtualization tools so the question here is what parts of the operating system they virtualize so docker virtualizes the applications layer so when you download a Docker image it actually contains the applications layer of the operating system and some other applications installed on top of it and it uses the kernel of the host because it doesn't have its own kernel the virtual box or the virtual machine on the other hand has the applications layer and its own kernel so it virtualizes the complete operating system which means that when you download a virtual machine image on your host it doesn't use your host kernel it boots up its own so what does this difference between Docker and virtual machine actually mean so first of all the size of Docker images are much smaller because they just have to implement one layer so Docker images are usually couple of megabytes uh virtual machine images on the other hand can be couple of gigabytes large a second one is the speed so you can run and start Docker containers much faster than the VMS because they every time you start them you have they have to put the operating system kernel and the applications on top of it the third difference is compatibility so you can run a virtual machine image of any operating system on any other operating system host but you can't do that with Docker so what is the problem exactly let's say you have a Windows operating system with a kernel and some applications and you want to run a Linux based Docker image on that Windows host the problem here is that a Linux based Docker image might not be compatiable with the windows kernel and this is actually true for the windows versions below 10 and also for the older Mac versions which if you have seen how to install Docker on different operating systems you see that the first step is to check whether your host can actually run Docker natively which basically means is the kernel compatible with the docker images so in that case a workaround is that you install a technology called Docker toolbox which abstracts away the kernel to make it possible for your host to run different Docker images so in this video I will show you how to install Docker on different operating systems the installation will differ not only based on the operating system but also the version of that operating system so you can actually watch this video selectively depending on which OS and the version of that OS you have I will show you how to find out which installation step applies to you in the before installing section which is the first one so once you find that out you can actually directly skip to that part of the video where I explained that into details I will put the minute locations of each part in the description part of the video and also I will put all the links that I use in the video in the description um so that you can easily access them also if you have any questions during the video or if you get stuck installing the docker on your system please post your question or problem in the comment section so that I can um get back to you and help you proceed or maybe someone from the community will uh so with that said let's let's dive right into it so if you want to install Docker you can actually Google it and you get an official documentation of Docker um it's important to note that there are two additions of Docker there is a community and Enterprise additions um for us to begin with Community Edition will be just fine in the docker Community Edition uh tab there there's a list of operating systems and distributions in order to install docker so for example if we start with Mac we can click in here and we see the documentation of how to install it on Mech which is actually one of the easiest but we'll see some other ones as well so before you install Docker on your Mac or Windows computer there are prerequisites to be considered so for mac and windows there has to be some criteria of the operating system and the hardware met in order to support running Docker if you have Mech go through the system requirements to see if your U Mech version is actually supporting Docker if you have Windows then you can go to the windows Tab and look at the system requirements there or what to know before you install for example one thing to note is that Docker natively runs only on Windows 10 so if you have a Windows version Which is less than 10 then Docker cannot run natively on your computer so if your computer doesn't meet the requirements to run Docker there is a workaround for that which is called Docker toolbox instead of Docker you basically just have to install a Docker toolbox that will become a sort of a bridge between your operating system and the docker and that will enable you to run Docker on your leg Legacy computer so if that applies to you then skip ahead in this video to the part where I explain how to install Docker toolbox on Mac and on windows so let's install Docker for Mac as you see here there are two um channels that you can download the binaries from or the application from we will go with the stable Channel and other things to can see if you have an older version of Mech either software or the hardware please go through the system requirements to see if you can actually install Docker so here there is a detailed description of what make version you need um to be able to run Docker and also you need at least four gab of RAM and by installing Docker you will actually have the whole package of it which is a Docker engine uh which is important or which is necessary to run the docker containers on your laptop the docker command line client which will enable you to execute some Docker commands Docker compose if you don't know it yet don't worry about it but it's just technology to orchestrate if you have multiple containers um and some other stuff that we're not going to need in this tutorial but you will have everything in a package installed so go ahead and download the stable version well I already have Docker installed from The Edge channel so I won't be installing it again but it shouldn't matter because the steps of installation are the same for both so once the docker DMG file is downloaded you just double click on it and it will pop up this window just drag the docker whale app into the applications and it will be installed on your Mach as the next step you will see Docker installed in your applications so you can just go ahead and start it so as you see the docker sign or icon is starting here if you click on it you see the status that Docker is running and you can configure some preferences and check the docker version and so on and if you want to stop Docker or quit it on your Mech you can just do it from here um an important maybe interesting note here is that if let's say you download or install Docker and you have uh more than one accounts on your laptop you will actually get some errors or conflicts if you run Docker at the same time or multiple accounts so what I do for example is that if I switch to another account where I'm also going to need Docker I quit it from here and then I start it from other account so that I don't get any errors so that might be something you need to consider if you use multiple accounts so let's see how to install Ducker for Windows the first step as I mentioned before is to go to the before you install section and to see that your operating system and your computer meets all the criteria to run Docker natively so if you're installing Docker for the first time don't worry about most of these parts like Docker toolbox and Docker machine there are two things that are important one is to double check that your windows version is compatible for Docker and the second one is to have virtualization enabled virtualization is by default always enable abled um other than you manually disabled it so if you're unsure then you can check it by going to the task manager performance CPU Tab and here you can see the status of the virtualization so once you have checked that and made sure that these two prerequisites are met then actually you can scroll up and download the windows installer for from the stable Channel once they install installer is downloaded you can just click on it and follow the installation wizard to install Docker for Windows once the installation is completed you have to explicitly start Docker because it's not going to start automatically so for that you can just go and search for the docker for Windows app on your windows just click on it and you will see the docker whale icon um starting and if you click on that icon you can actually see the status that says stalker is now up and running so this is basically it for the installation now let's see how to install Docker on different Linux distributions and this is where things get a little bit more complicated so first of all you see that in this menu on the on the left you see that for different Linux distributions the installation steps will differ but also for example if we just click on auntu for the guide you can see that in the prerequisites section there is also differentiation between the versions of the same Linux distribution and there may be some even more complicated scenarios where the combination of the version of the distribution and the architecture it's running in um also makes some difference into how to set up Docker on that specific environment because of that I can't go through a Docker installation process of every Linux environment because there are just too many combinations so instead what we'll do is just go through a general overview of the steps and configuration process to get Docker running on your Linux environment and you can just adjust it then for your specific setup so these are some general steps to follow in order to install Docker on your Linux Linux environment first of all all you have to go through the operating system requirements part on the relevant Linux distribution um that applies for you a second step in the documentation to is to uninstall old versions however if it's the first time you installing Docker then you don't have to worry about that you also don't have to worry about the supported storage drivers and you can skip ahead to the part of installing Docker Community Edition so for any Linux distribution here the steps will be or the options for installing Docker will be the same so first option is basically to set up a repository and download the docker from and install it from the docker repository um the second option is to install the packages manually however I wouldn't recommend it and I think the documentation doesn't recommend it either because then you'll have to do a lot of steps of the installation and the maintenance of the versions manually so I wouldn't do that the third one is just for the testing purposes it may be enough for the development purposes as well but I would still not do it which is basically just downloading some automated scripts that will install and setup Docker on your Linux environment however again I wouldn't go with it I would actually just do the first option which is just download Lo in the docker from the repository so in order to install Docker using the first option which is downloading it from the docker repositories you have two main steps so the first one is to set up the repository uh which differs a little bit depending on which distribution you have and then install the docker CE from that repository so from abunto and Debian the steps for setting up the repository are generally just updating your app package then setting up an https connection with the repository and adding the docker's official gpg key which only aono and dbn need you don't have to do this um steps for SOS and Fedora they just have to install the required packages and the last step uh for setting up the repository is basically setting up the stable repository of Docker which we saw previously on the overview that there are two channels which is a stable and Edge here you always have to set up the stable repository optionally you can also set up the edge repository but I'll just do uh stable this time and here also something to notice depending on architecture you have to actually set it or you have to set that as a parameter when you set up the repository so if you have for example a different architecture you can use those steps to display the correct command for it and um I guess that applies to other Linux distributions as well like for example here you also have the second tab where you see a separate command for it so these steps should actually um set up the repository so that as a Next Step you can then install the docker C from those repositories so installing Docker from the setup repository is actually pretty straightforward the steps are same for or similar to all the distributions basically just update the app package and then you just say install Docker CE so this command will just download the latest Docker version if you want to install a specific one which you will need to do in a production environment then you can just uh provide a version like like this to just say Docker minus C equals some specific versions and using this command you can actually look up what versions are available in that repository that you just and with this command actually Docker will be installed um on your Linux environment and then you can just verify using PSE sudo Docker run hello world which is this demo image of Docker you can verify that Docker is running and this will start hello world Docker container on your environment so as I mentioned previously for environments um that do not support running Docker natively there is an workaround which is called Docker toolbox so Docker toolbox is basically an installer for Docker environment setup on those systems so this is how to install uh Docker toolbox on your Mac um this is the whole package that comes with the installation of Docker toolbox which is basically the docker command line Docker machine Docker compose basically all the packages that we saw in the native installation and in on top of that you also get the Oracle VM virtual box so in order to install the docker toolbox it's actually pretty straightforward on this website you can go to the toolbox releases where we have all the leas of latest releases you just take the uh latest release and here you see two assets this one is for Windows obviously and you just download the package for mac and once it's downloaded you just click on it and go through the installation wizard leave all the options by default as they are do not change anything and after the installation you can just validate that the installation is successful and you can actually run Docker so so after seeing the installation was successful screen just go and look up in your launch pad dock quick start terminal and once you open it you should be able to run uh Docker commands and you can just try Docker run hello world which should just start up or bring up um this hello world Docker container on your environment so now let's see how to install Docker toolbox on Windows here see that you get the whole package of Docker Technologies with a toolbox which are basically the same package which you get on the uh Native Docker installation and on top of that you get Oracle VM virtual box which is the tool that enables Docker to run on an older system so before you install Docker tool books you have to make sure that you meet some of the preconditions number one you have to make sure your Windows system supports virtualization and that virtualization must be enabled otherwise Docker Docker won't start so depending on which Windows version you have looking up or checking the virtualization status will be different so I just suggest you Google it and look it up of how to find the virtualization status to see that it's enabled once you have that checked also make sure that your Windows operating system is 64 bits so if those two criteria are met then you can go ahead and install the Locker toolbox the place where you see the releases or the release artifacts is toolbox releases link here which I have open so it's basically a list of the releases you just take the latest one which has two artifacts this is the one for Windows you just download this executable file click on it and go through the installation wizard once the installation is completed there are just couple of steps here you can verify that Docker was installed or the toolbox was installed by just looking up the docker quick start terminal on your windows that app must be installed and once you click on it and open it you should be able to run Docker commands in the terminal so the basic Docker command that you can test will be Docker run hello world which will just fetch this basic uh Docker container from the public registry and run it on your computer if that command is successful it means that Docker was successfully installed on your computer and now you can proceed with the tutorial so in this video um I'm going to show you some basic Docker commands at the beginning I'm going to explain what the difference between container and images because that's something a lot of people confuse then very quickly go through version and tag and then show you a demo of how to use the basic Docker commands um commands that will be enough to pull an image locally to start a container to configure a container and even debug the container so with that said let's get started so what is the difference between container and image mostly people use those terms interchangeably but actually there is a fine difference between the two to see theoretically container is just the part of a container runtime so container is the running environment for an image so as you see in this graphic the application image that runs the application could be postgress redis some other application needs let's say a file system where it can save the log files or where it can store some configuration files it also needs some environmental configuration like environmental variables and so on so all of this environmental stuff are provided by container and container also has a Port that is binded to it uh which makes it possible to talk to the application which is running inside of a container and of course it should be noted here that the file system is virtual in container so the container has its own abstraction of an operating system including the file system and the environment which is of course different from the file system and environment of the host machine so in order to see the difference between container and image in action let's head over to the docker Hub and find for example a rice image another thing is that dockerhub all the artifacts that are in the docker Hub are images so we're not talking about containers here all of these things are images Docker official image so we're going to go ahead and pull a rad image out of the doah Hub to my laptop so you see the different layers of the image are downloaded and this will take a couple of minutes so once the download is complete I can check all the existing images on my laptop using Docker images command so I see I have two images radi and postgress with text image IDs and so on another important aspect of images is that they have texts or versions so for example if we go back to the docker Hub each one each image that you look up in the docker Hub uh will have any different versions the latest is always the one that you get when you don't specify the version of course if you have a dependency on a specific version you can actually choose the version you want and specified and you can select one from here so this is what you see here the tag is basically the version of the image so I just downloaded the latest and I can also see the size of the image so now to this point we have only worked with images there is no container involved and there is no redish running so now let's say I need red running so that my application can connect to it I'll have to create a container of that redice image that will make it possible to connect to the redis application and I can do it by running the redis image so if I say Docker Run Red this will actually start the image in a container so as I said before container is a running environment of an image so now if I open a new tab and do Docker PS I will get status of all the running Docker containers so I can see the container redis is running with a container ID based on the image of redis and some other information about it for example the port that it's running on and so on so as you see here the docker run redis command will start the redis container in the terminal um in an attached mode so for example if I were to terminate this with the control C you see that red application stops and the container will be stopped as well so if I do Docker PS again I see that no container is running so there is an option for Docker run command that make makes it able makes it possible to run the container in a detached mode and that is minus D so if I do dock run minus D redis I will just get the ID of the container as an output and the container will stop running so if we check again Docker PS I see the container with the ID starting with 838 which is the same thing here is running so this is how you can start it in the detached mode now for example if you would want to restart a container because I don't know some uh the application crashed inside or some error happened so you want to restart it you would need the doc container ID so just the first part of it not the whole string and you can simply say Docker stop ID of the container and that will stop the docker container nothing running if you want to start it again you can use the same ID to start again so let's say you stop Docker container at the end of the day you go home you come back the next day open your laptop and you want to restart the stopped container right so if you do Docker PS there is uh the output is empty you don't see any containers so what you can do alternative to just looking up your history command line history is you can do Docker PS minus a which will show you all the containers which uh are running or not running so here you see the container ID again and you can restart it okay so let's try another thing let's say you have two parallel applications that both use redish but in different versions so you would need two redish containers with different image versions running on your laptop right at different times maybe or at the same time so here we have the latest one which is radius 56 and let's head over to the dockerhub and select uh version let's say you need version 4 o so remember the first time that we downloaded the redis image we did Docker pull redis um however if you run Docker if you use Docker run with redice image and the tech which was 4.0 it will pull the image and start the container right away after it so it does two commands basically in one so it's Docker pole that Docker start in one one command so if I do this it says it can't find the image locally so it goes and pulls the image from the repository to my laptop and again we see some layers are downloaded and the container is started right away and now if I do Docker PS you see that I have two radices running so this is where it gets interesting now how do you actually use any container that you just started so in this output we you also see the ports section which specifies on which Port the container is listening to the incoming requests so both containers open the same port which is what was specified in the image so in the logs of the container you can see the information running mode stand loone Port 6379 so how does that actually work and how do we not have conflicts while both are running on the same port so to explain that let's head over to our slide and see how this works as you know container is just the virtual environment running on your host and you can have multiple containers running simultaneously on your host which is your laptop PC whatever you working on and your laptop has certain ports available that you can open for certain applications so how it works is that you need to create a so-called binding between a port that your laptop your host machine has and the container so for example in the first container part here you see container is listening on Port 5000 and you bind your laptops Port 5,000 to that containers now you will have conflict if you open two 5,000 ports on your host because you will get a message the port is already bound or is already in use you can do that um however you can have two containers as you see the second and third containers are both listening on Port 3000 which is absolutely okay as long as you bind them to two different ports from your host machine so once the port binding between the host and the container is already done you can actually connect to the running container using the port of the host so in this example URI you would have some app Local Host and then a port of the host and the host then will know how to forward the request to The Container using the port binding so if we head back here you see that containers have their ports and they're both running on the same one however we haven't made any binding between my laptop's port and the container port and because of that the container is basically unreachable by any application so I won't be able to use it so the way we actually do that is by specifying The Binding of the ports during the Run command so I'm going to break this and check that there is just one container running now I'm going to stop the other one as well so we can start the menu okay so we see both containers are here so now we want to start them using The Binding between the host and the container ports but again we have two Rices so we need to bind them to two different ports on my laptop so the way to do it is you do Docker run and you specify with minus P the port of the host that's the first one so let's go with 6,000 it doesn't really matter in this case and the second one is the port that you're binding this two which is the container Port so we know the container Port will be 6379 and this is where we bind our so my laptop's port 60002 and if I do this I forgot ra here so and now if I do Docker PS let's actually clean this Docker PS again here you see The Binding here all right so your laptops 6,000 Port is bound to the containers 6 37 9 so now let's do another thing and let's start it in a detached mode so like this let's check again it's running again and now I want to start the second container let's clear this again so here you see it created a bunch of containers because uh when I specified different options with the port binding it actually created new containers um that's why you see a couple of more here so I'm going to copy the image name with the tag for uh. o minus P so for example if I were to do this now and I would try to run the other redis the second redis container with the same port on my laptop I would get an error saying Port is already allocated so I can do 60001 and run it again I'll run it in detached mode so that I'm see port and if I go go over here and say Docker PS I see that I have two different radi versions running both of them bound to different ports on my laptop and the containers themselves listening to request on the same port so so far we have seen a couple of basic Docker commands we have seen Docker pull which pulls the image from the repository to local environment we also saw run which basically combines Docker pull and Docker start pulls the image if it's not locally available and then starts it right away then we saw Docker start and Docker stop which makes it possible to restart a container if um you made some changes and you want to um create a new version which makes possible to restart a container if you need to um we also saw docu run with options the one option that we saw was D minus D which is detach so you can run the container in DET detached mode so you can use terminal again minus P allows you to bind Port of your host to The Container so very important to remember minus P then comes the port of your host and then comes the port of your um container whatever it might be we also saw doap PS doap PS minus a which basically gives you all the containers no matter if they're running currently or not we also saw Docker images which gives you all the images that you have um locally so for example if after a couple of months you decide to clean up your space and get rid of some stale images you can actually check them check the list and then go through them and uh delete them you can do the same with stale Docker containers that you don't use anymore or you don't need anymore you can also get rid of them so the final part of the docker basic commands are commands for troubleshooting which are very very useful if something goes wrong in the container you want to see the logs of the container or you want to actually get inside of container get the terminal and execute some commands on it so let's see Docker PS we have two containers running right now we don't have any out we don't see any locks here so let's say something um happens your application cannot connect to redies and you don't know what's happening so ideally you would want to see what logs radi container is producing right the way to do that is very easy you just say Docker locks and you specify the container ID and you see the locks you can also do the lock locks if you don't want to uh remember the container ID or to Docker PS all the time you can remember the name of the container and you can get the logs using the name so a little side note here um as we're talking about the names of the containers so here as you see when a container is created you just get some random name like this so you can name your containers as you want um using another option of the docker run which might be pretty useful sometimes if you don't want to work with the container IDs and you just want to remember the names um or if you just want to differentiate between the containers so for example let's create a new container from r4.0 image using a different name that we choose so I'm going to stop this container and I'm going to create a new one from the same image so let's run it in the detached mode Let's Open the port th000 1 2 6 3 7 9 and give the name to the container and let's call since it's the older um version let's call it red older and we need to specify the image so remember this will create a new container since we're running the docker one command again so if we execute this and check again we see the redis 4.0 image based container is created which is um a fresh new you can see in it created and the name is red is older and we can do the same for the other container so that we kind of know which uh container is what so I'll stop this one and I will use the same command here this will be the latest and I will call this radius latest and since find another Port so I'm going to run it and let's see so here I have two containers running now I know R is older R is latest so for example if the older version has some problems I can just do logs R is older and I can get get my locks so another very useful command in debugging is Docker exit so what we can do with Docker exit is we can actually get the terminal of a running container so let's check again we have two containers running and let's say there is some problem with the latest ready latest container and I want to get a terminal of that container and to maybe navigate a directory inside check uh the lock file or maybe check the configuration file or uh print out the environmental variables um whatever so in order to do that we use Docker exit command with minus t which stands for interactive terminal then I specify the container ID and I say in so I get the B and here you see that the the cursor changed so I'm in inside of the container as a root user and here if I say LS okay the data is empty I can also print out in which directory I am I can go to home directory see what's there um so I have my virtual file system inside of a container and here I can um navigate the different directories and I can check stuff I can also print all the environmental variables to see that something is set correctly um and do all kinds of stuff here and this could be really useful if you have a container with some complex configuration or if for example you are running your own application that you wrote in a container H and you have some complex configuration there um or some kind of setup and you want to validate that everything um is correctly set in order to exit the terminal you just do exit and you're out you can also do the same using the name again if you don't want to work with the IDS and you just want to remember the names of the container to make it easier you can do it with the name as well same thing since most of the container images are based on some lightweight Linux distributions you wouldn't have much of their Linux um commands or applications installed here for example you wouldn't have a curl or some other stuff so you were a little bit more limited in that sense so you can execute a lot of stuff from the docker containers for most of the debugging work um it should be actually enough so the final part to review the difference between Docker run and Docker start which might be confusing for some people let's revisit them so basically Docker run is where you create a new container from an image so Docker run will take an image with a specific version or just latest right as option or as an attribute with Docker start you not working with images but rather with containers so for example um as we saw Docker run has a lot of options you specify with minus D and minus P the port binding and then you have this name of the container and all this stuff so basically you tell Docker at the beginning what kind of container with what attributes name and so on to create from a specific image but once the container is created and you can see that using a con uh the command so for example here the last ones that we created and if you stop it and you want to restart it you just need to use the command do start and specify the container ID and when you start it the container will retain all the attributes that we defined when creating the container using Docker run so Docker run is to create a new container Docker start is to restart a stopped container so once you've learned the docker basic concepts and understood how it works uh it's important to see how Docker is actually used in practice so in software development workflow you will know you have this uh classical steps of development and continuous delivery or continuous integration uh and then eventually gets deployed on some environment right could be a test environment develop environment so it's important to see how Docker actually integrates in all those steps so in the next couple of videos I'm going to concentrate exactly on that so we're going to see some overview of the flow and then we're going to zoom in on different parts and see how Docker is actually used in those individual steps so let's consider a simplified scenario where you are developing a JavaScript application on your laptop right on your local development environment your JavaScript application uses a mongodb database and instead of installing it on your laptop you download a Docker container from the docker Hub so you connect your JavaScript application with the mongodb and you start developing so now let's say you develop the application first version of the application locally and now you want to test it or you want to deploy it on the uh development environment where a tester in your team is going to test it so you commit your JavaScript application in git or in some other version control system uh that will trigger a continuous um integration a Jenkins build or whatever you have configured and Jenkins build will produce artifacts from your application so first it will build your JavaScript application and then create a Docker image out of that JavaScript artifact right so what happens to this Docker image once it gets created by Jenkins build it gets pushed to a private Docker repository so usually in a company you would have a private repository because you don't want other people to have access to your image images so you push it there and now as a Next Step could be configured on Jenkins or some other scripts or tools that Docker image has to be deployed on a development server so you have a development server that pulls the image from the private repository your JavaScript application image and then pulls the mongodb that your JavaScript application depends on from a dockerhub and now you have two containers one your custom container and a publicly available mongodb container running on dev server and they talk to each other you have to configure it of course they talk and communicate to each other and run as an app so now if a tester for example or another developer logs in to a Dev server they be they will be able to test the application so this is a simplified workflow how Brer will work in a real life development process in the next videos I'm going to show you Hands-On demo of how to actually do all this in practice so in this video we are going to look at some practical example of how to use Docker in a local development process so what we're going to do is a simple demo of a JavaScript and nodejs application in the back end to simulate the local development process and then we're going to connect it to a Docker container with a mongodb database in it so let's get started so in this video we're going to see how to work with Docker containers When developing applications so the first step will be is we're going to develop a very simple UI backend uh application using JavaScript very simple HTML structure and nodejs in the back end and in order to integrate all of this in the database we are going to use a Docker container of a mongodb database and um also to make working with the mongodb much easier so we don't have to execute commands in in the terminal we're going to deploy a Docker container of a UI which is called the Express where we can see the database structure and all the updates that our application is making in the database so this development setup should give you an idea of um how Docker containers are actually used in development process so I've already prepared some very simple JavaScript application um so in order to see the code basically we have this index HTML that is very simple code and we have some JavaScript here and we're using nodejs backend that just serves that index HTML file and listens on Port 3000 so we have the server running here in the back end and we have the UI that looks like this so basically it's just a user profile page with some user information and user can edit their profile so if I for example change the name here um and if I change the email address and do changes like this I can save my profile and I have my updates here um however if I refresh the page of course the changes will be lost because it's just JavaScript no JS so there is no persistent compon component in this application so in order to have that which is actually how real life applications work you'll know that you need to integrate the application with a database so using that example I will try to Showcase you how you can actually use the docker containers to make the development process Easier by just pulling one of the databases and attaching it or connecting it to the application so in this case we're going to go with the mongodb application and uh in addition to mongodb contain container we're going to also deploy a mongodb UI which is its own container it's called Express where we can manage or see the database insights and updates from our application much easier so now let's see how that all works so in order to get started let's go to dockerhub and find our uh mongodb image so here let's go to  and we have mongodb here actually and the Express which is another dock container that we're going to use for the UI so first let's pull the mongodb official image so I I already have mongodb latest so pulling doesn't take longer on my laptop but you're going to need a couple of seconds probably and the next one we're going to pull is the docker Express which I also have I believe so let's see yes it's also fast so if I check locally I have mongod TB and Express images so the next step is to run both and Express uh containers in order to make the mongod DB database available for our application and also to connect the  Express with the mongod DB container so let's do the connection between those two first in order to do that we have to understand another Docker concept Docker Network so how it works is that Docker creates its isolated Docker Network where the containers are running in so so when I deploy two containers in the same docken network in this case  and Express they can talk to each other using just the container name without Local Host port number Etc just the container name because they're in the same network and the applications that run outside of doer like our nodejs which just runs from node server is going to connect to them from outside or from the host using Local Host and the port number so later when we package our application into its own Docker image what we're going to have is again Docker network with mongodb container Express container and we're going to have a nodejs application which we wrote including the index HTML and JavaScript for front end in its own doc container and it's going to connect to the mongodb and the browser which is running on the host but outside the docker network is going to connect to our JavaScript application again using host name and the port number so Docker by default already provides some networks so if we say Docker Network LS we can already see these autogenerated Docker networks so we have four of them with different names and the drivers we're not going to go into details here but what we're going to do is create its own network for the mongodb and the Express and we're going to call it Network so let's do this right away going to say Docker Network create and we are going to call it Network so now if I do dock Network LS again I see my docken network has been created so now in order to make our m B container and the Express container run in this Network we have to provide this network option when we run the container in the docker run command so let's start with the so we all know that Docker run is the command to start a container from an image right so we have Docker run which is the basic Docker run command however in this case we want to specify a couple of things um as you learned from the previous videos you have to specify something called Port so we need to open a port of mongodb the default Port of mongodb is 27,7 so we will take that Port actually for both host and container so will run at this port inside of a container and we open the same port on the host so that will take care of the port then we will run it in a detach mode in addition to that there are a couple of things that we can specify when starting up the container and these are environmental variables of the mongot TB let's see um in the official image description you actually have couple of documentation about how to use the image which is very helpful to kind of understand what kind of configuration you can uh apply to it here you see some environmental variables so basically on Startup you can Define what the root username and the password will be which is very um handy because we're going to need those two for the express to connect to the and you can also specify the init database we're just going to provide the username and password because we can create the database from the Express UI later so let's do that and the way you can specify the environmental variables you can actually see here as well is by just let's copy this one so here you say environmental variable that's what the minus E Flex stands for and root username we'll say it's admin and another variable which is the password will be just password so in this way we can actually overwrite what the default username and password will be so two more things that we need to configure in this uh command our container name because we're going to need that container name to connect with the Express so we'll call this one  DB let's say and another one we need is the network that we created which was called  Network so in order to make this command a little bit more structured do it on multiple lines so let's see so it's more readable so basically all the options or all these flags that we set um to go one more time through them it it's going to start in detached mode uh we are opening the port on the host um username and password that we want mongodb to use uh in the startup process we're going to rewrite or overwrite the name of the container and this container is going to run in a Network and this should actually start the container okay so if you want to see whether it was successful we can log the container and see what's happening inside so as we see was started and everything actually looks good waiting for connections on Port 27,7 okay so now let's start Express we want Express to connect to the running mongod DB be container on Startup and here we have an example of how to run it and here we have the list of environmental variables that we can configure so let's quickly look at them username password we don't need them however we need the admin username and admin password of the motb this is actually what we overwrote with admin and password so we're going to use them because Express will need some username password to authenticate with the mongodb and to connect it the port is by default the correct one so we don't need to change that um and this is an important part this is the mongodb server right so basically this is the container name that Express will use to connect to the docker and because they running in the same network only because of that this configuration will work if I didn't if I hadn't specify the network then I could have I could specify the name correct name here of the container but it wouldn't work so with that said let's actually create the docker run command for Express as well so let's clear the history and let's start so again we run it in detached mode and let's see what parameters we need so first of all Port let's say what is the default Port that the express runs on that's 80 81 so we'll take that so basically it's going to run on our laptop on Port 8081 the next option would be these two and remember environmental variables need to be specified with minus E and this is the username of mongodb admin which is admin because we specified it when we started the mongodb container this is the password let's set this one as well don't forget the network minus minus net Network we have the name we can also call it Express and let's see what else we might need here yes this is important one um and our container name let's actually see it again toer PS the one running it's called mongodb that's the container name and this is what we need to specify here so I'm going to write this here and finally the image is called Express so I'm just going to copy this one here and that is it so basically with these commands do Express should be able to connect to the mongod Deb container so let's run it and just to make sure let's log the container and see what's happening there waiting for mongodb welcome to  express it looks like it connected successfully um it says here database connected and the Express is available at Port 8081 so let's check the Express out at the Port 881 so actually let's close these tabs we don't need them anymore and here if I say Local Host 881 I should be able to see the manga Express so these are the databases that already exist by default in or which are created on Startup and using the UI we can create our own database as we saw previously we could have specified an environmental variable init DB on mongodb Startup and that would have created a new database however it doesn't matter we will just create a database name here so we will call it user um account database so let's create one and now we can actually use it or connect to this database from node.js so let's see how that works so now we have the mongodb container and the Express container running so let's check that we have both of them we'll have to connect nodejs with the database so the way to do it is usually to give a protocol of the database and the URI and the URI for a mongodb database would be Local Host and the port that it's accessible at I already went ahead and prepared the code for node.js so basically we are going to use a client here which is a node module and using that client we are connecting to the mongodb database so this is the protocol the host and the port that we just saw that the mongodb is listening at and username and password of the root user of mongodb of course usually you wouldn't put the password here or not use an admin or root uh username password to connect to a database but this is for just the demonstration purposes and these are username and password that we set as environmental variables when we created the docker mongodb container so let's check that so this is the mongodb uh container command and this is the username root and root password that we specified and this is what we are going to use in the code as I said for demonstration purposes I will write the password directly here so then we connect to the database um so I also went ahead and in the Express user account database and inside that I created collection which is like a table in my SQL world called users so here I connect to user account database and I query The Collection users and this is a get request so I'm just fetching something from the database and this is update uh request same thing I connect to the database using the same URI and the database name and I update or insert something in the collection so now let's see how all that works so let's head over to the UI so in the users collection there is no data it's empty so we're going to refresh it and edit the data so I'm going to write here some and update and refresh we see that a new insert was made so this is the update profile section here so all this was executed it connected to the mongodb and now we have one entry which is email coding name that we changed so if I'm going to refresh it now I fetched the newly inserted user data in the UI and I displayed it here and also if you want to see what the mongod container actually logs during this process we can actually look at the logs so I'm going to say docks and log using the container ID so let's say if I wanted to see just the last part of it because I want to see what the last activity was I can also let's clear this and I can also do tail so I can just display the the last part of it or if I want it I could also stream the logs so I'll clear this again and I'll say stream the logs so I want have to do dockal logs all the time so if I make a line here for example to Mark the last logs I can refresh it let's make some other changes let's change it to own and save profile so I'm going to see some activity here as well so these connections are new and it also says received client metadata and this is where the nodejs request comes in with the nodejs and its version and at the end of each communication there is an end connection because we end the database connection at the end so we see that also in the logs so if for example something wasn't working properly you could always check them in the logs here so with that I have a fully functional JavaScript nodejs application which has a persistence in the mongodb database and we also have uh UI both of them running in a Docker container so this would be uh somehow an realistic example of how local development using Docker containers would look like so in the last video we created and started two Docker containers mongodb and Mong Express and these are the commands that we used to make it happen right the first we created a network where these two containers can can talk to each other using just the container name and no host Port Etc is necessary for that um and then we actually ran two Docker run commands with all the options and environmental variables Etc set now uh this way of starting containers all the time is a little bit tedious and you don't want to execute these run commands all the time on the command line terminal especially if you have a bunch of Docker containers to run you probably want to automate it or just make it a little bit easier and there's a tool that's that makes running multiple Docker containers with all this configuration much easier than with Docker run commands and that is Docker compose if you already know Docker comp post and you are wondering why is it useful and what it actually does then bear with me in the next slide I'm going to explain that so this is a Docker run command of the mongodb that we executed previously so basically with Docker compose file what we can do is we can take the whole command with its configuration and map it into a file so that we have a structured commands so if you have have let's say 10 Docker containers that you want to run for your application and they all need to talk to each other and interact with each other you can basically write all the Run commands for each container in a structured way in the docker compos and we'll see how that structure actually looks like so on the right side in the docker compos example the first two tags are always there right version three that's the latest version of the compose Docker compose and then we have the services this is where the container list goes so the first one is mongodb and that Maps actually to The Container name right this is going to be a part of container name when Docker creates a container out of this configuration blueprint the next one is actually the image right so we need to know which image that container is going to be built from and of course you can specify a version tag here um next to the name the next one one is Port so we can also specify which ports is going to open first one is on the host and the second one after the colum is on the container so the port mapping is there and of course the environmental variables can be also mapped in the docker compose and this is how actually the structure of Docker compose looks like for one specific commands let's actually see the second container command for Express that we executed it and how to map that so now again we have a Docker run command for Express and let's see how we can map it into a Ducker compose so as I said services will list the containers that we want to create and again names Express will map map to The Container name the next one will be the image again you can add a tag here if you want to be um have a specific one then you have the ports 80 to 8080 and then you have all the environmental variables again under the attribute environment and this is how the docker compose will look like so basically Docker compos is just a structured way to contain very normal common Docker commands and of course it's it's going to be easier for you to edit the the file uh if you want to change some variables s if you want to change the ports or if you want to add some new options to the Run command so to say and maybe you already noticed the network configuration is not there in the docker compost so this Network that we created we don't have to do it in a Docker compose we go to the next slide because we have the same concept here we have containers that will talk to each other using just the container name so what docker compose will do is actually take care of creating a common Network for these containers so we don't have to create the network and specify in which network these containers will run in and we're going to see that in action right away so let's actually create a Docker compost file so I'm going to paste all my contents here and this is exactly what we saw on the slide and I'm going to save it as a yl and we see the highlighting as well be very aware of the indentation they have to be correct so this is the list of all the containers on the same level and then each container has its configuration inside that so now compared to Docker run commands it will be very easy for me to go here and change these environment variables or add some new configuration options Etc so here again for demonstration we actually save the docker compose in the code so it's part of the application code so now that we have a Docker compose file the question is how do I use it or how do I start the containers using that so let's go to the command line and start Docker containers using this Docker compose file so the way to use it is using Docker compose command now if you've installed Docker on your laptop it usually gets installed with the docker compose packaged inside so you should have both Docker and Docker compose commands installed as a package so Docker compos command takes an argument which is the file so I'm going to specify which file I want to execute and in my case it's called yl and and at the end I want to say what I want to do with this file in this case the command is up which will start all the containers which are in the yl so let's actually check before that there there are no containers running so I don't have anything running here and I'm going to start those two containers okay so there are couple of interesting things here in this output so let's scroll all the way up so we've talked about Docker Network and how we created our own network at the beginning to run the containers inside and I said the docker compos takes care of it and here we see the output where it actually created a network called my app default this is the name of the network and it's going to run those two containers these are actually the names of the containers the do compos created this is what we specified and it just added prefix and suffix to it and it created those two containers uh in that Network so if I actually go here and do Docker Network LS I see the my app default is here so that's one important thing another one is that logs of both containers actually mixed because we are starting both at the the same time as you see the Express has to wait for mongod DB to start because it needs to establish a connection so we here see the locks so mongodb is starting we still get connection refus because it's not started uh completely and somewhere here when mongodb is started and listening for connections  Express is able to connect to it so this is something that you can also do with Docker compose uh when you have two containers that where one depends on another one starting you can actually configure this waiting logic in the docker compos okay so now let's see actually that the docker containers are running so we have both of them here you see the container names that Docker compos gave them and one thing here to note is that the Express actually started on Port 8081 inside the container so we can see that here so we are opening a port 8080 on my laptop that actually forwards the request to container at port 8080 one just so that you don't get confused because it was 8080 on the slides so now that we have restarted the containers let's actually check the first one which is Express so it's running on 8080 in the previous example we created a database and the collection which is gone because we restarted the container this is actually another very important concept of containers to understand when you restart a container everything that you configured in that container's application is gone so data is lost so to say there is no data persistence in the containers itself of course that is very inconvenient you want to have some persistence especially when you're working with a database and there is a concept we're going to learn later called volumes uh that makes it possible to have persistency between the container restarts okay so let's actually create the database again because we need it and inside the database we had actually users collection let's create that one as well and that is empty now let's actually start our application and there you go so now if I were to modify this one here and update I should see the updated entry here so the connectivity with mongodb works so now what do I do if I want to stop those containers of course I could go there and say Docker stop and I can provide all the IDS as we did previously or with Docker compose it's actually easier I can do Docker compose again specify the file and instead of up I'm going to say down and that will go through all the containers and shut them all and in addition to remove removing the containers or stopping them removing the containers it also removes the network so the next time we restarted it's going to recreate so let's actually check that the network LS that default my app default Network case G and when I do up see it gets recreated that should give you a good idea of what dock compose is and how to use it the next we're going to build our own Docker image from our node.js JavaScript application so now let's consider the following scenario you have developed an application feature you have tested it and now you're ready to deploy it right to deploy it your application should be packaged into its own Docker container so this means that we are going to build an Docker image from our JavaScript no JS backend application and prepare it to be deployed on some environment to review this diagram that we saw at the beginning of the tutorial so we have developed a JavaScript application we have used the mongodb docker container to use it and now it's time to commit it to the git right so in this case we're going to simulate this steps on the local environment but still I'm going to show you how these steps actually work so after commit you have a continuous integration that runs so the question is what does actually Jenkins do with this application when it builds the application so the JavaScript application using the npm build Etc it packages it then in a Docker image and then pushes it into a Docker repository so we're going to actually simulate what Jenkins does with their application and how it actually packages it into a Docker image on the local environment so I'm going to do all this on my laptop but it's basically the same thing that Jenkins will do and then on later step we can actually push the built image into a Docker repository in order to build a Docker image from an application we basically have to copy the contents of that application into the docker file it could be an artifact that we built in our case we just have three files so we're going to copy them directly in the image and we're going to configure it and in order to do that we're going to use a blueprint for building images which is called a Docker file so let's actually see what is a Docker file and how it actually looks like so as I mentioned Docker file is a blueprint for creating Docker images a syntax of a Docker file is super simple so the first line of every Docker file is from image so whatever image you building you always want to base it on another image in our case we have a JavaScript application with no JS backend so we are going to need node inside of our container so that it can run our node application instead of basing it on a Linux Alpine or some other lower level image because then we would have to install node ourselves on it so we are taking a ready node image and in order to see that let's actually go to dockerhub and search node here and here you see there is a ready node image that we can base our own image from so here we have a lot of different text so we can actually use one specific one or we can just go with the latest if we don't specify any take so what that actually means basing our own image on a node image is that we're going to have node installed inside of our image so when we start a container and we actually get a terminal of the container we can see that node command is available because there's node installed there this is what from node actually gives us so the next one is we can configure environmental variables inside our Docker file now as you know we have already done this in the using the docker on commands or the docker compos so this will be just an alternative to defining environmental variables in a Docker compos for example I would say it's better to define the environmental variables externally in a Docker compos file because if something changes you can actually overwrite it you can change the docker compos file and override it instead of rebuilding the image but this is an option so this n command basically would translate to setting the environment of variables inside of the image environment the next one is run so all these Capital case words that you see from in and run they're basically part of a syntax of a Docker file so using run basically you can execute any kind of Linux commands so you see make directory is a Linux command that creat creates home slome app um directory very important to note here this directory is going to leave inside of a container so when I start a container from this image the slh home/ app directory will be created inside of the container and not on my laptop not on the host so all these commands that you have in Docker file will apply to The Container environment none of them will be affecting my host environment or my laptop environment so with run basically you can execute any Linux commands that you want so that's probably one of the most used ones and we also have a copy command now you would probably ask I can execute a copy command a Linux copy command using run yes you could but the difference here is that as I said all these commands in run for example they apply to they get executed in inside of the container the copy command that you see here it actually uh executes on the host and you see the first parameter is Dot and the second parameter is slome app so source and the Target so I can copy files that I have on my host inside of that container image because if I were to execute run CP Source destination that that command would execute inside of the docker container but I have the files that I want to copy on my host in the last one so from and CMD or command is always part of Docker file what command does is basically executes an entrypoint Linux command so this line with the command actually translates to node server JS so remember here we actually do node server JS so we start a node server with the nodejs this is exactly what it does but inside of the container so once we copy our server JS and other files inside of a container we can then execute node server.js and we are able to do it because we are basing on the Node image that already has node pre-installed and we are going to see all this inaction so another question here what is the difference between run and CMD because I could also say run node server.js the difference again is that CMD is an entry point command so you can have multiple run commands with different Linux commands but CMD is just one and that marks for Docker file that this is the command that you want to execute as an entry point so that basically runs the server and that's it so now let's actually create the docker file and just like the docker compos file Docker file is part of the application code so I'm going to create a new file here and I'm going to paste here the contents so again we're basing off Note image and actually instead of just having the latest node I'm going to specify a node version so I'm going to take 13 minus Alpine so all these that you see here are Texs so I can use any of them as a TCH so I'm going to say 13 minus Alpine like this so this is going to be a specific node image that I'm going to use as my base image let's actually stop here for a moment and take a little bit of a deep dive on this line so since we saw that Docker file is a blueprint for any Docker image that should actually mean that every docker ER image that there is on dockerhub should be built on its own Docker file right so if we actually go to Let's actually look at one of the latest versions which is 13 minus Alpine and let's click inside and as you see this specific image has its own Docker file and here as you see we have the same from that we just saw and this is what this node official image is based of which is a base image Alpine 3.10 right and then we have this environmental variable set and all these Linux commands using run and some other environmental variable and you have this entry point which is a script so you can also execute the whole shell script instead of separate commands and you have this final Command right so you don't have to understand any of this I just want to demonstrate that every image is based of another base image right so in order to actually visually comprehend how this layer stacking works with images let's consider this simplified visualization so our own image that we're building app with a version 1.0 is going to be based on a node image with a specific version that's why we're going to specify from node 13 Alpine and the node 13 Alpine image as we saw in the docker file is based on Alpine base image with a version 3.10 that's why it specifies from Alpine 3.10 so Alpine is a lightweight base image then we install node on top of it and then we install our own application on top of it and basically this is how all the images are built so now let's go back and complete our Docker file so we have the from specified we have the environmental variables specified and in just a second we're going to actually see these commands in action so let's copy that and this is also very important Docker file has to be called exactly like that you can just give it any name it is always called Docker file starting with a capital D and that's it it's a simple text file so just save it like this and here you even see the highlighting and this Docker icon so now that we have a Docker file ready let's see how to actually use it so how do we build an image out of it so in order to build an image using the docker file we have to provide two parameters one is we want to give our image a name in the tag just like all the other images have so we are going to do it using minus t so we are going to call our image my app and we're going to give it a tag of 1.0 the tag could be anything you can even call it actually version one it wouldn't matter so we're going to do 1 point0 and the second required parameter actually is a location of a Docker file because we want to tell Docker here build an image using this Docker file and in this case because we're in the same fold as the docker file we're just going to say current directory when we execute this we're going to see that image is built and this is an ID of the image that was built because I already have node 13 Alpine on my laptop this just use the the one I have lying around locally for you if it's the first time you will actually see that it's pulling node image from the dockerhub so now with the docker images I can actually see that my image is here it says created two days ago I don't know why but anyways so I have the image name which is this one here and I have the name of the image and the tag of the image so if we go back to this diagram that we saw in the review so basically we've got all these steps or we have simulated some of the steps we've built the JavaScript application using a Docker containers and one once the the application is ready let's say we made the commit and we we just simulated what Jenkins server also does so what Jenkins does is actually it takes the docker file that we create so we have to commit the docker file into the repository with the code and Jenkins will then build a Docker image based on the docker file and what is an important Point here is that usually you don't develop long you are in the team so other people might want to have access to that upto-date image of your application that you developed it could be a tester maybe who wants to pull that image and test it locally or you want that image to be deployed on a development server right and in order to do that you have to actually share the image so it is pushed into a Docker repository and from there either people can take it for example a tester maybe want to download the image from there and test it locally or a development server can actually pull it from there so let's actually just run a container I'm just going to say Docker run the image name obviously and a tag like this and in this case I'm not going to specify any other options because we just want to see what's going on inside of the container so I'm just going to run it okay so the problem is that it can't find the server JS file which is actually logical because we are not telling it to look in the correct directory so since we are copying all the resources in this home/ home/ app directory server JS is going to be there as well and this is another topic whenever you adjust a Docker file you have to rebuild an image because the old image cannot be over written so to say so what I'm going to do now is actually I'm going to delete the one that I built so I'm going to I'm going to actually take the image this is how you delete an image but I can delete it because as as it says the docker is used by a stopped container so if I do Docker PS minus a actually let's crap to my app like this I have to first delete the container so this is how you delete a container it's doer RM and once I've deleted the container I can delete an image so the image deletion is RMI like this so if I do images now I see my image isn't there okay so we' have modified the docker file so let's rebuild it now so Docker build again and let's see the image is here so let's start it again so it's my app 1.0 and let's run it and as you see the problem is fixed app listening on Port 3000 so our app is running so this one here I app 1.0 first of all we can see the logs here like this we see that the EP is listening on Port 3000 we know everything is cool to actually just get a little bit more inside let's enter the containers or let's get the terminal the command line terminal of the container and look around there so I'm going to say Docker exit interactive terminal I'm going to specify the container ID in like this and since bin bash doesn't work we can actually try shell so this is something you will also encounter because some containers do not have bash installed so we'll have to connect using bin sh so one of them has to work always so let's see in which directory we are so we are in the root directory and we see our virtual file system there and as you see the cursor changed as well so that means we're inside of a container so now let's actually check some of the stuff so first of all we specified some environmental variables here in the docker file and this means that these environmental variables have to be set inside the docker environment so if we do inv we actually see the mongodb username this one here and mongodb password are set and there are some other environmental variables automatically said we don't care about them so another thing we can check is this directory because remember because with this line we actually created this slome app directory so let's see slome Slash app and as you can see the directory was created and with the next Land We copied everything in the current folder so if we actually go and see reveal in finder so this is where the docker file resides so basically we copied everything that is inside of this directory so all of these into the Container now we don't actually need to have Docker file and Docker compose and uh this other stuff in here because the only thing we need are the JavaScript files or if we build a JavaScript application artifact just the artifact so let's go ahead and improve that so what I'm going to do is I'm going to create an app directory and I'm going to copy just the files that I'm going to need for starting an application inside of a container so I'm going to take those and the images as well so all these are just external ones we don't need them there and images the index HTML file package Jon server JS and node modules are inside of app so what we can do it now is instead of copying the whole directory where where the docker file is I just want to copy all the contents of EP folder so what I'm going to do is I'm going to say copy all the app contents and again because we modified a Docker file we need to recreate the image in order to leave the docker Container Terminal can actually exit so now we are on the host again so if I do Docker images again I have to first delete the container and then image but in order to delete the container I have to first stop it so now I can remove the container and now I can actually remove the image that the container was based on and let's check again so let's ex execute that build command again so now that we have the image built let's actually run it so I'm going to say my app 1.0 and of course I could have executed with a minus D in a detached mode it doesn't matter now and if I do a Docker PS I see my um image container running and now let's actually enter the container G so it and as we learned it was in sh and again we're going to see the home app and here we just have the contents of app directory so no unnecessary Docker file Docker compose Etc files which is actually how it's supposed to be or as I said because I just had a couple of files here I copied all of them but usually if you have this huge application you would want to compress them and package them into an artifact and then copy that artifact into a Docker image container okay but as I said this was just for demonstration purposes because I just wanted to show you um how you can actually start it as a container and how it should to look inside and in this case we improved a couple of things but usually we would start this container from a Docker compose as well together with all the other Docker images that the application uses and it's also doesn't have any ports open so uh this is just for demonstration purposes so in this video we're going to create a private repository for Docker images on AWS ECR there are many more options for Docker Registries among them Nexus and digital ocean so we're going to see how to create a registry there build and tag an image so that we can push them into that repository and in order to push the images into a private repository you first have to log into that repository so let's see how it all works so the first step is to actually create a private repository for Docker it's also called Docker registry in this case we're going to do it on AWS so let's see so I already have an account on AWS so the service that we're going to use is called elastic container registry so ECR doer container registry and because I don't have a repository there yet I am presenting with the screen so in order to create a repository click on get started and here we have a repository name and we're actually going to name it the name of the application that we have so I'm actually going to name it my app this is the domain of the registry from AWS and this is the repository name which is the same as my image name and don't worry about the other stuff right now and just create a repository it's as simple as that now one thing I think specific to Amazon container service is that here you create a Docker repository per image so you don't have a repository where you have where you can actually push multiple images of different applications but rather for each image you have its own repository and you go inside of the repository here it's empty now but what you store in a repository are the different tags or different versions of the same image so this is how the Amazon container service actually works there are other dep Docker Registries that work differently for example you create a repository and you can just throw all of your container images inside of that one repository so I think this is more or less specific for AWS so anyways we have repository which is called my app and let's actually see how we can push the image that we have locally so actually check that once more so we want to push this image here into that repository so how do we do that if you click on this one the view push commands will be highlighted this is different for each registry but basically what you need to do in order to push an image into repository are two things one you have to log in into the private repository because you have to authenticate yourself so if you are pushing from your local laptop or local environment you have to tell that private reposit hey I have access to it this is my credentials if a Docker image is built and pushed from a Jenkins server then you have to give Jenkins credentials to login into the repository so Docker login is always the first step that you need to do so here AWS actually provides a Docker login command for AWS so it doesn't say Docker login but in the background it uses one so I'm going to execute this login command for AWS Docker repository uh so in the background it uses actually Docker login to authenticate so in order to be able to execute this you need to have AWS command line interface and the credentials configured for it so if you don't I'm going to put a link to the guide of how to do that in the description I have configured both of them so I can execute this command and I should be logged in successfully to the docker repository so now I have authenticated myself to the docker repository here so I'm able to push the image that I have locally to that repository but before I do that there is one step I need to do so I've already built my image so that's fine and now I have to tag my image and if this command here looks a little bit too complicated for you or too strange let's actually go and look at image naming Concepts in Docker repositories so this is the naming in Docker Registries this is how it works the first part of the image name the image full name is the registry domain so that is the host Port Etc slash repository or image name and the tag now you may be wondering every time we were pulling an image out of dockerhub we actually never had this complex long name of the image right so when we were pulling an image it looked like this Docker pole 4.2 the thing is with dockerhub we're actually able to pull an image with a short hand without having to specify a registry domain but this command here is actually a shorthand for this command what actually gets executed in the background when we say Docker pole is Docker pole the registry domain so docker.io library is a registry domain then you have the image name and then you have the tag so because we we were working with dockerhub we were able to use a shortcut so to say in a private Registries we can just skip that part because there is no default configuration for it so in our case in AWS ECR what we going to do is we're going to execute Docker pull the full registry domain of the repository this is what we're going to see here and a tag and this is how AWS just generates the docker registry name that's why we see this long image name with the tag here and we have to tag our image like this so let's go back and take a look at our images our image that we built again and under the repository it says my app now the problem is we can just push an image with this name because when we say Docker push my app like this Docker wouldn't know to which repository we're trying to push by default it will actually assume we're trying to push to dockerhub but it's not going to work obviously because we want to push it to AWS so in order to tell Docker you know what I want this image to be pushed to AWS repository with the name my app we have to tag the image so we have to include that information in the name of the image and that is why we have to tag the image tag basically means that we are renaming our image to include the repository domain or address and the name okay and AWS already gives us the command that we can execute we want to use the specific version so I'm going to use 1.0 in both so what this is going to do is it's going to rename this is what tech does my app 1.0 this is what we have locally this is what the name is to this one here so let's execute that and let's see what the outcome is and as you see it took the image that we had it made a copy and renamed it into this one so these two are identical images they're just called in a different way and now when we go back we see the docker push command so basically this thing here is a same as Docker push and name of the image and the take so this push command will tell Docker you know what I want you to take the image with tag 1.0 and push it into a repository at this address so when I execute this command see the push command will actually push those layers of the docker image one by one this is the same thing as when we're pulling it we also pulled the images layer by layer and this is what happens in the reverse Direction when we push it so this is also going to take a little bit great so the push command was complete and we should be able to see that image in the AWS repository now so if I go inside see I have image tag with 1.0 this is our tag here and push the time the digest which is the unique hash of that image and the image URI which is again the name of the image using the the repository address image name or repository name in this case and the tag so now let's say I made some changes in the docker file you know let's say I renamed this home slome to node app like this or what could also lead to need to recreate an image is obviously when I change something in the code right so you know let's say I were to delete this line because I don't want to console log to be in my code and now I have a different version of the application where I have changes in the application so now I want to have those changes in the new Docker image so now let's build a new Docker image out of it so Docker build let's call it my app with a version 1.1 and a path to a Docker file and now I have a second image which is called my app with version 1.1 so now again because I want to push this to repository I have to rename it to include the repository address inside of it so I'm going to do Docker tag the first parameter is the image that I want to rename and the second one is the name of that image a new name so it's going to be the same as the previous one because the repository name and the address is the same remember we have one repository for one image but for different versions so we're building a version 1.1 so it should end up in the same repository so now here we have 1.1 and if I tag that and images I have a second image here so I'm going to copy that and I'm going to do Docker build and do not forget that tag it's important because because it's part of the complete name sorry it's Docker push and now some of the layers that I already pushed are there only the ones that changed are being rep pushed sort of say and also know that I just have to do Docker login once at the beginning and then I can pull and push images uh from this repository as many times as I want so do login is done once so now that is complete let's actually reload this so my repository now has two versions so this is pretty practical if you are for example testing with different versions and you want to have a history of those image Texs if you want to for example test a previous version and I think in AWS the repos each repository has a capacity of hold holding up to 1,000 uh image versions so for example my app here can have thousand different tags or of the same image okay so now going to compare it to the initial diagram that we saw for this complete flow let's actually switch back to it quickly so here what we did is basically simulate how Jenkins would push an image to a Docker repository so whatever we did on our lap top will be the same commands executed on a Docker on the Jenkins server and again Jenkins user or Jenkins server user has to have credentials to the docker repository to execute Docker login depending on the registry or repository configuration will look different and Jenkins needs to tag the image and then push it to the repository and this is how it it's done and the next step of course we need to use that image that is lying now in the repository and we're going to see how it's pulled from that repository and again we're going to do it on the local environment but it's the same thing that a development server or any other environment will actually execute so in this video we're going to see how to deploy an application that we built into a Docker image so after you package your application in a Docker image and save it in the private repository you need to somehow deploy it on a development server or integration server or whatever other environment and we're going to use Docker compose to deploy that application so let's imagine we have logged in to a development server and we want to run our image that we just push the repository so our my app image and the mongodb image uh both the database and the Express on the development server so the my app image will be pulled from private repository of AWS the in the two containers will be pulled from the docker Hub so let's see actually how that would work so usually again you have developed your application you done with it and you have created uh your own Docker image right now in order to start an application on development server you would need all the containers that make up that application environment okay so we have mongodb and Express already so what we are going to do is here we're going to add a new container in the list which is going to be our own image so let's go ahead and copy the image from our repository so let's actually use the 1.0 so again remember we said that this image name is a shortcut for having a docker.io do library SL with like a specific version so instead of that because we are pulling these images from a Docker Hub we can actually skip that repository domain in front of the images but here because we're pulling it from a private repository so if we were to specify our image like this Docker will think that our image resides on dockerhub so we try to pull it from dockerhub and of course it won't find it because we have to tell Docker go and look at this repository with this repository name and this TCH and of course in order to be able to pull this image or the docker composed to be able to pull this image the environment where you execute this Docker compost file has to be logged into a Docker repository so here as the development server has to pull the image from the repository what we would need to do on the development server is actually do a dock login before we execute the docker compose and obviously you don't need a Docker login for Docker Hub those images will be pulled freely okay so the next thing that we have to configure are the ports because obviously want to open the ports if we go back we see that our application runs on Port 3000 so the port of the container or the where the container is listening on is 3,000 and here we can open the port on the host machine so it's going to be 3,000 me to 3,000 we have actually the environment variables inside of the docker file but obviously we could have configured them in the docker compose just like this so it's an alternative so this will be a complete Docker compost file that will be used on the development server to deploy all the all the applications inside so again if we're trying to simulate a development server the first step will be to do the docker login in this case you have this on command for logging into the AWS Repository which I have done already in this terminal so the next step is to have the docker compos file available on this development server because we have to execute the docker compost file because we're simulating here the way I would do it is I'm going to create a yl file in the current directory where I am I'm going to copy this and save so now I have my ml file and now we can start all three containers using Docker compose command minus f up and here we see that app started on 3000 and mongodb and express started as well so let's check again now and here we saw that database is lost every time we recreate a container and of course that's not good and we're going to learn how to preserve the database data between the container restarts using Docker volumes in the later tutorials because this is not an ideal State okay so now that we have database in a collection let's actually refresh and our application works as well let's check awesome so application works let's refresh this one as well and there is actually one thing that I needed to change in the code to connect nodejs with mongodb so let's actually go and look at that these are my handlers you know nodejs where I connect to the mongodb database so the uis are the same and what I changed here is that it was a local host before so instead of Local Host I changed it to mongodb because this actually is a name of the container or of the service that we specify here so this actually leads back to the docker Network and how Docker compos takes care of it is that in the URI or when I connect one application in a Docker container with another one in another Docker container I don't have to use this uh Local Host anymore actually I wouldn't even need to use the port even because I have all that information so the host name and the port number in that configuration so my application will be able to connect to mongodb using the service name and because of that you don't have to specify here a local host and a port number which is actually even more Advantage when you consider using Docker containers to run all of your applic ations because it makes the connectivity between them even more easier and that actually concludes the this uh diagram that we saw previously we have gone through all of the steps where we saw uh how to develop uh a JavaScript application locally with Docker containers then we saw how to build them into an image uh just like a continuous integration build will do it then we push it into a private repository and we simulated a development server where we pulled the image from U private repository and the other images from the dockerhub where we started the whole application setup with our uh own application and the two applications uh using a Docker compose which is how you would deploy an application on a Dev server so that now testers or other developers will be able to um access the development server and actually try out the applic that you just deployed or you can also use it for demos so in this video we're going to learn about Docker volumes in a nutal Docker volumes are used for data persistence in Docker so for example if you have databases or other stateful applications you would want to use Docker volumes for that so what are the specific use cases when you need Docker volumes so a container runs on a host let's say we have a database container and a container has a virtual file system where the data is usually stored but here there is no persistence so if I were to remove the container or stop it and restart the container then the data in this virtual file system is gone and it starts from a fresh state which is obviously not very practical because I want to save the changes that my application is making in the database and that's where I need Docker volumes so what are the docker volumes exactly so on a host we have a physical file system right and the way volumes work is that we plug the physical file system path it could be a folder a directory and we plug it into the containers file system path so in simple terms a directory a folder on a host file system is mounted in into a directory or folder in the virtual file system of Docker so what happens is that when a container writes to its file system it gets replicated or automatically written on the host file system directory and vice versa so if I were to change something on the host file system it automatically appears in the container as well so that's why when a container restarts even if it starts from a fresh state in its own virtual file system it gets the data automatically from the from the host because the data is still there and that's how data is populated on a startup of a container every time you restart now there are different types of Docker volumes and so different ways of creating them usually the way to create Docker volumes is using Docker run command so in the docker run there is an option called minus V and this is where we Define the connection or the reference between the host directory and the container directory and this type of volume definition is called host volume and the main characteristic of this one is that you decide where on the host file system that reference is made so which folder on the host file system you mount into the Container so the second type is where you create a volume just by referencing the container directory so you don't specify which uh directory on the host should be mounted but that's taking care of the docker itself so that directory is first of all automatically created by Docker under the VAR leap Docker volumes so for each container there will be a folder generated that gets mounted automatically to the container and this type of volumes are called Anonymous volumes because you don't have a reference to this automatically generated folder basically you just have to know the path and the third volume type is actually an improvement of the anonymous volumes and it specifies the name of the folder on the host file system and the name is up to you it's just to reference the directory and that type of volumes are called named volumes so in this case compared to Anonymous volumes you H you can actually reference that volume just by name so you don't have to know exactly the path so from these three types the mostly used one and the one that you should be using using in production is actually the named volumes because there are additional benefits to letting Docker actually manage those uh volume directories on the host now they showed how to create Docker volumes using Docker run commands but if you're using Docker compose it's actually the same so this actually shows how to use volume definitions in a Docker compose and this is pretty much the same as in Docker run commands so we have volumes attribute and underneath you define your volume definition just like you would in this minus V option and here we use a named volume so db- data will be the name reference name that you can just think of could be anything and in vly MySQL data is the path in the container then you may have some other containers and at the end so on the same level as the services you would actually list all the volumes that you have defined you def find a list of volumes that you want to mount into the containers so if you were to create volumes for different containers you would list them all here and on the container level then you actually Define under which path that specific volume can be mounted and the benefit of that is that you can actually mount a reference of the same uh folder on a host to more than one containers and that would be beneficial if those containers need to share the data in this case you would Mount the same volume name or reference to two different containers and you can mount them into different path inside of the container even in this video we are going to look at Docker volumes in practice and this is a simple nodejs mongodb application uh that we're going to attach the volume to so that we don't lose the database data every time we restart start the mongodb container so let's head over to the console and I'm going to start the mongodb with the docker compose so this is how the docker compose looks like we're going to start the mongod TB uh container and the Express container so that we have a UI to it so I'm going to execute the docker compost which is going to start mongodb and the Express so when it started I'm going to check that Express is running on port 8080 and here we see just the default databases so these are just created by default on Startup um and we're going to create our own one for the node.js application and inside of that database I'm going to create users collection so these are the prerequisites or these are the things that my node.js applic ation needs so this one here in order to connect to the database might DB this is what we just created ITB and inside of that to the collection called users so let's start the application which is running on Port 3000 so here and this is our app which when I edit something here we'll write the changes to my database now if I were to restart now the mongodb container I would lose all this data so because of that we're going to use named volumes inside of the docker compos file to persist all this data in the mongodb let's head over to dock compose so the first step is to Define what volumes I'm going to be using in any of my containers and I'm going to do that on the services level so here I Define the list of all the volumes that I'm going to need in any of my containers and since we need data persistency for mongod TB we're going to create uh data volume here now this is going to be the name of the volume reference uh but we also need to provide here a driver local so the actual store storage path that we're going to see later once it's created it's is actually created by Docker itself and this is kind of an information additional information for Docker to create that physical storage on a local file system so once we have a name reference to the volume defined we can actually use it in the container so here I'm going to say volumes and here I will Define a mapping between the data volume that we have on our host and the second one will be the path inside of the mongodb container it has to be the path where mongodb explicitly persists its data so for example if you check it out online you see that the default path where mongodb stores its data is data/ dat DB and we can actually check that out so if I say docker s and go inside the container it's minus it I can actually see data DB and here is all the data that mongodb actually holds but this is of course only the container so when the container restarts the data get regenerated so nothing persists here so this is the path inside of the container not on my host that we need to reference in the volumes here so we're attaching our volume on the host to data/ dat DB inside of a container so for example for MySQL it's going to be um VAR leap MySQL for postgress it's also going to be VAR leap postris SQL SL data so each database will have its own so you have to actually find the right one so what this means is that all the data with that we just saw here all of this will be replicated on a container startup on our host on this persistent volume that we defined here and vice versa meaning when a container restarts all the data that is here will be replicated inside of that directory inside of a container so now that we have defined that let's actually restart the docum compose and restart it so once we create the data and I'm going to the collection and let's actually change this one here and update it so we have a data here so now that we have the persistent volume defined if I were to restart all these containers these data should be persisted so in the next restart I should see the database my DB collection and the entry here so let's do that I'm going to do down great so let's check see the database is here the collection is here and the entry has persisted so now let's actually see where the docker volumes are located on our local machine and that actually differs between the operating systems for example on a Windows laptop or a computer uh the path of the docker volume will be at program data Docker SL volumes the program data Docker folder actually contains all the other container information so you would see other folders in this Docker directory besides the volumes on Linux the path is actually /ar leap Docker volumes which is comparable to the windows path so this is where the docker saves all this configuration and the data and on the mech it's also the same one inside of this volumes directory you actually have a list of all the volumes that one or many containers are using and each volume has its own hash which is or which has to be unique and then slore data will actually contain all the files and all the data that is uh persisted let's head over to the command line and actually see um the volumes that we persisted for mongodb now interesting note here is that if I were to go to this path that I just showed you in the presentation which is VAR Le Docker see there is no such directory so that could be a little little bit confusing but the way it works on Mac specifically on Linux you would actually have that path directly on your host but on Mech it's a little bit different so basically what happens is that Docker for Mech application seems to uh actually create a Linux VM uh in the background and store all the docker inform or doer data about the containers and the volumes Etc inside of that vm's storage so if we execute this command here so this is actually the physical storage on my laptop that I have where all the data is stored but if I execute this command I actually get the terminal of that VM and inside here if I look I have a virtual different virtual file system and I can find that path that I showed you here so it's VAR leap Docker see so I have all this Docker information here I have the containers folder and I have volumes folder so this is the one we need sort of that actually go to the volumes and this is a list of volumes that um I have created and this is the one that came from our Docker compose right this is the name of our app this is do this is what Docker compose actually takes as the name we can actually take a look here so when it's creating these containers it depends these name as a prefix and then there is mongodb and our volume has the same pattern it has the prefix and then mongod data this is the name that we defined here so now if we look inside of that mongod data volume directory we see that underscore data this would be the anonymous volumes so basically here you don't have a name reference it's just some random uh unique ID but it's the same kind of directory as this one here the difference being that this one has a name so it's more it's easier to reference it with a name so this is an onymous volume this is a named volume but the content will be used in the same way um so here as you see in this underscore data we have all the data that mongodb uses so this will be where it gets the default databases and the changes that we make through our application inside and if I go inside of the container so remember this volume is attached to mongodb and is replicated inside of the container under path SL dat DB so if we go go inside of the container actually it here PS SL DP we'll see actually the same kind of data here so we have all this index and collection um files just like we did in this one so now whenever we make changes to our application for example we change it to SM whatever and this will make the container update its data and that will Cascade into this volumes directory that we have here so that on the next startup of a container when the SL data/ DB is totally empty it will actually populate this directory with the data from this uh persistent volume so that we will see all the data that we uh created through our application again on Startup and that's how do loer volumes work in order to end that screen session that we just started because exit doesn't work in this case uh somehow on Mac you can actually click on control a k and then just type Y and the session will be closed so when you do screen LS you should see actually it's terminating congratulations you made it till the end I hope you learned a lot and got some valuable Knowledge from this course now that you've learned all about containers and Docker technology you can start building complex applications with tens or even hundreds of containers of course these containers would need to be deployed across multiple servers in a distributed way you can imagine what overhead and headache it would be to manually manage those hundreds of containers so as a Next Step you can learn about container orchestration tools and kubernetes in particular which is the most popular tool to automate this task if you want to learn about kubernetes be sure to check out my tutorials on that topic And subscribe to my channel for more content on modern devops tools also if you want to stay connected you can follow me on social media or join the private Facebook group I would love to see you there so thank you for watching and see you in the next video"
R8_veQiYBjI,2020-10-08T15:03:01Z,GitHub Actions Tutorial - Basic Concepts and CI/CD Pipeline with Docker,in this GitHub actions tutorial we'll go through the following topics first I'm going to explain what GitHub actions actually is and we're going to look at specific developer workflow use cases that you can automate with GitHub actions after that I will explain the basic concepts of GitHub actions including the events and actions and workflow and how GitHub actions actually automates these workflows using these components having understood what GitHub action solves and how it makes it possible I will go through the most common workflow which is CI CD pipeline I will explain shortly why it's not just another CI CD tool or what are the benefits of GitHub actions CI CD pipeline finally we will go through a Hands-On demo where I explain the syntax of GitHub action's workflow file and then we will go through a complete CI pipeline setup with my example Java Gradle project which we will build into a Docker image and push to a private Docker repository on Docker hub so what is GitHub actions GitHub actions is a platform to automate developer workflows so software development workflows many of the tutorials that I've seen seem to convey that GitHub actions is a CI CD platform but as I said it's for automating developer workflows and CI CD pipeline is just one of the many workflows that you can automate with GitHub actions so now we need to understand what are those developer workflows in other words what is that the developers typically do that is so time consuming or air prone or just tedious that it needs automation so let's go through a couple of specific examples now as you probably already know GitHub is a platform for a lot of Open Source projects so a lot of developers who have developed their own libraries for Java or some other programming language they can host their projects on GitHub and make them publicly available is open source projects so that the community can use those projects but also to contribute to those projects so when a team or individual developers who manage those projects can new contributors or things happen inside the repository people creating pull requests people joining in as contributors and so on they have a lot of organizational tasks to manage let's see examples of such tasks let's say you have created a library that makes it easier to work with daytime in Java so it's a Java library that you created and you have some contributors and users of that library and whenever a use of the library sees that a new release of the library has a bug or something isn't more working they can create an issue that something is not working so you have to check that issue you have to sort it is it minor is it major is it something urgent is it something that others may have also reported is it reproducible for example maybe you assign it to one of the contributors or to yourself and so on let's say one of the contributors fixes the issue and creates a pull request so that you can merge it into the next release of that Library so you look at the pull request you review the code you make sure that the issue is not reproducible anymore with the fix and you merge it into the master node so this is going to be part of the next release so to say so you want to release the next version so the people who use the library can upgrade the version where the issue is fixed so after the pull request is merged into the master Branch you want to start a pipeline a build pipeline that will test your code build your artifact and so on you also want to maybe prepare some release notes where you document and what got added in the new version and maybe also adjust the version tag or the version number so all these things are workflow examples of what you have to do as a maintainer of such repository so you can imagine the bigger the project gets and the more contributors you get and the more features and issues they fix and more pull requests they create and the more people use your project the more organizational effort it is going to be so obviously as a developer you don't want to be sitting there doing all these tedious organizational stuff you want to automate as much as possible of those management tasks so that you can also concentrate on programming and developing new features and new functionalities in the project and for that purpose GitHub actions was created so with GitHub actions every time something happens in your repository or to your repository you can configure automatic actions to get executed in response and these things that are happening in your repository or to your repository are events in the GitHub space so someone creating a pull request is an event someone creating an issue is an event someone joining as a contributor is an event or you merging that pull request into the master branch is an event also note that other applications or tools that you may have integrated into your GitHub can also produce such events that you can respond to with automatic actions so when you automate these flows basically the concept is pretty simple you listen to any such events and depending on what event happens you want a certain workflow to execute automatically so every time someone creates an issue that's an event maybe you want to automatically sort that issue maybe label it assign it to respective contributor or maybe assign it to you per default maybe categorize it and also maybe write a script or a test that will try to automatically reproduce the issue and then add some status or comment or something that says reproducible or not reproducible so all these things can be automated with actions so each small task that you automatically trigger on an event is going to be a separate action so writing a comment putting a label on an issue assigning it to someone etc those are actions and this chain of actions or these combination of actions actually make up workflow so now that we understand the basic concepts of GitHub actions let's look at a specific workflow example so obviously not everybody has an open source project on GitHub you can have your own private projects of GitHub for the application that you're developing so the most common workflow you will think of for your repository would be CI CD pipeline you commit your code the build starts it tests your code builds it into an artifact then pushes the artifact in some storage and then deploys the application on a deployment server now why is it a big deal to have just another CI CD tool well the first and obvious Advantage is that if you're already hosting your code on GitHub now you can use the same tool for CI CD pipeline as well you don't have to set up another third-party tool and manage it separately you have the integrated into your code Repository another advantage of GitHub actions that I see is that it makes it really easy to build your pipeline so the setup process of the CI CD pipeline is really easy it is actually meant to be a tool for developers so they made it so that if you have a developers team you don't need an extra devops person who is dedicated to setting up and maintaining that CI CD pipeline in your project so now the question is how did they make it easy or how does it compare to other CI CD tools like Jenkins for example and why is it much easier to set up and manage so you know that when you think about CI CD pipeline one of the most important things is it's integration with different tools so what do I mean by that whether you are developing a node.js application which will be built into a Docker image and then pushed into a Nexus repository and deployed on digitalocean server or you're developing a Java application with Maven you have integration tests to test your application on Linux and windows servers then build it into a Docker image and push it to AWS container registry and deploy it on AWS eks so basically you can have many different combinations of tools that you're using in your development process so you don't want to be sitting there trying to configure your CI CD Pipeline with all these tools like installing Java and Maven and Docker and all these Integrations with Nexus and AWS and so on installing plugins and configuring them instead you want a simpler way of saying hey I need an environment which has node and Docker both available without me installing any of it with a version that I want and the same way I wanted to do the deployment part easily by simply connecting to the Target environment and deploying the application there and that's exactly the advantage that you have when you're using CI CD pipeline in GitHub actions and of course I will show you and explain how this works in the next demo part with my example Java Gradle project which we will build into a Docker image and push to our private Docker Repository so to see all this in action let's go to GitHub in here we can create a test repository call it my project public and that's it so this is my Mt project so to say so whenever you create a new project you have this actions tab integrated into the project that lets you get started with automating one of your workflows so now I can push my local code to the remote Repository so let's go back and refresh it and here I have my Java application which uses Gradle project so let's go to actions so here if we scroll down we see a big list of workflow templates which means you don't have to start writing your workflow file from scratch you can use one of the templates that matches technology your project uses and these are actually grouped in three main categories here we see the deployment workflows to deploy your code to cloud services or using some automation tools and here we have big section of continuous integration workflows and here if you look at the list a lot of options based on what programming language you're using what tools you're using and also combinations of such tools so for example you have Java with Gradle and you also have Java with Maven and so on so you have the build and test workflows as well as publish workflows where you publish your artifact to some repository and that's where I was talking about when I mentioned that different applications use different combination of tools and it's important for Sci CD tool to have an easy integration with many different tools so that it works for different projects all the way down these are the workflow examples that I mentioned at the beginning like greeting someone for example if a contributor joins your project you might want to send an automated greeting message to welcome or labeling your issues and so on and obviously you can make your own workflow with different combination and adjust it so let's create a build workflow for our Java Gradle application and obviously I will choose the Java Gradle workflow template and let's see what the workflow file looks like and see what happens it automatically creates this configuration view in my project or my repository it creates this path dot GitHub workflows and this is the file that basically holds my workflow logic it is written in yaml it's a yaml format and what's great with this list of workflow suggestions is that you get a pre-configured workflow that you may need to adjust just a little bit but most of the stuff is already here so you don't have to start from blank file so let's go through the syntax of this workflow file in detail so that we understand how to write our own workflows so I'm gonna copy this in my editor so we can see better so first of all we have the name of the workflow this is basically for you to describe what the workflow is doing these are the events that I mentioned so every time an event happens or events happen we can trigger a certain workflow so this is a section where we Define events that should trigger the following workflow and I think it's pretty intuitive every time someone pushes to master Branch we want to trigger this workflow or every time a pull request gets created with Master Branch as a Target this workflow will get executed which in this case makes sense because every time something gets pushed into a master or you want to merge something into Master it makes sense to run tests or to test our application to make sure that it's mergeable so to say or that we didn't break something in the master branch so that's pretty straightforward other examples that I mentioned could be creating an issue or a contributor joining this will be all events listed here you have a complete list of such events documented on this page so here you see the list and here you see some more detailed explanation and also the usage and as always I will put all the relevant links for this tutorial in the video description and this is a part that gets executed whenever these events happen so we have jobs these are the names of the job this could be arbitrary just like the name of the workflow so you can name it yourself and job basically groups a set of actions that will be executed right so as I mentioned events trigger a chain of actions or combination of actions and these are defined here so let's look at the first one pretty logical whenever we want to build application or run tests we need to check out the repository or the code first so how does this get executed or what is behind this syntax so the actions path in GitHub is where pre-created or predefined actions are hosted so basically you can assume that everybody who uses a CI CD pipeline in GitHub actions will need to use checkout Command right so instead of letting everybody do that on their own they're creating an action called checkout that people can use so if I go to GitHub slash actions I will see list of repositories that contain all those actions so let's look for our checkout action so these are all these actions are basically repositories let's go inside and here you have action yemo so this is a normal repository with some code in it and we have action EML file here this is basically what checkout action does in the background or the logic that people already wrote so you don't have to write it in your workflow and just reuse it and each action in the repository will have some sort of documentation where you see if you can configure some additional parameters for this checkout action and this here is version of that action so to say because as I said these are simple repositories that are built and released and have versioning so this is our checkout step and whenever you're using an action that is already available either at slash actions or maybe some other community or team has created one you can basically use any such action using the uses attribute so these are the official ones but whenever someone creates an action basically a repository with action yaml file you can use it here using the uses attribute so the second one second step is action called setup Java which is another repository in this actions list and what it does is basically prepares your environment with Java with a version that you defined here and this is the part where I mentioned that you don't have to install or configure anything like in Jenkins for example you would go and configure job version here you just Define that you want to use environment with Java on it so Java version 1.8 will be installed and available there the next one is a command so here you see the difference whenever we are referring to action in repository we use this attribute whenever we're running a command just like a Linux command for example command line command we are using run attribute so this basically just changes the permissions of Gradle file and the next step just calls Gradle build and all of this is done in the same environment so your code gets checked out Java version gets installed and then you call Gradle build in the same environment so obviously for this to work you have to have Java application that is built with cradle and now let's actually go ahead and execute this workflow for our Java project the name of the yaml file is also something that you can decide for yourself we can actually call it CI and start commit let's create a new branch and create a pull request that will be merged into Master branch and here you see that the workflow got triggered because our event matched to what just happened so we created a pull request against must branch and that triggered the workflow this is in progress and if I go into details we're gonna see what is actually happening so the bill completed so let's actually look at the steps that got executed setup job which basically prepares the job environment for executing the workflow here you see for example this action repositories got downloaded so that it can be used here you see the checkout action and you actually see pretty helpful information in all these steps and they also highlight the comment that gets executed so that you can easily see first of all the command and differentiate it from the logs and also see how they kind of interpret your comments for example with the options and flags and environmental variables and so on then we have the setup jdk again these are the commands that are got executed and some log files this is where the build the actual build happened build successful and then we have some post build actions which we didn't Define these are out of the box things get cleaned up so in my opinion for an initial setup of workflow or such a workflow it it's actually pretty straightforward and easy to set up and it's pretty difficult to mess this up so now you may be asking where do all these things get actually executed because you see that the code got checked out then you see some commands got executed Java version got installed and the Gradle build actually happened so where do all these things happen and how do they get executed so the way it works is that workflows on GitHub actions get executed on GitHub servers so it's managed by GitHub you don't need to set up some servers and configure your build tool install some plugins or whatever and prepare it for building the application so GitHub will manage all of this for you the servers will be configured and ready to execute your jobs an important to note here is that whenever you create a new job or whenever you create a new workflow with a set of jobs for every single job a fresh new GitHub server will be prepared or used to execute all those steps inside the job so one job will run on a single server at a time so for example if you have a list of jobs here maybe you have a job that builds the application and then you have another job that publishes Java artifact let's say to a repository so one job will run on one server another job will run on another server by default these jobs will run in parallel but of course in such a case you would want to wait until the first job was successful to execute the publish so here I could have a publish job of course in this case we want to wait for the build job to successfully execute before we publish the artifact so we can override this default parallel execution using needs and we can reference the build that it depends on and then we'll have a set of steps and actions here and another thing that should be noted here as well is this line here runs on so the servers that I mentioned that GitHub makes available for the workflows to run come in three categories so you can choose either Ubuntu Windows or Mac OS so for example if you have an application that you are shipping out to customers that have all three operating systems you can test each release for example or each commit to master you can test that on all three operating systems and the way we do that is using those attributes so we have a strategy a metrics metrics is used basically whenever you want to use multiple operating systems or maybe multiple versions of Java or whatever technology you're using for your application and here I'm going to Define OS options as an array so we have the Ubuntu latest we have Windows latest and we have Mac OS latest and here on runs on we're going to reference at least using metrics dot OS and let's actually try to apply this change so I have merged my pull request so here you see in the master Branch we have this dot GitHub workflows path with the CI EML file inside so now this has become part of the application code so I can adjust it here and let's actually commit straight to master branch and let's see our workflow and here you see three builds are getting executed in parallel on all three operating systems so as next steps we are gonna take that Java artifact file and we're gonna build a Docker image out of it because we live in world with containerization so jar file won't do it so once we have the docker image built we're gonna publish that to a Docker Hub private repository so let's do that first of all I have my Docker repository set up on Docker Hub it is super easy actually just create an account and you get one private repository for free so this is my private repository and I just have two images here of different application and this is where we're gonna push our Java demo image so what we're gonna do is uh let's go back to the editor so we can see the syntax highlighting better so here as a next step I will add a step and let's call it build and push Docker image because that's what we are doing and here we have a choice of either running the commands or using an action so of course we can write here all the commands we need for building and pushing Docker image this will be Docker login with credentials because first we need to log into the repository from GitHub so that it can push the image there um Docker build Docker tag and Docker push two nodes here whenever you want to execute a step with multiple commands so our own command line command so to say you can do that using the pipe syntax so this is a multi-line syntax in yeml so here we would have Docker login and some credentials here then we would have Docker builds Docker tag and so on and another point is that on a Linux Ubuntu machine so this one here we have Docker pre-installed so I don't have to set up environment I can execute Docker commands right away but as an alternative to command as I mentioned we can use an action and since building and pushing a Docker image is a pretty common step something that a lot of projects will be doing we can expect an action to exist or multiple such actions to already exist so what I'm going to do is I'm gonna go and find an action that does exactly that so we can Google and here you see we land on the marketplace of GitHub actions so if I go here I see a bunch of actions and here I can look for different functionalism I may need in my pipeline so we can go with this action and here you see the usage example so what's important with actions is that you have a possibility to pass in the parameters so it basically does all those Docker login Docker build Etc commands but obviously we want to set our own credentials we want to tell which Docker registry it should connect to in this case we have a Docker Hub registry and of course the image name so we can actually pass all those as parameters here you see it supports multiple Docker Registries here we have the docker Hub and here we have the example usage as well so I'm just going to copy that and we can paste it right here so this is the action with the version this has a version four right here and these are the parameters that we can overwrite you also have the description of the parameters and which one does what and what parameters you have available so let's see which ones we need to overwrite the docker registry name for Docker Hub is Docker dot IO we need to set the image repository name and image name so I'm going to go back to my Docker Hub and just copy that so this is the docker ID and this is the repository name and here you see credentials for username and password because as I mentioned GitHub needs to authenticate itself with the private Docker registry because it's obviously secured and we need to provide credentials here and since this yaml file this workflow yaml file is part of the code we can't just put plain text credentials here so we're using placeholders instead so these are referencing secret so where do those Secrets come from these ones actually can be created in the GitHub itself which is pretty convenient way to store all your secrets for your repository so if you go to settings and secrets here you can add secrets that your workflow uses so I'm gonna post my secrets in here so this is the username and this is a docus password the name should be of course what you're referencing here so I'm going to put in the password so these are the username and password that you use to log in in the docker Hub right here so I have my secrets here for my repository so now I can reference those secrets from my workflow files using secrets.the name of that secret and note that the syntax right here is the same as here this is basically yaml Syntax for referencing values and for this example we actually don't need all three operating systems let's go back to Ubuntu also as I mentioned Ubuntu is the one that has Docker pre-installed so this will be basically our step for building and pushing Docker image and I think this is more convenient because I don't have to write out all this Docker commands for building and pushing the image it all happens in the background you can also override the tag name here you can override the location of dockerfi but default is just the current directory we're going to leave this at defaults so let's actually go ahead and execute this step so I'm going to copy the whole file and let's place that and let's start our commit I'm just going to commit it straight to the master branch so here is our new file and if I go to actions here I see that the workflow ran and it completed so let's actually look inside the steps here we have built with Gradle and here we have built-in push Docker images let's actually open this one up seems like everything executed just fine it was successful so if I go to my Docker Hub repository and refresh here I have a new tag of my Java application and this is the default tag that this action gives my Docker image which has a branch name as a prefix and as you saw here in the parameters you can actually override that tag as well with the tech para so that was our continuous integration workflow we built Java artifact we built a Docker image and we pushed it to Docker repository I will make a full course on the GitHub actions for multiple other use cases including deploying the docker image on cloud or kubernetes environment testing and building with node.js application as well and also automating some other workflows so if you're interested stay tuned for that so hope you learned a lot in this video let me know your feedback also what else you would like to see and learn on this channel or if you have any questions in the comment section below thank you for watching and see you in the next video
asIS4KIs40M,2020-05-30T13:55:30Z,Pull Image from Private Docker Registry in Kubernetes cluster | Demo,in this video I will show you how to deploy your own application in covenants cluster so let's look at a common setup when deploying your own application in Co brandies you commit your code get that triggers jenkins bill that packages your application with its environmental configuration into a docker image and this docker image then gets pushed to a docker registry and this can be nexus it could be AWS container registry or some other private docker repository and now that you have your docker image in the private registry the question is how do you get this docker image on your communities cluster for images like MongoDB elastic some of them that I've shown you in other videos there it's pretty straightforward because they are hosted in a public repository like docker hub and anyone can access them so I can pull them on my machine without excess permission but your own application lives in a private repository and needs explicit access from the cluster so how do you pull their application images from private repository on cabinets cluster you do that using two steps the first one is you create secret component in kubernetes that contains access token or credentials to your daughter registry so that it can authenticate with the registry and second is you configure your deployment or your pod to use that secrets using a specific attribute which is called image pool secrets so I'm gonna show you all that in practice and in order to show you the demo I have my environment already set up so first of all I have a docker private repository which is hosted on AWS so this is the container registry and I have one repository there for my app which is a simple node.js application if you want to learn how to set up AWS container registry I have a separate video about that where I demo the whole process of tagging and building the image locally and then pushing it to AWS container registry so there you can basically learn how to get to this state and I will link that video here and in addition I also link the git repository of this my application if you want to use it and inside the repository I have three images with different version text so we're gonna use that for the demo and luckily I have a mini cube cluster setup which is currently empty so we're gonna start from a clean state so the first step as I said is to create a secret component which will have the access token or credentials to this private repository which will allow docker to actually pull that image inside the cluster so the first thing we need to do in order to create this secret is we need to login to this repository and for that there is a docker login command that looks like this so basically with dr. Logan you provides options which is username or password and you have the doctor repository in point there and the third one is password standard in which basically means that you don't type in the password on the command line but you take it from a standard input source this is more secure and I think this is a recent addition or or more recent decision in docker because you don't have your password written in the command line history so I recommend you use this one generally so for AWS if you see the view push commands there is a log in command here actually that gets the login password into a standard input and then you can do doctor login on that but what I'm gonna show you just for demonstration is I'm gonna show you the complete log in command that this will execute and this is the one so you have docker login the user name this is a password and I'm showing you the password because I'm gonna delete the repository afterwards this is the email flag which I believe is deprecated in the new docker version and there is this URL and this is basically the endpoint of my private repository which is this one right here okay so with this command I will be able to log in so if I execute that command so doctor log in with all the credentials I see log in succeeded and what it does in the background is it creates or it generates a convict JSON file that holds the authentication to my private repository so in dot docker directory and user folder you have a configuration and this holds the repository access now there are two ways that this config Jason will store the authentication either you will have the authentication directly here or you had the external credit stored this is more secure because your access token isn't stored in the file but in a credential store and this is the file that we need for the secret so now whenever docker tries to pull the image from my private repository like this image for example it will use those credentials for this private registry to pull that image to authenticate itself and pull that image however there is a small problem with this specific file which is that I am running my cluster in mini cube and mini cube doesn't have access to my creds store because it's running in a virtual box right so it cannot access my max credential store so this is not gonna work so when the docker which is packaged in the mini cube which I explained in another video you can check that out how mini cube is set up some mini cube has its own darker so when docker inside the mini cube tries to pull that image from this private repository it will see the Kratt store anyone be able to access that ok but I showed you this one who demonstrate how dr. login works with credit store so what I'm gonna do now is enter the mini cube so I'm going to SSH into mini cube which goes like this you can do this pretty easy and here I am in the directory of home docker and just check there is no dot docker here so what I'm gonna do is I'm gonna login to my private repository from mini cube doctor directly not from my laptop okay and let's see how that looks like so I'm gonna copy that command like this and by the way mini cube has a recent more recent version of docker that's why it doesn't recognize the email flag because it got deprecated so I'm gonna remove that so you have the same username password and the private repository URL so I'm gonna execute that remember I'm in the mini cube login succeeded and now if I do - eh I see the dot docker directory created and now we can see what's inside the conflict Jason so you see config Jason doesn't have a credential store anymore but it has this is the repository URL it's the same one here and the credentials or the authorization is inside those brackets so right here you have the authorization token which is this whole thing so depending on your setup it might look like this for you or like this but as I said mini cube can't access my credit store on my operating system so that's why I'm logging in directly from the mini cube talker now what we gonna do is we're gonna use that file so this file right here to create the secret for kubernetes cluster and this is a secret configuration so basically I have a secret kind the name of the secret and note the type here so it's darker config Jason which is its own secret type for this specific use case and the dot docker config Jason the value of this attribute will contain the base64 encoded contents of this config JSON file so all this basics before encoded will be assigned as a value here now since I have my cube CTL setup on my laptop not in the mini cube what I'm gonna do is I'm gonna copy that file from inside the mini cube to my host so that I can use it for my cube CTL command so I'm gonna clear this up and I'm gonna secure copied from mini cube and by the way I'm gonna collect all these commands and the configuration files and I'm gonna put them in its repository and link it in the video description so you can check that out if you want to follow along the video so I'm gonna type out the whole command and I'm gonna explain it so what I'm gonna do is I'm gonna secure copy and for that I'm gonna need SSH key which you have in a variable like this so it will be the same for you can directly copy this command and this is the source so we want to copy it from mini cube this is a user inside mini cube and this is the IP address so this is the server IP you can also login on your console and you can see what the value of it is and inside that I want to copy dot docker slash config Jason and I want to copy now its destination I want to copied in my docker config so I'm gonna replace the one that I have and as I mentioned directly having the authorization inside the configuration file is not as secure as having it in credit or so if you are running your kubernetes cluster on some servers you may want to use credential store inside but this is just for mini cube demonstration so I'm gonna execute that and now if I check my own docker config see I have replaced the content now I can go and create a secret from this file so as I said this should be basics different Kodi content so what I'm gonna do is pipe that and base64 encode it and I'm gonna paste it directly in here okay so this is my secret or if you want to spare yourself this you know base64 encoding it on the command line and then copying the contents there you can actually do that with cube CTL command as well right so let's clear that up and I'm gonna show you that keep city away as well so the cube CTO command will be like this keeps it will create secret and this is the secret type generic so let's name of the secret and let's break the line to have it structured from file attribute from which file the secret content should be created and this is the same attribute that we have in the secret file docker config Jason this is the same as this one here and now we have to specify the file which it should base64 encode and finally we specify the type which is darker config Jason type so it's gonna create the secret of docker config Jason type from these files contents and set it as a valley for these attributes so I'm gonna apply that so this will do the same as applying this configuration file here so we have our secret my registry key in the cluster so just to review now we did talk her login in this case I did from mini cube Stoker and then we use that file first of all I copied that file from mini cube to my host where execute the cube CTL and I use that file to create the secret which now has contents like this let's actually see that gets secret this is output Y mo file that was created and I have my data here doctor config jason and this is the basics de for encoded contents now there is a second way to create this docker config jason secret where you don't have to do it in two steps you don't have to do doctor login and then create the secret from that convict JSON file you can do both in one step so let's see that as well so it's gonna be keep CTL create secret and now instead of generic I'm gonna specify docker registry the secret type and let's give it Oh let's give it name my registry key - and here I have different flags which are specific for docker registry right so I have docker server and I'm actually gonna execute this command again so that I see the values so I have docker server which is my AWS private registry URL so I'm gonna that here then I have docker username which is AWS I have docker password which is this whole thing here and this is basically it so this command will do both steps in one so it's gonna do docker login and it's gonna create a secret based on the login authentication so if I execute this I have both secrets and I can use any of those for my deployment now you may be asking why there are two different ways to do the same thing can we just go with one but there's a difference between the two and the difference is that with this command you can only create one secret that has access token for one dollar registering for this specific registry for example but it's convenient because you do doctor login and secret creation in one step however if in your cluster you're using more than one private registry where you pull the images from then it will be more practical more convenient to go with other option the first one because for example if you have five private repositories that I want to pull the images from I can do docker login in each one of them and all the access tokens will get stored in this config JSON file like this so you're gonna have a list of them and if you use that file in your secret then with one secret you can have access to all the reports so this is a difference so depending on your use case you may prefer to use one or the other okay so now I've cleared everything and we see that we have our two secrets and now the second step is to configure deployment for my app application so I have a deployment configuration here and this is just the minimum configuration you need for deployment and this is our pot specification I have one container the name is my app and this is the image and the image name of the application has to be the complete name which includes the repository URL and the image name so it's going to be the complete image URL I'm gonna copy that and paste it so this is the repository URL this is the image name and this is the image version because if you just write this one doctor won't know which repository to pull it from and I have the port 3000 because that's why my application runs is that no J's application and now since I already built this all locally on my laptop I have that image available locally show that so 1.3 with this version I have it locally however because we are testing pulling the image from the private repository I want to force docker to pull that image from a repository instead of taking it from the local docker repository which is on my laptop and to do that you can force it by image pull policy attribute and here you can say always and every time pod is created this will force docker to reap all the image even if it already exists locally on your localhost so that's what we're gonna do and the next step will be to configure these employment with the secret so I have to give this deployment access to the secret that I created so that it will be able to pull that image from the stream however I just want to demonstrate what its gonna do or how it's going to behave without that secret configuration so I'm just gonna leave it like this and let's actually apply this configuration apply have any documents like this and if I do cube CTL pod I see image pull back off and it can connect to the container because it's waiting trying and failing to pull the image because it doesn't have authorization so the pot started but container can't be pulled so it's gonna retry now multiple times so I'm gonna delete that and all let's actually add that secret reference to the docker authentication and the way to do that is in the pod specification so on the same level as containers we're gonna configure attribute which is called image pull secrets image pull secrets and here we provide a name of the secret and this is it these two lines will configure deployment with access to the secret that contains docker registry access okay so let's apply that configuration again documents and let's see the pot and you see it's running because it was able to pull the image so so here we see pulling image successfully pulled image so that's how you configure that plumbing now since we have the second secret as well it's actually make sure that both are working so I'm gonna rename my deployment let's call it like this and I'm gonna use the second secret here and let's execute that again as well so this is how you configure your cluster to be able to pull the images from private repository there is one important note here the secret has to be in the same namespace as the deployment or stateful set or any other component that you're creating that needs to pull the image from that repository it has to be in the same namespace which means we have three applications in three different namespaces that all pull images from the private repository you have to create the secret three times in each namespace so to quickly wrap up in order to configure that you have to create a secret there's two ways to creating a secret either you can do dr. Logan manually and use the generated config JSON file to create the secret out of it or you can do it in one step using cube CTL create secret of docker registry type where you provide all the credentials in that command and the second step you will use that secret in the deployment or any other component that needs to pull the image from the docker registry I hope this was helpful and you learned something from this video if you want to see other videos about kubernetes you can subscribe to my channel and follow all my tutorials thank you for watching and see you in the next video
pMO26j2OUME,2020-02-15T13:12:59Z,Run Jenkins in Docker Container - Jenkins Pipeline Tutorial for Beginners 1/4,"so in this video we're gonna set up Jenkins from scratch in a docker container then we're gonna create a multi branch pipeline and connected with a git repository during this step I will also explain how to create different types of credentials in Jenkins and finally I will show you the basics of a Jenkins file to configure this multi branch pipeline so step one is to run Jenkins in a docker container so to use that we need to find an official image for Jenkins on docker hub so let's go and as you see here Jenkins results in an official image and some other images and if you click here you see that this image the official image has been deprecated and the last version is 2.6 T which is very old Jenkins version and instead of it we're gonna use this image here so that would be actually this one because this one is now officially maintained by the Jenkins community and also if you see the text the latest image actually has Jenkins version of to point 219 which for most of the plugins it's important to have a higher Jenkins version otherwise they can't be installed so we're going to take this image and this name of the image so on my command line I'm gonna execute docker run using this image right so I'm gonna dock run and the image however before I run this I need to add some options like I need to expose the port so that I can access it from my browser etc so let's actually go back and see the documentation of how to run this so all the explanation is here so these are some of the options that we're gonna use and actually let's go and do it so first one is I'm gonna expose the ports and the first part I'm gonna expose is port 8080 the Jenkins application inside container will run it port 8080 because Jenkins runs on Tomcat which by default starts at this port and I'm gonna bind it to my hosts port 8080 another port we can expose is 50,000 which is basically a port where drink is master and slave communicate so this will enable my Jenkins to find slaves in case I had some so the ports are open now another thing I'm gonna do is I'm gonna run it in detach mode and I'm also gonna bind volumes to it so to do that I'm gonna use minus B and I'm gonna use a named volume which I'm gonna call Jenkins home meaning I don't have to have this as a folder it's gonna be automatically created I'm just giving it a name which can be whatever you want and I'm gonna bind it to directory inside Jenkins container drink its image under slash bar slash Junkins hope this director is real and it's inside of docker container and this will create automatically create a volume now you have to be careful here because if you were to use a host volume here which basically means that you take an already existing directory on your laptop machine and binding it to the Jenkins container you might get some permission issues meaning the Jenkins user might not be able to write to that folder and that could be a problem you can actually check out my video which I made about docker volumes where you can learn in more detail how this works and the reason why we need to create a volume for Jenkins is to have data persistency for Jenkins so when we remove the container and restarted recreated the data will still be there and the reason why we need to persist this data is because all the Jenkins builds all the configuration and all the Jenkins plugins and also Jenkins users will be stored there so without this data you will basically have to reinitialize the whole Jenkins so important part here and we're gonna use the latest take of Jenkins and these are all the options we need so I'm gonna correct this one here look at here and I'm gonna run this and it's pulling the image from the docker hub it's gonna start in just couple of seconds so the image has been pulled and the docker container must be running already so if I do docker PS I see my Jenkins container running and we actually need its locks so I'm gonna do docker locks with the container ID and I'm gonna see that Jenkins started and its initial setup is needed and this is the password I'm gonna need to initialize it so I'm gonna copy that and if I go to localhost 8080 I will paste that password and it should start initializing so I'm going to go with the suggested plugins and this is gonna take some time to install all the plugins so depends on what technologies your application uses or what source management tools etc he will need different plugins for Jenkins and I'm just gonna go with the community one suggested ones and later of course in Jenkins you can manage these plugins you can delete them you can add new ones etc this is just the base so now that it's done I'm gonna create the first admin user let's go at Jenkins user some password really interesting I'm gonna just mine and I'm gonna leave it localhost Jenkins is ready and this is view I get so if I go to new item because I installed all these plugins I see different types of Jenkins projects I can create but default usually have the freestyle project freestyle project is used for simple single tasks like if you want to just run tests for example whereas with pipeline you can configure the whole delivery flow like test build package deploy your application etc pipeline is a more recent addition so before that they would just chain multiple freestyle projects to get a pipeline like project the pipeline project type is just for a single branch but in this video we're going to create a multi branch pipeline which will apply to multiple branches of the same repository and if I go to manage Jenkins that's where you as an admin user have all the tools to configure Jenkins and under manage plugins you can actually add or delete or manage or update the plugins that you have so in available I have all the plugins that I could install in addition to my Jenkins and install I have all the ones that I have actually selected at the beginning you can remove them again or you can add new plugins in the next section of this video I'm gonna show you how to create a multi branch pipeline and how to connect it to your kid repository and how to build your project using that pipeline"
9_s3h_GVzZc,2019-12-15T18:24:40Z,Docker vs Kubernetes vs Docker Swarm | Comparison in 5 mins,I've seen this question a lot and many of you have also asked what is the difference between docker and kubernetes because I guess it seems like they're competing technologies but the fact is they're not alternatives to each other in fact they go hand in hand together you could use this docker without kubernetes and you can use communities without docker however many projects in best practices actually combine those two technologies to get the best out of both so to demonstrate that docker and kubernetes go actually hand-in-hand here a couple of points for each technology so docker is a container technology which basically means that it creates an isolated environment for applications while kubernetes is an infrastructure for managing those containers where docker really shines is actually automating the building and deployment process of applications so it's actually widely used in the CI CD process while kubernetes comes into action after the application container has been deployed and it takes care of automating scheduling and management of that deployed application containers so overall docker is a container platform to configure build and distribute those build containers or kubernetes is an ecosystem for managing a cluster of multiple docker containers here's a diagram that shows exactly where docker and kubernetes fit in the whole process so docker is mainly used in the local development process so when you're developing a software application you would use docker containers for different services that your application depends on like databases message brokers etc and it's also used in the CI process to build your application and package it into an isolated container environment as we mentioned before once built that container gets stored or pushed into a private repository so now is where kubernetes actually comes into the game so if you have a development server that is made up of multiple virtual or physical servers you would basically install kubernetes on top of those servers and once kubernetes is running you will create a cluster that would actually run your docker containers on top of it and this diagram actually demonstrates how docker and kubernetes technologies can actually be used together so now in order to get a little bit more detailed view of how kubernetes cluster works so you have a kubernetes engine that spends multiple virtual physical servers to create one cluster where docker containers are actually running and you can distribute the number of doctor containers across those physical or virtual servers as you wish where each container will be its own application and the kubernetes service that actually enables docker to run in that cluster is cubelets so each node in the community's cluster will actually have one cubelet and the technology that is actually comparable with kubernetes is docker swarm so as we saw this is a kubernetes cluster set up and the docker swarm is basically an alternative to kubernetes which is a container orchestration tool so instead of cubelets you would have services called docker daemons that will run on each node and instead of the kubernetes engine you would just have docker that actually spends those multiple nodes that make up the cluster and the rest is the same so you have the same docker containers with the same applications running on that cluster set up so now that we saw that docker swarm is an alternative to kubernetes let's see how they actually compare what are the advantages and disadvantages of each technology so the first difference is that kubernetes is much more complex to install and set up then toku swarm and the reason for that is that kubernetes is actually more complex and has much more power but of course comes with a higher learning curve whereas docker swarm is more lightweight however is limited in its functionality so some of the powerful functionality that kubernetes offers in comparison is that it supports auto scaling where a stockist worm needs manual scaling to be configured it also has a built-in monitoring where as docker swarm depends on third-party tools for monitoring but also to talk about some advantages of docker swarm for example kubernetes doesn't support auto load balancing whereas docker swarm supports that feature and also with kubernetes you actually need to learn a new command-line tool which is the cube CTL for example whereas with docker swarm you actually have the same docker command line that you use with docker so you don't have a need for a separate command-line tool there thanks for watching the video I hope it was helpful and if it was don't forget to like it if you want to be notified whenever a new video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in a comment section below and I will try to answer them so thank you and see you in the next video
SBUCYJgg4Mk,2019-12-13T15:28:40Z,Docker Volumes Demo || Docker Tutorial 13,in this video we are gonna look at docker volumes in practice and this is a simple node.js MongoDB application that we're gonna attach the volume to so that we don't lose the database data every time we restart them I'm going to be container so let's head over to the console and I'm gonna start the MongoDB with the docker compose so this is how the compose looks like we're gonna start the MongoDB on container in the Express container so that we have a UI to it so I'm gonna execute the docker compose which is gonna start MongoDB and the Express so when it started I'm gonna check that Express is running on port 8080 and here we see just a default databases so these are just created by default on startup and we're gonna create our own one for the node.js application and inside of that database I'm gonna create users collection so these are the prerequisites or these are the things that my node.js application needs so this one here in order to connect so the database my DB this is what we just created my DB and inside of that to the collection called users so let's start the application which is running on port 3000 so here and this is our app which when I edit something here will write the changes to my database now if I were to restart now the MongoDB container I would lose all this data so because of that way to do it as explained in the previous video docker volumes we're gonna use named volumes inside of the docker compose file persist all this data in the MongoDB let's head over to dr. Campos so the first step is to define what volumes I'm gonna be using in any of my containers and I'm gonna do that on the services level so here I define the list of all the volumes that I'm gonna need in any of my containers and since we need data persistence in for MongoDB we're gonna create data volume here now this is gonna be the name of the volume reference but we also need to provide here a driver local so the actual storage path that we're gonna see later once it's created it is actually created by toker itself and this is a kind of an information additional information for docker to create that physical storage on a local file system so once we have a name reference to a volume defined we can actually use it in the container so here I'm gonna save volumes and here I will define a mapping between the data volume that we have on our host and the second one will be the path inside of the MongoDB container but it has to be the path where MongoDB explicitly persists its data so for example if you check it out online you see that the default path where MongoDB stores its data is data slash data slash to B and we can actually check that out so if I say doc repeat s and go inside the container it's minus 80 I can actually see data DP and here is all the data that MongoDB actually holds but this is of course only the container so when the container restarts the data get regenerated so nothing persists here so this is the path inside of the container not on my host that we need to reference in the volumes here so we're attaching our volume on the host to data slash data slash to be inside of a container so for example for my sequel it's going to be far leap my sequel for Postgres it's also going to be Varley Postgres sequel slash data so each database will have its own so you have to actually find the right one so what this means is that all the data with that we just so here all of this will be replicated on a container start up on our host on this persistent volume that we defined here and vice-versa meaning when a container restarts all the data that is here will be replicated inside of that directory inside of a container so now that we have defined that let's actually restart the document post and restart it so once we create the data and I'm gonna collection and let's actually change this one and update it so we have the data here so now that we have the persistent volume defined if I were to restart all these containers these data should be persisted so on the next restart I should see the database my DB collection ends the entry here so let's do that great so let's check see the database is here the collection is here and the entry has persisted so now let's actually see where the darker volumes are located on our local machine and that actually differs between the operating systems for example on a Windows laptop or computer the path of the docker volume will be at program data docker slash volumes the program data docker folder actually contains all the other container information so you would see other folders in this docker directory besides the volumes on Linux the path is actually /var leap docker volumes which is comparable to the Windows path so this is where the docker saves all this configuration in the data end on the Mac it's also the same one inside of this volumes director you actually have a list of all the volumes that one or many containers are using and each volume has its own hash which is or which has to be unique and then slash underscore data will actually contain all the files and all the data that is persisted let's head over to the command line and actually see the volumes that we persisted for MongoDB now interesting a note here is that if I were to go to this path that I just showed you in the presentation which is var Lib docker C there is no such directory so that could be a little little bit confusing but the way it works on Mac specifically on Linux you would actually have that path directly on your host but none Mac it's a little bit different and actually I learned this fact from the stack or a flow discussion so basically what happens is that docker for Mac applications seems to actually create a Linux VM in the background and store all the docker information or toker data at all the containers and the volumes etc inside of that VMs storage so if we execute this command here so this is actually the physical storage on my laptop that I have where all the data is stored but if I execute this command I actually get the terminal of that VM and inside here if I look I have a virtual different virtual file system and I can find that path that I showed you here so it's var leap docker see so I have all this docker information here I have the containers folder and I have volumes folder so this is the one we need sort of that is usually go to the volumes and this is a list of volumes that I have created and this is the one that came from our docker compose right this is the name of our F this is the this is what docker compose actually takes as the name you can actually take a look here so when it's creating this containers it depends this name is a prefix and then there is MongoDB and our volume has the same pattern it has the prefix and then data this is the name that we defined here so now if we look inside of that data volume directory we see that underscore data and if you have seen my previous video where I explain the different types of doc-doctor volumes this would be the anonymous volumes so basically here you don't have a name reference it's just some random unique ID but it's the same kind of directory as this one here the difference being that this one has a name so it's more it's easier to reference it with the name so this is anonymous volume this is a named volume but the contents will be used in the same way so here as you see in this underscore data we have all the data that MongoDB uses so this will be where it gets the date the default databases and the changes that we make through our application inside and if I go inside of containers so remember this volume is attached to MongoDB and is replicated in side of the container under path slash data / dB so if we go inside of the container here PS / DP we'll see actually the same kind of data here so we have all this index and collection files just like we did in this one so now whenever we make changes to our application for example we change it to Smith and this will make the container update its data and that will cascade into these volumes directory that we have here so that on the next startup of a container when the slash data slash to be is totally empty it will actually populate this directory with the data from this persistent volume so that we will see all the data that we created through our application again on startup and that's how docker volumes work in order to end that screen session that we just started because exceeds doesn't work in this case somehow on Mac you can actually click on ctrl a K and then just type Y and the session will be closed so when you do screen LS you should see actually it's terminating thanks for watching the video I hope it was helpful and if it was don't forget to like it this is a video series so I will create a new one every week so if you want to be notified whenever a new video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in the comment section below and I will try to answer them so thank you and see you in the next video
ZowjOhpAcIc,2019-12-06T16:03:56Z,Deploying the containerized application with Docker Compose || Docker Tutorial 12,so in this video we're gonna see how to deploy an application that we built into a docker image so after you package your application in a docker image and save it in the private repository you need to somehow deploy it on a development server or integration server or whatever other environment in this specific video we are using a docker registry of Amazon Web Services called ECR that's where we have pushed our own application docker image and we're gonna use docker compose to deploy that application this video is part of a docker tutorial series so if you want to actually follow through the whole process of developing an application packaging it into a docker image pushing it into a private repository and then finally deploying it on a application server here's a link to the video series so you can check that out so again back to our initial overview we have gone through all these individual steps so we built an image just like a Jenkins server would do and we pushed it into a repository so now let's actually simulate a development environment so let's imagine we have logged into a development server and we want to run our image that we just pushed the repository so our my app image and the MongoDB image both the database in the Express on the development server so the my app image will be pulled from private repository of EWS the in the to containers will be pulled from the docker hub so let's see actually how that would work so usually again you have developed your application you're done with it and you have created your own docker image right now in order to start an application on development server you would need all the containers that make up that application environment okay so we have a longer  Express already so what we are going to do is here we're gonna add a new container in the list which is gonna be our own image so let's go ahead and copy the image from our repository so let's actually use the 1.0 so again remember we said that this image name is a shortcut for having a docker dot IO dot library slash with like a specific version so instead of that because we are pulling these images from a docker hub we can actually skip that repository domain in front of the images but here because we're pulling it from a private repository so if we were to specify our image like this docker will think that our image resides on docker hub so we try to pull it from docker hub and of course it won't find it because we have to tell docker go and look at this repository with this repository name in this tag and of course in order to be able to pull this image or the docker composed to be able to pull this image the environment where you're executing this docker compose file has to be logged into a docker repository so here as the development server has to pull the image from the repository what we would need to do on the development server is actually do a docker login before we execute the docker compose and obviously you don't need a doctor login for doc hub those images will be pulled freely ok so the next thing that we have to configure are the ports because obviously want to open the ports if we go back we see that our application runs on port 3000 so the port of the container or that where the container is listening on is 3000 and here we can open the port on the host machine so it's going to be 3000 map to 3000 we have actually the environment variables inside of the docker file but obviously we could have configured them in the docker compose just like this so it's an alternative so this will be a complete docker compose file that will be used on a development server to deploy all the all the applications inside so again if we're trying to simulate a development server that the first step will be to dock to the docker login in this case you have this on command for logging in to the AWS repository which I have done already in this terminal so the next step is to have the docker compose file available on this development server because we have to execute the docker compose file because we're simulating here the way I would do it is I'm gonna create an demo file in the current directory where I am I'm gonna copy this and save so now I have my ml file and now we can start all three containers using munger docker compose comment - eff up and here we see that app started on 3000 and MongoDB and Express started as well so let's check again now and here we saw that database is lost every time we recreate a container and of course that's not good and we're going to learn how to preserve the database data between the container restarts using docker volumes in the later tutorials because this is not an ideal State okay so now that we have database in a collection let's actually refresh in our application works as well let's check awesome so our application works let's refresh this one as well and there is actually one thing that I needed to change in the code to connect no J's with MongoDB so let's actually go and look at that so this is my these are my handlers you know no J's where I connect to the MongoDB database so the your eyes are the same and what I changed here is that it was a localhost before so instead of localhost I changed it to MongoDB because this actually is a name of the container or of the service that we specify here so this actually leads back to the doctor Network and how docker compose takes care of it is that in the URI or when I connect one application in a docker container with another one in an other docker container I don't have to use this localhost anymore actually I wouldn't even need to use the port even because I have all that information so the host name and the port number in that configuration so my application will be able to connect to MongoDB using the service name and because of that you don't have to specify here a localhost and the port number which is actually even more advantage when you consider using docker containers to run all of your applications because it makes the connectivity between them even more easier and that actually concludes the this diagram that we saw previously we have gone through all of the steps where we saw how to develop a JavaScript application locally with docker containers then we saw how to build them into an image just like continuous integration build we'll do it then we pushed it into a private repository and we simulated a development server where we pull the image from private repository and the other images from the docker hub where we started the whole application set up with our own application in the two applications using a docker compose which is how you would deploy an application on a dev server so that now testers or other developers will be able to access the development server and actually try out the application that you just deployed or you can also use it for demos thanks for watching the video I hope it was helpful and if it was don't forget to like it this is a video series so I will create a new one every week so if you want to be notified whenever a new video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in a comment section below and I will try to answer them so thank you and see you in the next video
p2PH_YPCsis,2019-11-30T17:45:27Z,Docker Volumes explained in 6 minutes,[Music] so in this video we're gonna learn about docker volumes in a natural docker volumes are used for data persistence in docker so for example if you have databases or other stateful applications you would want to use docker volumes for that so what are the specific use cases when you need docker volumes so container runs on a host let's say we have a database container and a container has a virtual file system where the data is usually stored but here there is no persistence so if I were to remove the container or stop it and restart the container then the data in this virtual file system is gone and it starts from a fresh state which is obviously not very practical because I want to save the changes that my application is making in the database and that's where I need docker volumes so what are the docker volumes exactly so on a host we have a physical file system right and the way volumes work is that we plug the physical file system path it could be a folder a directory and we plug it into the containers file system path so in simple terms a directory a folder on a host file system is mounted into a directory or folder in the virtual file system of docker so what happens is that when it container rights to its file system it gets replicated or automatically written on the host file system directory and vice-versa so if I were to change something on the host file system it automatically appears in the container as well so that's why when a container restarts even if it starts from a fresh state in its own virtual file system it gets the data automatically from that from the host because the data is still there and that's how data is populated on the startup of a container every time you restart now there are different types of docker volumes and so different ways of creating them usually the way to create docker volumes is using the kourin command so in the docker run there is an option called - V and this is where we define the connection of the reference between the host directory and the container directory and this type of volume definition is called a host volume and the main characteristic of this one is that you decide where on the host file system that reference is made so which folder on the host file system you mount into the container so the second type is where you create a volume just by referencing the container directory so you don't specify which directory on the host should be mounted but that's taking care of the docker itself so that directory is first of all automatically created by docker under the VAR lip docker volumes so for each container there will be a folder generated that gets mounted automatically to the container and this type of volumes are called anonymous volumes because you don't have a reference to this automatically generated folder basically just have to know the path and the third volume type is actually an improvement of the anonymous volumes and it specifies the name of that folder on the host file system and the name is up to you it just to reference the directory and that type of volumes are called named volumes so in this case compared to anonymous volumes you have you can actually reference that volume just by name so you don't have to know exactly the path so from these three types the mostly used one and the one that you should be using in production is actually the named volumes because they are additional benefits to letting docker actually manage those volume directories on the host now they showed how to create docker volumes using docker run commands but if you're using docker compose it's actually the same so this actually shows how to use volume definitions in a docker compose and this is pretty much the same as in docker run commands so we have volumes at buuuut and underneath you define your volume definition just like you would in this - B option and here we use a named volume so DB - data will be the name reference name that you can just think of it could be anything and inviolable data is the path in the container then you may have some other containers and at the end so in the same level as the services you would actually list all the volumes that you have defined you define at least volumes that you want to mount into the containers so if you were to create volumes for different containers you would list them all here and on the container level then you actually define under which path that specific volume can be mounted and the benefit of that is that you can actually mount a reference of the same folder on a host to more than one containers and that would be beneficial if those containers need to share the data in this case you would mount the same volume name or reference to two different containers and you can mount them in two different paths inside of the container thanks for watching the video I hope it was helpful and if it was don't forget to like it if you want to be notified whenever a new video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in the comment section below and I will try to answer them so thank you and see you in the next video
vWSRWpOPHws,2019-11-29T16:59:37Z,Private Repository explained | Registry on AWS - Docker in Practice || Docker Tutorial 11,so in this video we're gonna create a private repository for docker images on aws ecr there are many more options for docker registries among them nexus and digitalocean so we're gonna see how to create a registry there build and tag an image so that we can push them into that repository and in order to push the images into a private repository you first have to log into that repository so we're going to see how that works this video is part of a video series of how to use docker in practice in a software development and later in deploying that application so if you want to go through the whole series here's a link to the playlist and make sure to check it out so let's see how it all works [Music] so the first step is to actually create a private repository for docker it's also called docker registry um in this case we're gonna do it on aws so let's see so i already have an account on avws so the service that we're going to use is called elastic container registry so ecr docker container registry and because i don't have a repository there yes i am presenting with the screen so in order to create a repository click on get started and here we have a repository name and we are actually going to name it the name of the application that we have so i'm actually going to name it my app this is the domain of the registry from aws and this is the repository name which is the same as my image name and don't worry about the other stuff right now and just create a repository it's as simple as that now one thing i think specific to amazon container service is that here you create a docker repository per image so you don't have a repository where you have uh where you can actually push multiple images of different applications but rather for each image you have its own repository and when i when you go inside of the repository here it's empty now but what you store in a repository are the different tags or different versions of the same image so this is how the amazon container service actually works there are other docker registries that work differently for example you create a repository and you can just throw all of your container images inside of that one repository so i think this is more or less specific for aws so anyways we have repository which is called my app and let's actually see how we can push the image that we have locally so actually check that once more so we want to push this image here into that repository so how do we do that if you click on this one the view push commands will be highlighted this is different for each registry but basically what you need to do in order to push an image into repository are two things one you have to login into the private repository because you have to authenticate yourself so if you are pushing from your local laptop or local environment you have to tell that private repository hey i have access to it this is my credentials if a docker image is built and pushed from a jenkins server then you have to give jenkins credentials to login into the repository so docker login is always the first step that you need to do so here aws actually provides a docker login command for aws so it doesn't say docker login but in the background it uses one so i'm going to execute this login command for aws docker repository so in the background it uses actually docker login to authenticate so in order to be able to execute this you need to have aws command line interface and the credentials configured for it so if you don't i'm gonna put a link to uh the guide of how to do that in the description i have configured both of them so i can execute this command and i should be logged in successfully to the docker repository so now i have authenticated myself to the docker repository here so i'm able to push the image that i have locally to that repository but before i do that there is one step i need to do so i've already built my image so that's fine and now i have to tag my image and if this command here looks a little bit uh too complicated for you or too strange let's actually go and look at image naming concepts in docker repositories so this is the naming in docker registries this is how it works the first part of the image name the image full name is the registry domain so that is the host port etc slash repository or image name and the tag now you may be wondering every time we were pulling an image out of docker hub we actually never had this complex long name of the image right so when we were pulling an image it looked like this docker poll 4.2 the thing is with docker hub we're actually able to pull an image with a shorthand without having to specify a registry domain but this command here is actually a shorthand for this command what actually gets executed in the background when we say docker pull is docker pull the repo the registry domain so docker dot io slash library is a registry domain then you have the image name and then you have the tag so because we were working with docker hub we were able to use a shortcut so to say in the private registries we can't just skip that part because there is no default configuration for it so in our case in aws ecr what we're gonna do is we're gonna execute docker pull the full registry domain of the repository this is what we're gonna see here and a tag and this is how aws just generates uh the docker registry name that's why we see this long image name with the tag here and we have to take our image like this so let's go back and take a look at our images our image that we built again and under the repository it says my app now the problem is we can just push an image with this name because when we say docker push my app like this docker wouldn't know to which repository we're trying to push by default it will actually assume we're trying to push to docker hub but it's not going to work obviously because we want to push it to aws so in order to tell docker you know what i want this image to be pushed to aws repository with the name my app we have to take the image so we have to include that information in the name of the image and that is why we have to tag the image tag basically means that we are renaming our image to include the repository uh domain or the address and the name okay and aws already gives us the command that we can execute we want to use the specific version so i'm gonna use 1.0 in both so what this is going to do is it's going to rename this is what tech does my app 1.0 this is what we have locally this is what the name is to this one here so let's execute that and let's see what the outcome is and as you see it took the image that we had made a copy and renamed it into this one so these two are identical images um they're just called in a different way and now when we go back we see the docker push command so basically this thing here is the same as docker push and name of the image and the take so this push command will tell docker you know what i want to take the image with take 1.0 and push it into a repository at this address so when i execute this command see the push command will actually push those layers of the docker image one by one this is the same thing as when we're pulling it we already we also pulled the images layer by layer and this is what happens in the reverse direction when we push it so this is also gonna take a little bit great so the push command was complete and we should be able to see that image in the aws repository now so if i go inside see i have image tag with 1.0 this is our tag here and push the time the digest which is the unique hash of that image and the image uri which is again the name of the image using the the repository address image name or repository name in this case and the tag so now let's say i made some changes in the docker file um you know let's say i re renamed this home slash home slash app to node app like this or what could also lead to need to recreate an image is obviously where when i um change something in the code right so you know let's say i were to delete this line because i don't want to console.log to be in my code and now i have a different version of the application where i have changes in the application so now i want to have those changes in the new docker image so now let's build a new docker image out of it so docker build let's call it my app with a version 1.1 and a path to a docker file and now i have a second image which is called my app with version 1.1 so now again because i want to push this to a repository i have to rename it to include the repository address inside of it so i'm going to do docker tag the first parameter is the image that i want to rename and the second one is the name of that image a new name so it's going to be the same as the previous one because the repository name and the address is the same remember we have one repository for one image but for different versions so we're building a version 1.1 so it should end up in the same repository so now here we have 1.1 and if i take that and images i have a second image here so i'm gonna copy that and i'm gonna do docker build and do not forget the tag it's important because it's part of the complete name sorry it's docker push and now some of the layers that i already pushed are there only the ones that changed are being re-pushed so to say and also know that i just have to do docker login once at the beginning and then i can pull and push images uh from this repository uh as as many times as i want so docker login is done once so now that is complete let's actually reload this so my repository now has two versions so this is pretty practical if you are for example testing with different versions and you want to have a history of um those were image tags um if you want to for example test a previous version and i think in aws the repos each repository has a capacity of holding up to 1 000 image versions so for example my app here you can have a thousand different tags of the same image okay so now again to compare it to the initial diagram that we saw for this complete flow let's actually switch back to it quickly so here what we did is basically simulate how jenkins would push an image to a docker repository so whatever we did on our laptop will be the same commands executed on a docker on the jenkins server and again jenkins user or jenkins server user has to have credentials to the docker repository to execute docker login depending on the registry or repository configuration will look different and jenkins needs to tag the image and then push it to the repository and this is how it it's done and the next step uh of course we need to use that image that that is lying now in the repository and we're gonna see how it's uh pulled from that repository and again we're gonna do it on the local environment but it's the same thing that's a development server or any other environment will actually execute thanks for watching the video i hope it was helpful and if it was don't forget to like it this is a video series so i will create a new one every week so if you want to be notified whenever a new video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in the comment section below and i will try to answer them so thank you and see you in the next video
WmcdMiyqfZs,2019-11-22T16:43:06Z,Dockerfile Tutorial - Docker in Practice || Docker Tutorial 10,[Music] so now let's consider a following scenario you have developed an application feature you have tested it and now you're ready to deploy it right to deploy your application should be packaged into its own docker container so this means that we're gonna build in docker image from our JavaScript no J's backing application and prepare it to be deployed on some environment to review this diagram that we saw at the beginning of the tutorial so we have developed a JavaScript application we have used the MongoDB docker container to use it and now it's time to commit it to the game right so in this case we're gonna simulate these steps on the local environment but still I'm gonna show you how these steps actually work so after commit you have a continuous integration that runs so the question is what does actually Jenkins do with this application when it builds the application so the JavaScript application using the npm build etc it packages it then in a docker image and then pushes it into docker repository so we're gonna actually simulate what Jenkins does with their application and how it actually packages it into a docker image on the local environment so I'm gonna do all this on my laptop but it's basically the same thing that Jenkins will do and then on later step we're gonna push it we can actually push the built image into a docker repository in order to build a docker image from an application we basically have to copy the contents of that application into the docker file could be an artifact that we built in our case we just have three files are we gonna copy them directly in the image and we're gonna configure it and in order to do that we're gonna use a blueprint for building images which is called a docker file so let's actually see what is a docker file and how it actually looks like so as I mentioned docker file is a blueprint for creating docker images a syntax of a docker file is super simple so the first line of every dock file is from image so whatever image you are building you always want to base it on another image in our case we have a JavaScript application with no J's back-end so we are gonna need node you know inside of our container so that it can run our node application instead of basing it on a Linux Alpine or some other lower level image because then we would have to install node ourselves on it so we are taking a ready node image and in order to see that let's actually go to docker hub and search node here and here you see there is a ready node image that we can base our own image from so here we have a lot of different text so we can actually use one specific one or we can just go with the latest if we don't specify any take so what that actually means basing our own image on a node image is that we're gonna have node installed inside of our image so when we start a container and we actually get a terminal of the container we can see that node command is available because there is no install there this is what from node actually gives us so the next one is we can configure environmental variables inside our dock file now as you know we have already done this in the using the doctor and comments or the docker composed so this will be just an alternative to defining environments of variables in a docker compose for example I would say it's better to define the environmental variables externally in a docker compose file because if something changes you can actually override it you can change the docker compose file and override it instead of rebuilding the image but this is an option so this end command basically would translate to setting the environmental variables inside of the image environment the next one is run so all these capital case words that you see from in and run they're basically part of a syntax of a docker file so using run basically you can execute any kind of Linux commands so you see make directory is a Linux command that creates a home slash home slash app directory very important to note here this directory is going to live inside of the container so when I start a container from this image the slash home slash app directory will be created inside of the container and not on my laptop not on the host so all these commands that you have in docker file will apply to the container environment none of them will be affecting my hosts environment or my laptop environment so with run basically you can execute any Linux comments that you want so that's probably one of the most use ones and we also have a copy command now you will probably ask I can execute a copy come in a Linux copy command using run yes you could but the difference here is that as I said all these commands in run for example they apply to they get executed inside of the container the copy command that you see here it actually executes on the host and you see the first parameter is dot and second parameter is slash home slash app so source and the target so I can copy files that I have on my hosts inside of that container image because if I were to execute run CP source destination that command would execute inside of the docker container but I have the files that I want to copy on my host in the last one so from an CMD or command is always part of the aqua file what commend does is basically executes an entry point Linux command so this line with the command actually translates to node server js so remember here we actually do node services so we execute so we start a node server with the nodejs this is exactly what it does but inside of the container so once we copy our server JS and other files inside of a container we can then execute node server chase and we are able to do it because we are basing on the node image that already has node pre installed and we are gonna see all this in action so another question here what is the difference between run and CMD because I could also say run node server chase the difference again is that CMD is an entry point command so you can have multiple run comments with the different Linux commands but CMD is just one and that marks for docker file that this is the command that you want to execute as an entry point so that basically runs the server and that's it so now let's actually create the dockerfile and just like the Taku compose file docker file is part of the application code so I'm gonna create a new file here and I'm gonna paste here the contents so again we're basing off note image and actually instead of just having the latest node I'm gonna specify a node version so I'm going to take 13 - Alpine so all these that you see here are text so I can use any of them as a tag so I'm gonna say 13 - Alpine like this so this is gonna be a specific node image that I'm gonna use as my base image let's actually stop here for a moment and take a little bit of a deep dive on this line so since we saw that docker file is a blueprint for every for any docker image that should actually mean that every docker image that there is on docker hub should be built on its own docker file right so if we actually go to let's actually look at one of the latest versions which is 13 - alpine and then let's click inside and as you see this specific image has its own docker file and here as you see we have the same from that we just saw and this is what this node official image is based off which is a base image Alpine 3.10 right and then we have this environmental variable set and all these linux commands using run and some other environmental variable and you have this entry point which is a script so you can also execute the whole shell script instead of instead of separate commands and you have this final comment right so you don't have to understand any of this I just want to demonstrate that every image is based of another base image right so in order to actually visually comprehend how this layers stacking works with images let's consider this simplified visualization so our own image that we're building up with the version 1.0 is going to be based on a node image with a specific version that's why we're going to specify from node 13 alpine and the node 13 Alpine image as we saw in the dockerfile is based on alpine based image with the version 3.1 that's why it specifies from Alpine 3.10 so Alpine is lightweight based image then we install node on top of it and then we stole our own application on top of it and basically this is how all the images are built so now let's go back and complete our docker file so we have the from specified we have the environmental variables specified and in just a second we can actually see this commands in action so let's copy that and this is also very important docker file has to be called exactly like that you can't just give it any name it is always called docker file starting with a capital D and that's it it's a simple text file so just save it like this and here you even see the highlighting and this docker icon so now that we have a docker file ready let's see how to actually use it so how do we build an image out of it so in order to build an image using docker file we have to provide two parameters one is we want to give our image a name in the tag just like all the other images have so we are gonna do it using minus T so we're gonna call our image my app and we're gonna give it a tag of 1.0 the TEC could be anything you can even call it actually version 1 it wouldn't matter so we're gonna do 1.0 and ii required parameter actually is a location of a docker file because we want to tell docker here build an image using this docker file and in this case because we're in the same folder as the docker file we're just gonna say current directory when we execute this we're gonna see that image is built and this is an idea of the image that was built because I already have note 13 Alpine on my laptop dishes use the the one I have lying around locally for you if it's the first time you will actually see that it's pulling node image from the docker hub so now with the docker images I can actually see that my image is here it says created two days ago I don't know why but anyways so I have the image name which is this one here and I have the name of the image and the tag of the image so if we go back to this diagram that we saw in the review so basically we've gone all these steps or we have simulated some of the steps we've built the JavaScript application using a docker containers and once the application is ready let's say we made the commit and we're we just simulated what Jenkins server also does so what Jenkins does is actually it takes the dockerfile that we create so we have to commit the dockerfile into the repository with the code and Jenkins will then build a docker image based on the docker file and what is an important point here is that usually you don't develop a loan you are in the team so other people might want to have access to that up-to-date image of your application that you developed it could be a test or maybe who wants to pull that image and test it locally or you want that image to be deployed on a development server right in order to do that you have to actually share the image so it is pushed into a docker repository and from there either people can take it for example a tester maybe want to download the image from there and test it locally or a development server can actually pull it from there so let's actually just run a container I'm just gonna say docker run the image name obviously and a tank like this and in this case I'm not gonna specify any other options because we just want to see what's going on inside of the container so I'm just gonna run it okay so the problem is that it can't find the server JS file which is actually logical because we are not telling it to look in the correct directory so since we're copying all the resources in this home slash home slash app directory server JS is gonna be there as well and this is another topic whenever you adjust a docker file you have to rebuild an image because the old image cannot be overwritten so to say so what I'm gonna do now is actually I'm gonna delete the one that I built so I'm gonna I'm gonna actually take the image this is how you delete an image but but I can delete it because as it says the docker is used by a stopped container so if I do docker PS - a actually let's crap - my app like this I have to first delete the container so this is how you delete a container it's toker RM and once I've deleted the container I can delete an image so the image deletion is RM I like this so if I do images now I see my image isn't there okay so we've modified the docker file so let's rebuild it now so talk a build okay and let's see the image is here so let's start it again so it's my app 1.0 and let's run it and you see the problem is fixed at listening on port 3000 so our app is running so this one here my app 1.0 first of all we can see the logs here like this we see that the app is listening on port 3000 we know everything is cool to actually just get a little bit more inside let's enter the containers or let's get the terminal the command line terminal of the container and look around there so I'm gonna say docker exec interactive terminal I'm gonna specify the container ID like this and since being bash doesn't work we can actually try shell so this is something you will also encounter because some containers do not have bash installed so I have to connect using pin SH so one of them has to work always so let's see in which directory we are so we are in the root directory and we see our virtual file system there and as you see the cursor changed as well so that means we're inside of a container so now let's actually check some of this stuff so first of all we specified some environmental variables here in the docker file and this means that this environmental variables have to be set inside the docker environment so if we do in we actually see the MongoDB username this one here and MongoDB password are set there's some other environmental variables automatically set we don't care about them so another thing we can check is this directory because remember because with this line we actually created this slash home slash AB directory so let's see slash home slash app and as you can see the director was created and with the next line we copied everything in current folder so if we actually go and see reveal in finder so this is where the dockerfile resides so basically we copied everything that is inside of this directory so all of this into the container now we don't actually need to have docker file and docker compose and these other stuff in here because the only thing we need are the Java Script files or if we build a JavaScript application artifact just an artifact so let's go ahead and improve that so what I'm gonna do is I'm gonna create an app directory and I'm gonna copy just the files that I'm gonna need for starting an application inside of a container so I'm gonna take those and the images as well so all these are just external ones we don't need them there and images the index.html file package JSON server J's and node modules are inside of app so what we can do it now is instead of copying the whole directory where the docker file is I just want to copy all the contents of EPP folder so what I'm gonna do is I'm gonna say copy all the contents and again because we modified a docker file we need to recreate the image in order to leave the docker container terminal you can actually exit so now we are on the hosts again so if I do docker images again I have to first delete the container and an image but in order to delete the container I have to first stop it so now I can remove the container and now I can actually remove the image that the container was based on and let's check again so let's actually execute that build command again so now that we have the image built let's actually run it so I'm gonna say my F one point zero and of course I could have executed with a minus D in a detached mode it doesn't matter now and if I do it or PS I see my image container running and now let's actually enter the container again so my team and as we learned it was in SH and again we're gonna see the home app and here we just have the contents of app directory so no unnecessary docker file docker compose etc files which is actually how it's supposed to be or as I said because I just had a couple of files here I copied all of them but usually if you have this huge application you would want to compress them and package them into an artifact and then copy that artifact into a docker image container okay but as I said this was just for demonstration purposes because I just wanted to show you how you can actually start it as a container in how it should look inside and in this case we improved a couple of things but usually we would start this container from a docker compose as well together with all the other docker images that the application uses and it also doesn't have any ports open so this is just for demonstration purposes so in the next video we're actually gonna see how to create a private repository and how to push images into that private repository again we're gonna simulate these from a local environments so I'm gonna execute these comments from my laptop but this is exactly the same comments that Jenkins server will execute thanks for watching the video I hope it was helpful and if it was don't forget to like it this is a video series so I will create a new one every week so if you want to be notified whenever and video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in the comment section below and I will try to answer them so thank you and see you in the next video
MVIcrmeV_6c,2019-11-17T16:53:12Z,Docker Compose Tutorial - Docker in Practice || Docker Tutorial 9,[Music] so in the last video we created and started two docker containers mongodb and mong express and these are the commands that we used to make it happen right the first we created a network where these two containers can talk to each other using just the container name and no host port etc is necessary for that and then we actually ran two docker run commands with all the options environmental variables etc set now this way of starting containers all the time is a little bit tedious and you don't want to execute these run commands all the time on the command line terminal especially if you have a bunch of docker containers to run uh you probably want to automate it or just make it a little bit easier and there's a tool that's that makes running multiple docker containers with all this configuration much easier than with docker run commands and that is docker compose if you already know the compose and you are wondering why is it useful and what it actually does then bear with me in the next slide i'm going to explain that so this is a docker run command of the mongodb that we executed and previously so basically with docker compose file what we can do is we can take the whole command with its configuration and map it into a file so that we have a structured commands so if you have let's say 10 docker containers that you want to run for your application and they all need to talk to each other and interact with each other you can basically write all the run commands for each container in a structured way in the docker compose and we'll see how that structure actually looks like so on the right side in the docker compose example the first two takes are always there right version three that's the latest version of the compose docker compose and then we have the services this is where the container list goes so the first one is mongodb and that maps actually to the container name right this is going to be a part of container name when docker creates a container out of this configuration blueprint the next one is actually the image right so we need to know which image that container is going to be built from and of course you can specify a version tag here next to the name the next one is port so we can also specify which ports is going to open first one is on the host and the second one after the colon is on the container so the port mapping is there and of course the environmental variables can be also mapped in the docker compose and this is how actually the structure of docker compose looks like for one specific commands let's actually see the second container command for express that we executed and how to map that so now again we have a docker run command for express and let's see how we can map it into a docker compose so as i said services will list the containers that we want to create and again names  express will map to the container name the next one will be the image again you can add a tag here if you want to be um have a specific one then you have the ports 80 to 80 to 8080 and then you have all the environmental variables again under the attribute environment and this is how the docker compose will look like so basically docker compose is just a structured way to contain very normal common docker commands and of course it's it's going to be easier for you to edit the the file if you want to change some variables or if you want to change the ports if you want to add some new options um to the run command so to say and maybe you already noticed the network configuration is not there in the docker compose so this network that we created we don't have to do it in a docker compose we go to the next slide because we have the same concept here we have containers that will talk to each other using just the container name so what docker compose will do is actually take care of creating a common network for these containers so we don't have to create the network and specify in which network these containers will run in and we're going to see that in action right away so let's actually create a docker compose file so i'm gonna paste all my contents here and this is exactly what we saw on the slides and i'm gonna save it as a yaml and we see the highlighting as well be very aware of the indentation they have to be correct so this is the list of all the containers on the same level and then each container has its configuration inside that so now compared to docker run commands it will be very easy for me to go here and change these environment variables or add some new configuration options etc so here again for demonstration we actually save the doctor compose in the code so it's part of the application code so now that we have a docker compose file the question is how do i use it or how do i start the containers using that so let's go to the command line and start docker containers using this docker compose file so the way to use it is using docker compose command now if you've installed docker on your laptop it usually gets installed with the docker compose packaged inside so you should have both docker and docker compose commands installed as a package so docker compose command takes an argument which is the file so i'm going to specify which file i want to execute and in my case it's called yemo and at the end i want to say what i want to do with this file in this case the command is up which will start all the containers which are in the yemo so let's actually check before that there there are no containers running so i don't have anything running here and i'm gonna start those two containers okay so there are a couple of interesting things here in this output so let's scroll all the way up so we've talked about docker network and how we created our own network at the beginning to run the containers inside and i said the docker compose takes care of it and here we see the output where it actually created a network called my app default this is the name of the network and it's going to run those two containers these are actually the names of the containers that docker compose created this is what we specified and it just added prefix and suffix to it and it created those two containers in that network so if i actually go here and do docker network ls i see the my app default is here so that's one important thing another one is the logs of both containers actually mixed because we're starting both at the same time as you see the express has to wait for mongodb to start because it needs to establish a connection so we here see the locks so mongodb is starting we still get connection reviews because it's not started completely and somewhere here when mongodb is started and listening for connections express is able to connect to it so this is something that you can also do with docker compose when you have two containers that where one depends on another one starting you can actually configure this waiting logic in the docker compose okay so now let's see actually that the docker containers are running so we have both of them here you see the container names that docker compose gave them and one thing here to note is that the  express actually started on port 8081 inside the container so we can see that here so we are opening a port 8080 on my laptop that actually forwards the request to container at port 8081 just so that you don't get confused because it was 8080 on the slides so now that we have restarted the containers let's actually check the first one which is  express so it's running on 8080. in the previous example we created a database and the collection which is gone because we restarted the container this is actually another very important concept of containers to understand when you restart a container everything that you configured in that container's application is gone so data is lost so to say there is no data persistence in the containers itself of course that is very um inconvenient you want to have some persistence especially when you're working with the database and there is a concept we're going to learn later in this tutorial series called volumes that makes it possible to have persistency between the container restarts okay so let's actually create the database again because we need it and inside the database we had actually users collection let's create that one as well and that is empty now let's actually start our application and there you go so now if i were to modify this one here and update i should see the updated entry here so the connectivity with mongodb works so now what do i do if i want to stop those containers of course i could go there and say docker stop and i can provide all the ids as we did previously or with docker compose it's actually easier i can do docker compose again specify the file and instead of up i'm going to say down and that will go through all the containers and shut them all and in addition to removing the containers or stopping them removing the containers it also removes the network so the next time we restart it it's going to recreate so let's actually check that token network ls that default my app default network is gone and when i do up see it gets recreated that should give you a good idea of what docker compose is and how to use it in the next video we're gonna build our own docker image from our node.js javascript application thanks for watching the video i hope it was helpful and if it was don't forget to like it this is a video series so i will create a new one every week so if you want to be notified whenever a new video comes out then subscribe to my channel um if you have any questions if something wasn't clear in the video please post them in the comment section below and i will try to answer them so thank you and see you in the next video
6YisG2GcXaw,2019-11-10T10:32:54Z,Developing with Docker - Docker in Practice || Docker Tutorial 8,[Music] so in this video we are going to look at some practical example of how to use docker in a local development process so the prerequisites before you get started with this video is you have to understand some basic concepts of docker and you have to know some of the basic commands so if you don't you can check out my videos where I cover both the concepts and the comments and then you can come back to this video so what we're gonna do is a simple demo of a JavaScript and node.js application in the backend to simulate the local development process and then we're gonna connect it to a docker container with a MongoDB database in it so let's get started so in this video we're gonna see how to work with docker containers when developing applications so the first step will be is we're gonna develop a very simple UI backend application using Java Script very simple HTML structure and no J's in the backend and in order to integrate all of this in the database we are gonna use a docker container of a MongoDB database and also to make working with the MongoDB much easier so we don't have to execute commands and in the terminal we're gonna deploy a docker container of a mongrel UI which is called Express where we can see the database structure and all the updates that our application is making in the database so this development setup should give you an idea of how docker containers are actually used in development process so I've already prepared some very simple JavaScript application so in order to see the code basically we have this index.html that is very simple code and we have some JavaScript here and we're using node.js back-end that just serves that index.html file and listens on port 3000 so we have the server running here in the backend and we have the UI that looks like this so basically it's just the user profile page with some user information and user can edit their profile so if I for example change the name here and if I change the email address and do changes like this I can save my profile and I have my updates here however if i refresh the page of course the changes will be lost because it's just JavaScript no J's so there is no persistent component in this application so in order to have that which is actually how real-life applications work you'll know that you need to integrate the application with the database so using that example I will try to showcase you how you can actually use the docker containers to make the development process easier by just pulling one of the databases and attaching it or connecting it to the application so in this case we're gonna go with the MongoDB application and in addition to MongoDB container we're gonna also deploy a MongoDB UI which is its own container it's called Express where we can manage or see the database in insights and updates from our application much easier so now let's see how that all works so in order to get started let's go to docker hub and find our MongoDB image here let's go to Congo and we have MongoDB here and the Express which is another container that we're gonna use for the UI so first let's pull the MongoDB official image so I already have MongoDB latest so pulling doesn't take longer but on my laptop but you're gonna need a couple of seconds probably and the next one we're gonna pull is the docker Express which I also have I believe so let's see yes it's also fast so if I check luckily I have MongoDB and  Express images so the next step is to run both and Express containers in order to make the MongoDB database available for our application and also to connect the Express with the DB container so let's do the connection between those two first in order to do that we have to understand another darker concept doctor network so how it works is that Dhaka creates its isolated token network where the containers are running in so when I deploy two containers in the same token Network in this case and Express they can talk to each other using just the container name without localhost port number etc just the container name because they are in the same network and the applications that run outside of docker like our node.js which just runs from node server is going to connect to them from outside or from the host using localhost and the port number so later when we package our application into its own docker image what we're gonna have is a game token network with MongoDB container Express container and we're gonna have a node.js application which we wrote including the index HTML and JavaScript for fronting in its own docker container and it's gonna connect to the MongoDB and the browser which is running on the house but outside the docker network is going to connect to our JavaScript application again using hostname and the port number so docker by default already provides some networks so if we say docker Network LS we can already see these auto-generated dog networks so we have four of them with different names and the drivers we're not gonna go into details here but what we're gonna do is create its own network for the MongoDB and Express and we're gonna call it mobile network so let's do this right right away I'm gonna say docker Network create and we're gonna call it Network so now if I do doctor Network LS again I see my token network has been created so now in order to make our MongoDB container in the Express container run in this Mangal Network we have to provide this network option when we run the container in the docker run command so let's start with the so we all know that docker run is the command to start a container from an image right so we have current which is the basic docker run command however in this case we want to specify a couple of things as you learned from the previous videos you have to specify something called port so we need to open a port of MongoDB the default port of MongoDB is 27 thousand seventeen so we'll take that port actually for both host and container so  will run at this port inside of a container and we open the same port on the host so that will take care of the port then we will run it in a detached mode in addition to that there are a couple of things that we can specify when starting up the container and these are environmental variables of the MongoDB let's see in the official image description you actually have a couple of documentation about how to use the image which is very helpful to kind of understand what kind of configuration you can apply to it here you see some environmental variables so basically on startup you can define what the root username and the password will be which is very handy because we're gonna need those two for the experts to connect to the and you can also specify the init database we're just gonna provide the username and password because we can create the database from the Express you why later so let's do that and the way you can specify the environmental variables you can actually see here as well is by just let's copy this one so here you say environmental variable that's what they - e flag stands for and the root username will say it's admin and another variable which is the password will be just password so in this way we can actually override what the default username and password will be so two more things that we need to configure in this commend our container name because we're gonna need that container name to connect with the Hmong Express so we'll call this one dB let's say and another one we need is the network that we created which was called Network so in order to make this command a little bit more structured do it on multiple lines so let's see so it's more readable so basically all these options or all these flags that we set to go one more time through them it's gonna start in detached mode we are opening the port on the host username and password that we want a MongoDB to use in the startup process we're gonna rewrite or override the name of the container and this container is gonna run in a Network and this should actually start the container okay so if you want to see whether it was successful we can lock the container and see what's happening inside so as we see  was started and everything actually looks good waiting for connections on port 20 7017 okay so now let's start Express we want  Express to connect to the running MongoDB container on startup and here we have an example of how to run it and here we have a list of environmental variables that we can configure so let's quickly look at them username password we don't need them however we need the admin username and admin password of the MongoDB this is actually what we overrode with admin and password so we're gonna use them because Express will need some username password to authenticate with the MongoDB interconnected the port is by default the correct one so we don't need to change that and this is an important part this is the MongoDB server right so basically this is the container name that experts will use to connect to the docker and because they are running in the same network only because of that this configuration will work if I didn't if I hadn't specify the network then I could have I could specify the name correct name here of the container but it wouldn't work so with that said let's actually create the docker run command for express as well so let's clear the history and let's start so again we run in detached mode and let's see what parameters we need so first of all port let's say with what is the default port that the Express runs on that's 80 81 so we'll take that so basically it's gonna run on our laptop on port 80 81 the next option would be these two and remember environmental variables need to be specified with minus E and this is the username of MongoDB admin which is admin because we specified it when we started the MongoDB container this is the password let's set this one as well don't forget the network - - net  Network we have the name we can also call it Express and let's see what else we might need here yes this is important one and our container name let's actually see it again dr. PS the one running it's called MongoDB that's the container name and this is what we need to specify here so I'm gonna write this here and finally the image is called  Express so I'm just gonna copy this one here and that is it so basically with these commands talk Express should be able to connect to the MongoDB container so let's write it and just to make sure let's lock the container and see what's happening there waiting for MongoDB welcome to my express it looks like it connected successfully it says here database connected and the  Express is available at port 8081 so let's check the manga Express out at the port 8081 so actually let's close these tabs you don't need them anymore and here if I say localhost 8080 one I should be able to see the manga Express so these are the databases that are already exist by default in or which are created on startup and using the UI we can create our own database as we saw previously we could have specified an environmental area variable in it TB on MongoDB startup and that would have created a new database however it doesn't matter we will just create a database name here so we will call it user account database so let's create one and now we can actually use it or connect to this database from nodejs so let's see how that works so now we have the MongoDB container and the manga Express container running so let's check that we have both of them we'll have to connect no J's with the database so the way to do it is usually to give a protocol of the database and the URI and the URI for a MongoDB database would be localhost and the port that it's accessible at so when I already went ahead and prepared the code for no J's so basically we are gonna use a client here which is a node module and using that client we are connecting to the MongoDB database so this is the protocol the host and the port that we just saw that the MongoDB is listening at and username and password of the root user of MongoDB of course usually you wouldn't put the password here or not use an admin or root username password to connect to a database but this is for just the demonstration purposes and these are username and password that we set as environmental variables when we created the docker MongoDB container so let's check that so this is the MongoDB container command and this is the username root and root password that we specified and this is what we are gonna use in the code as I said for demonstration purposes I will write the password directly here so then we connect to the database so I also went ahead and in the Express user account database and inside that I created collection which is like a table in my sequel world called users so here I connect to a user account database and I query the collection users and this is a get requests so I'm just fetching something from the database and this is update request same thing I connect to the database using the same URI and the database name and I update or insert something in the collection so let's see how all that works so let's head over to the UI so in the users collection the there is no data it's empty so we're gonna refresh it and edit the data so I'm gonna right here some and update it and refresh we see that a new insert was made so this is the update profile section here so all this was executed it connected to the MongoDB and now we have one entry which is email coding name that we changed so if I'm gonna refresh it now I fetched a newly inserted use the data in the UI and I displayed it here and also if you want to see what the MongoDB container actually logs during this process we can actually look at the locks so I'm gonna say docker PS and block using the container ID so let's say if I wanted to see just the last part of it because I wanted to see what the last activity was I can also let's clear this and I can also do tail so I can just display the last part of it or if I wanted I could also stream the logs so I'll clear this again and I'll say stream the logs so I don't I won't have to do doctor logs all the time so if I make a line here for example to mark the last logs I can refresh it let's make some other changes let's change it and save profile so I'm gonna see some activity here as well so these connections are new and it also says received client metadata and this is where the node.js request comes in with a noches and it's version and at the end of each communication there is an in connection because we end the database connection at the end so we see that also in the logs so for example something wasn't working properly you could always check them in a logs here so with that I have a fully functional JavaScript node.js application which has a persistence in the MongoDB database and we also have UI both of them running in a docker container so this would be somehow a realistic example of how local development using docker containers would look like thanks for watching the video I hope it was helpful and if it was don't forget to like it this is a video series so I will create a new one every week so if you want to be notified whenever a new video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in a comment section below and I will try to answer them so thank you and see you in the next video
YdKUkDe22RA,2019-11-10T10:02:34Z,Overview of Workflow with Docker - Docker in Practice || Docker Tutorial 7,[Music] so once you've learned the docker basic concepts and understood how it works it's important to see how docker is actually used in practice so in software development workflow you will know you have this classical steps of development and continuous delivery or continuous integration and then eventually it gets deployed on some environment right it could be a test environment develop environment so it's important to see how docker actually integrates in all those steps so in the next couple of videos I'm gonna concentrate exactly on that so we're gonna see some overview of the flow and then we're going to zoom in on different parts and see how docker is actually used in those individual steps so let's consider a simplified scenario where you are developing a JavaScript application on your laptop right on your local development environment your JavaScript application uses a MongoDB database and instead of installing it on your laptop you download a docker container from the docker hub so you connect your JavaScript application with the MongoDB and you start developing so now let's say you develop the application first version of the application locally and now you want to test it or you want to deploy it on the development environment where a tester in your team is gonna test it so you commit your JavaScript application in git or in some other version control system that will trigger a continuous integration a Jenkins build or whatever you have configured and Jenkins build will produce artifacts from your application so first you will build your JavaScript application and then create a docker image out of that javascript artifact right so what happens to this docker image once it gets created by a Jenkins build it gets pushed to a private docker repository so usually in a company you would have a private repository because you don't want other people to have access to your images so you push it there and now as the next step could be comes configured on Jenkins or some other scripts or tools that docker image has to be deployed on a development server so you have a development server that pulls the image from the private repository your JavaScript application image and impulse the MongoDB that your JavaScript application depends on from a docker hub and now you have two containers one your custom container and a publicly available MongoDB container running on them on dev server and they talk to each other you have to configure it of course they talk and communicate to each other and run as an app so now if a tester for example or another developer logs in to a dev server maybe they will be able to test the application so this is a simplified workflow how docker will work in a real life development process in the next videos I'm gonna show you hands-on demo of how to actually do all this in practice thanks for watching the video I hope it was helpful and if it was don't forget to like it this is a video series so I will create a new one every week so if you want to be notified whenever a new video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in a comment section below and I will try to answer them so thank you and see you in the next video
5GanJdbHlAA,2019-11-04T14:40:32Z,Docker vs Virtual Machine | simply explained || Docker Tutorial 6,n/a
tLK9nNFHWH8,2019-10-28T21:53:35Z,Debugging Docker Containers with docker exec and docker logs || Docker Tutorial 5,[Music] so so far we have seen a couple of basic docker commands we have seen dr. pool which pulls the image from the repository to local environment we also saw run which basically combines docker pool and docker start pulls the image if it's not locally available and then starts it right away then we saw dock a start and a stop which makes it possible to restart the container if you made some changes and you want to create a new version which makes possible to restart the container if you need to we also saw docker run with options the one option that we saw was D minus T which is detach so you can run the container djay detached mode so you can use terminal again - P allows you to bind port of your host to the container so very important to remember - P then comes the port of your host and then comes the port of your container whatever it might be we also saw dr. PS dr. PS - a which basically gives you all the containers no matter if they're running currently or not we also saw docker images which gives you all the images that you have locally so for example if after a couple of months you decide to clean clean up your space and get rid of some stale images you can actually check them check the list and then go through them and delete them you can do the same with stale docker containers that you don't use anymore or you don't need it anymore you can also get rid of them so the final part of the docker basic commands are commands for troubleshooting which are very very useful if something goes wrong in the container you want to see the locks of the container or you want to actually get inside of container get the terminal execute some comments on it so let's see dr. PS we have two containers running right now we don't have any output we don't see any logs here so let's say something happens your application cannot connect to Redis and you don't know what's happening so ideally you would want to see what logs Ready's container is producing right the way to do that is very easy you just say docker locks and you specify the container ID and you see the locks you can also do the lock logs if you don't want to remember the container ID or to dr. PS all the time you can remember the name of the container and you can get the logs using the name so a little side note here as we're talking about the names of the containers so here is you see when a container is created you just get some random name like this so you can name your containers as you want using another option of the docker run which might be pretty useful sometimes if you don't want to work with the container IDs and you just want to remember the names or if you just want to differentiate between the containers so for example let's create a new container from readies for the old image using a different name that we choose so I'm gonna stop this container and I'm going to create a new one from the same image so let's run it in the detached mode let's open the port 2000 1 2 6 3 7 9 and give the name to the container and let's go since it's the older version let's call it Redis older and we need to specify the image so remember this will create a new container since we're running the docker 1 command again so if we execute this and check we see the readies for the O image-based container is created which is fresh new you can see I created and the name is ready solder and we can do the same for the other container so that we kind of know which container is what so to stop this one and I will use the same command here this will be the latest and I will call this readies latest and since find another port so I'm gonna run it and let's see so here I have two containers running now I know Ready's older readies latest so for example if the older version has some problems I can just do logs readies older and I can get my locks so another very useful command in debugging is docker exec so what we can do with docker exec is we can actually get the terminal of a running container so let's check again we have two containers running and let's say there is some problem with the latest Redis latest container and I want to get a terminal of the container and - maybe navigate a directory inside check the lock file or maybe check the configuration file or print out the environmental variables whatever so in order to do that we use docker exit command with minus T which stands for interactive terminal then I specify the container ID and I say in flash so I get that - and here you see that the cursor changed so I'm inside of the container as a root user and here if I say LS okay the data is empty I can also print out in which directory I am can go to the home directory see what's there so I have my virtual file system inside of a container and here I can navigate the different directories and I can check stuff I can also print all the environmental variables to see that something is set correctly and do all kinds of stuff here and this could be really useful if you have a container with some complex configuration or if for example you are running your own application that you wrote in a container and you have some complex configuration there or some kind of setup and you want to validate that everything is correctly set in order to exit the terminal use the exit and you're out you can also do the same using the name again if you don't want to work with the IDs and you just want to remember the names of the container to make easier you can do it we the name is well same thing since most of the container images are based on some lightweight linux distributions you wouldn't have much of the linux commands or applications installed here for example you wouldn't have curl or some other stuff so you were a little bit more limited in that sense so you can execute a lot of stuff from the docker containers for most of the debugging work it should be actually enough so the final part to review the difference between lock run and docker start which might be confusing for some people let's revisit them so basically docker run is where you create a new container from an image so docker run will take an image with a specific version or just latest right as a option or as an attribute with docker start you not working with images but rather with Tanner's so for example as we saw the Quran has a lot of options you specify with minus D and minus P the port binding and then you have this name of the container and all this stuff so basically you tell docker at the beginning what kind of container with what attributes name and so on to create from a specific image but once the container is created and you can see that using a command for example here the last ones that we created and if you stop it and you want to restart it you just need to use the command docker start and specify the container ID and when you started the container will retain all the attributes that we defined when creating the container using docker run so dr. Ron is to create a new container toku start is to restart a stopped container thanks for watching the video I hope it was helpful and if he was don't forget to like it this is a video series so I will create a new one every week so if you want to be notified whenever a new video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in the comment section below and I will try to answer them so thank you and see you in the next video
xGn7cFR3ARU,2019-10-28T21:05:09Z,8 Basic Docker Commands || Docker Tutorial 4,"[Music] so in this video I'm gonna show you some basic doc recommends at the beginning I'm gonna explain what the difference between container and image is because that's something a lot of people confuse then very quickly go through version intake and then show you a demo of how to use the basic docker comments commands that will be enough to pull an image locally to start a container to configure a container and even debug the container so with that said let's get started so what is the difference between container and image mostly people use those terms interchangeably but actually there is a fine difference between the two to see theoretically image is just the part of the container runtime so container is the running environment for an image so as you see in this graphic the application image that runs the application could be Postgres Reddy's some other application needs let's say a file system where it can save the log files or where it can store some configuration files it also needs some environmental configuration like environmental variables and so on so all this environmental stuff are provided by container and container also has a port that is binded to it which makes it possible to talk to the application which is running inside of a container and of course it should be noted here that the file system is virtual in the constant container so the container has its own abstraction of an operating system including the file system and the environment which is of course different from the file system and environment of the host machine so in order to see the difference between container and image in action let's head over to the talker hub and find example a Redis image another thing is that docker hub all the artifacts that are in the docker hub are images so we're not talking about containers here all of these things are images talker official image so we're gonna go ahead and pull a Redis image out of the docker hub to my laptop so you see the different layers of the image are downloaded and this will take a couple of minutes so once the download is complete I can check all the existing images on my laptop using docker images command so I see I have two images readies and Postgres with text image IDs and so on another important aspect of images is that they have texts or versions so for example if we go back to the dock hub each one each image that you look up in the docker hub will have many different versions the latest is always the one that you get when you don't specify the version of course but if you have a dependency on a specific version you can actually choose the version you want and specified you can select one from here so this is what you see here the tag is basically the version of the image so I just downloaded the latest and I can also see the size of the image so now to this point we have only worked with images there is no container involved and there's no Redis running so now let's say I need readies running so that my application can connect to it I'll have to create a container of the Redis image that will make it possible to connect to the Redis application and I can do it by running the Redis image so if I say docker run Redis this will actually start the image in a container so as I said before container is a running environment of an image so now if I open a new tab and do docker PS I will get stairs of all the running docker containers so I can see the container radius is running with a container ID based on the image of Redis and some other information about it for example the port that it's running on and so on so as you see here the docker run readies command will start the race container in the terminal in an attached mode so for example if I were to terminate this with a control C you see that Redis application stops and the container will be stopped as well so if I do docker PS again I see that no container is running so there is an option for docker run command that makes it able makes it possible to run the container in a detached mode and that is minus D so if I do docker run - t Redis I will just get the ID of the container as an output and the container will stop running so if we check again docker PS I see the container with the ID starting with 8 3 8 which is the same thing here is running so this is how you can start it in a detached mode now for example if you would want to restart a container because I don't know some the application crashed inside or some error happens so you want to restart it you would need the container ID so just the first part of it not the whole string and you can simply say doctor stop idea of the container and that will stop the docker container I think running if you want to start it again you can use the same ID to start the game so let's say you stopped docker container at the end of the day you go home you come back the next day open your laptop and you want to restart the stop container right so if you do dr. PS there is the output it's empty you don't see any containers so what you can do alternative to just looking up your history command-line history is you can do docker PS - a which will show you all the containers which are running or not running so here you see the container ID again and you can restart it okay so let's try another thing let's say you have two parallel applications that both use Redis but in different versions so you would need to read ease containers with different image versions running on your laptop right at different times maybe or at the same time so so here we have the latest one which is ready 5:06 and let's head over to the docker hub in select version let's say you need version 4.0 so remember the first time that we downloaded the Redis image we did dr. pol Redis however if you run docker if you use docker run with Redis image and the tech which was 4.0 it will pull the image and start the container right away after it so it does two commands basically in one so it's dr. pol that doctor start in one comment so if I do this it says it can find the image locally so it goes and pulls the image from the repository to my laptop and again we see some layers are downloaded and the container is started right away and now if I do dr. PS you see that I have two radiuses running so this is where it gets interesting now how do you actually use the any container that you just started so in this output we you also see the ports section which specifies on which port the container is listening to the incoming requests so both containers open the same port which is what was specified in the image so in the logs of the container you can see the information running mode standalone port six three seven nine so how does that actually work and how do we not have conflicts while both are running on the same port so to explain that let's head over to our slide and see how this works as you know container is just a virtual environment running on your host and you can have multiple containers running simultaneously on your host which is your laptop PC whatever you working on and your laptop has certain poe ports available then you can open for certain applications so how it works is that you need to create a so-called binding between a port that your laptop your host machine has and the container so for example in the first container part here you see container is listening on port 5000 and you bind your laptop's port 5000 to that containers now you will have conflict if you open to 5,000 ports on your hosts because you will get a message the port is already bound or is already in use you can do that however you can have two containers as you see in the second and third containers are both listening on port 3000 which is absolutely okay as long as you bind them to two different ports from your host machine so once the port binding between the host and the container is already done you can actually connect to the running container using the port of the host so in this example you are I you would have some F local host and then the port of the host and the host then we'll know how to forward the request to the container using the port binding so if we head back here you see that containers have their ports and their report running on the same one however we have made any binding between my laptop's ports and the container port and because of that the container is basically unreachable by any application so I won't be able to use it so the way we actually do that is by specifying the binding of the ports during the run command so I'm gonna break this and check that there is just one container running no I'm gonna stop the other one as well so we can start them in you okay so we see both containers are here so now we want to start them using the binding between the host and the container ports but again we have to read is's so we need to bind them to two different ports on my laptop so the way to do it is you do docker run and you specify with - P the port of the host that's the first one so let's go with 6000 it doesn't really in this case and the second one is the port that you're binding this tube which is the container port so we know the container port will be six three seven nine and this is where we bind our so my laptop's port 6002 and if I do this here so you know if I do  PS that's actually clean this here you see the binding here alright so your laptop's 6,000 port is bound to the containers six three seven nine so now let's do another thing and let's start it in a detached mode like this let's check again it's running again and now I want to start the second container that's clear this again so here you see it created a bunch of containers because when I specified different options with the port binding it actually created new containers that's why you see a couple of them over here so I'm gonna copy the image name with the tag for that o- p so for example if I were to do this now and I would try to run the other readies the second readies container with the same port on my laptop I would get an error saying port is already allocated so I can do six thousand one and run it again I'll run it in detached mode and if I go over here and say docker PS I see that i have two different Redis versions running both of them bound to different ports on my laptop and they the containers themselves listening to requests on the same port thanks for watching the video I hope it was helpful and if it was don't forget to like it this is a video series so I will create a new one every week so if you want to be notified whenever a new video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in the comment section below and I will try to answer them so thank you and see you in the next video"
wH9XesmPUOk,2019-10-24T16:07:04Z,How to install docker? Step by Step || Docker Tutorial 3,[Music] so in this video I will show you how to install darker on different operating systems the installation will differ not only based on the operating system but also the version of the operating system so you can actually watch this video selectively depending on which OS and the version of that OS you have I will show you how to find out which installation step applies to you in the before installing section which is the first one so once you find that out you can actually directly skip to that part of the video where I explain that into details I will put the mini locations of each part in the description part of the video and also I will put all the links that I use in the video in the description so that you can easily access them also if you have any questions during the video or if you get stuck installing the docker on your system please post your question or problem in the comment section so that I can get back to you and help you proceed or maybe someone from the community will so with that said let's dive right into it so if you want to install docker you can actually google it and you get an official documentation of docker it's important to note there are two editions of docker there is a community and Enterprise editions for us to begin with Community Edition will be just fine in the docker Community Edition tab there there is a list of operating systems and distributions in order to install docker so for example if we start with Mac we can click in here and we see the documentation of how to install it on Mac which is actually one of the easiest but we'll see some some other ones as well so before you install darker on your Mac or Windows computer there are prerequisites to be considered so for Mac and Windows there has to be some criteria of the operating system and the hardware met in order to support running if you have make go through the system requirements to see if your make version is actually supporting docker if you have windows then you can go to the Windows tab and look at the system requirements there or what to know before you install for example one thing to note is that docker natively runs only on Windows 10 so if you have a Windows version which is less than 10 then we docker cannot run natively on your computer so if your computer doesn't meet the requirements to run docker there is a workaround for that which is called docker toolbox that of docker you basically just have to install a docker toolbox that will become a sort of a bridge between your operating system and the docker and that will enable you to run docker on your legacy computer so if that applies to you then skip ahead in this video to the part where I explain how to install docker toolbox on Mac and on windows so let's install docker for Mac as we see here there are two channels that you can download the binaries from or the application from will go with a stable channel and other things you consider if you have an older version of Mac the software or the hardware please go through the system requirements to see if you can actually install docker so here there is a detailed description of what make version you need to be able to run docker and also you need at least 4 gigabytes of RAM and by installing darker you will actually have the whole package of it which is a darker engine which is important or which is necessary to run the docker containers on your laptop the docker command-line client which will enable you to execute some token commands docker compose if you don't know it yet don't worry about it but it's just technology to orchestrate if you have multiple containers and some other stuff that we're not gonna need in this tutorial but you will have everything in a package installed so go ahead and download the stable version well I already have darker installed from the edge channel so I won't be stalling it again but it shouldn't matter because the steps of installation are the same for both so once the docker dmg file is downloaded you just double click on it and it will pop up this window just drag the docker well up into the applications and will be installed on your Mac as the next step you will see docker installed in your applications so you can just go ahead and start it so as you see the docker sign or icon is starting here if you click on it you see the status that docker is running and you can configure some preferences and check the docker version and so on and if you want to stop talker or quited on your Mac you can just do it from here and importance may be interesting note here is that if let's say you download or install docker and if you have more than one accounts on your laptop you will actually get some errors or conflicts if you run docker at the same time or multiple accounts so what I do for example is that if I switch to another account where I'm also gonna need docker I quit it from here and then I started from from the other account so that I don't get any errors so that might be something you need to consider if you use multiple accounts so let's see how to install Tucker for Windows the first step as I mentioned before is to go to the before you install section and to see that your operating system and your computer meets all the criteria to run docker natively so if you are installing dog for the first time don't worry about most of these parts like doctor toolbox and doctor machine there are two things that are important one is to double check that your Windows version is compatible for docker and the second one is to have virtualization enabled virtualization is by default always enabled other than you manually disabled it so if you're unsure then you can check it by going to the task manager performance CPU tab and here you can see the status of the virtualization so once you have checked that and made sure this that these two prerequisites are met then actually you can scroll up and download the windows installer for from the stable channel once the Installer is downloaded you can just click on it and follow the installation wizard to install docker for Windows once the installation is completed you have to explicitly start docker because it's not going to start automatically so for that you can just go and search for the docker for Windows app on your Windows and just click on it and you will see the darker whale icon starting and if you click on that icon you can actually see the status that says Stoker is now up and running so this is basically it for the installation now let's see how to install Tucker on different Linux distributions and this is where things get a little bit more complicated so first of all you see that in this menu on the on the left you see that for four different Linux distributions the installation steps will differ but also for example if we just click on one two for the guide you can see that in the prerequisites section there is also differentiation between the versions of the same Linux distribution and there may be some even more complicated scenarios where the combination of the version of the distribution and the architecture it's running in also makes some difference in to how to setup docker on that specific environment because of that I can't go through a docker installation process of every Linux environment because they're just too many combinations so instead what we'll do is just go through a general overview of the steps and configuration process to get darker running on your Linux environment and you can just adjust it then for your specific setup so these are some general steps to follow in order to install docker on your Linux Linux environment first of all you have to go through the operating system requirements part on your on the relevant Linux distribution that applies for you a second step in the documentation to is to any sell old versions however if it's the first time you installing darker then you know I have to worry about that you also don't have to worry about the support its storage drivers and you can skip ahead to the part of installing darker Community Edition so for any Linux distribution here the steps will be or the options for installing docker will be the same so first option is basically to set up a repository and download the docker from install it from the dock repository the second option is to install the packages manually however I wouldn't recommend it and I think the documentation doesn't recommend it either because then you will have to do a lot of steps of the installation in the maintenance of the versions manually so I wouldn't do that the third one is just with the testing purposes it may be enough for the development purposes as well but I would still not do it which is basically just download downloading some automated scripts that will install and setup docker on your Linux environment however again I wouldn't go with it I would actually just do the first option which is just downloading the darker from the repository so in order to install darker using the first option which is downloading it from the Dockers repositories you have two main steps so the first one is to set up the repository which differs a little bit depending on which distribution you have and then install the docker seee from the repository so from Ubuntu and Debian the steps for setting up the repository are generally just updating your package and then setting up an HTTP connection with the repository and adding the Dockers official jpg key which only Ubuntu and Debian need you don't have to do this steps for CentOS and fedora there just have to install the required packages in the last step for setting up the repository is basically setting up the stable repository of docker which we saw previously on the overview that there are two channels which is a stable and edge here you always have to set up the stable repository optionally you can also set up the edge repository but I'll just do stable this time and here also something to notice depending on architecture you have to actually set it or you have to set that as a parameter when you setup the repository so if you have for example a different architecture you can use those steps to display the correct command for it and I guess that applies to other Linux distributions as well like for example here you also have the second tab where you see a separate comment for it so this steps should actually set up the repository so that as a next step you can then install the docker C from those repositories so installing docker from the setup repository is actually pretty straightforward the steps are same for or similar to all the distributions basically just update the app package and then you just say install docker seee so this command will just download the latest stock version if you want to install a specific one which you will need to do in a production environment then you can just provide a version like this you just say docker - EE equals some specific versions and using this command you can actually look up what versions are available in that repository that you just and with this command actually docker will be installed on your Linux environment and then you can just verify using sudo docker run hello world which is this demo image of docker you can verify that docker is running and this will start hello world docker container on your environment so as I mentioned previously for environments that do not support running docker natively there is an workaround which is called docker toolbox so darker Tool Works is basically an installer for darker environment set up on those systems so this is how to install darker toolbox on your Mac this is the whole package that comes with the installation of tako toolbox which is basically the darker command-line doctor machine docker compose basically all the packages that we saw in the native installation and in on top of that you also get the Oracle VM VirtualBox so in order to install the darker toolbox it's actually pretty straightforward on your on this website you can go to the toolbox releases we have all the list of latest releases you just take the latest release and here you see two assets this one is for Windows obviously and you just download the package for Mac and once it's downloaded you just click on it and go through the installation wizard leave all the options by defaults as they are do not change anything and after the installation we can just validate the the installation is successful and you can actually run docker so after seeing the installation was successful screen just go and look up in your Launchpad talk a QuickStart terminal and once you open it you should be able to run docker commenced and you can just try docker run hello world which should just start up or bring up this hello world docker container on your environment so now let's see how to install dr. toolbox on windows here is you that you get the whole package of docker technologies with the toolbox which are basically the same package which you get on the native docker installation and on top of that you get Oracle VM VirtualBox which is the tool that enables docker to run on an older system so before you install dr toolbox you have to make sure that you meet some of the preconditions number one you have to make sure your windows system supports virtualization and that virtualization must be enabled otherwise docker docker won't start so depending on which we version you have looking up or checking the virtualization status will be different so I just suggest you google it and look it up of how to find the virtualization status to see that it's enabled once you have that checked also make sure that your windows operating system is 64 bits so if those two criteria are met then you can go ahead and install the docker toolbox the place where you see the releases or the release artifacts is toolbox releases link here which I have opened so it's basically a list of the releases you just take the latest one which has two artifacts this is the one for Windows you just download this executable file click on it and go through the installation wizard once the installation is completed there just couple of steps here you can verify that docker was installed or the toolbox was installed by just looking up the docker a QuickStart terminal on your windows that it must be installed and once you click on it and open it you should be able to run docker commands in the terminal so the basic docker command that you can test will be docker run hello world which will just fetch this basic docker container from the public registry and run it on your computer if that command is successful it means that docker was successfully installed on your computer and now you can proceed with the tutorial thanks for watching the video I hope it was helpful and if he was don't forget to like it this is a video series so I will create a new one every week so if you want to be notified whenever a new video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in the comment section below and I will try to answer them so thank you and see you in the next video
GeqaTjKMWeY,2019-10-11T13:02:57Z,What is a Docker Container? Docker Demo || Docker Tutorial 2,[Music] now that you know what a container concept is let's look at what a container is technically so technically container is made up of images so we have layers of stacked images on top of each other and at the base of most of the containers you would have a linux-based image which is either Alpine with a specific version or it could be some other Linux distribution and it's important for those base images to be small that's why most of them are actually Alpine because that will make sure that the containers stay small in size which is one of the advantages of using container so on top of the base image you would have application image and this is a simplified diagram usually you would have this intermediate images that will lead up to the actual application application image that is going to run in the container and of course on top of that you'll have all this configuration data so now I think it's time to dive into a practical example of how you can actually use a docker container and how it looks like when you install it and download it and run it on your local machine so to give you a bit of an idea of how this works let's head over to docker hub and search for PostgreSQL so here which is a docker official image I can see some of the versions and let's say I'm looking specifically for older version I don't know 96 something so I'm going to pull that one so this is a docker repository so that I can actually go ahead and pull the image pull the containers from the repository directly and because it's a public repository I don't have to login to it I don't have to provide any authentication credentials or anything I can just get started with a simple docker command without doing or configuring anything 2x stalker hub so on my terminal I can just do docker pull I can even do docker run and then just copy the the image name and if I don't specify any version it will just give me the latest but I want a specific version so I'm just I'm gonna go with 9.6 actually just to demonstrate so I can provide the version like this with a column and I can start run so as you see the first line says unable to find image locally so it knows that it has to go to docker hub and pull it from there and the next line says pulling from library Posterous and here you see a lot of hashes that says downloading and the this is what I mentioned earlier which is docker containers or any containers are made up of layers right you have the linux image layer you have the application layers and so on so what what you see here are actually all those layers that are separately downloading from the docker hub on my machine right and each one is the the advantage of splitting those applications and layers is that actually for example if the image changes or I have to download a newer version of Postgres what happens is that the layers they're the same between those two applications two versions of Posterous will not be downloaded again but only those layers that are different so for example now it's gonna take around 10 or 15 minutes to download this one image because I don't have any Postgres locally but if I were to download the next version I will it will take a little bit less time because some layers already exist on my local machine so now you see that it's already logging because if this command that I ran here the docker run we the container name inversion it fetches or it pulls the the container but it also starts it so it executes the start script right away as soon as it downloads it and here you see the output of the starting of the application so it just gives some output about starting the server and doing some configuration stuff and here you see database system is ready to accept connections and won't you start it so now let's open the new tab and see with dr. PS command you can actually see all the running containers so here you see that Postgres 9:6 is running and it actually says image so this is another important thing to understand when we're talking about containers there are two def two technical terms image and a container and a lot of people confuse those two I think and there is actually a very easy distinction between the two image is the actual packaging so actually saw one of the previous slides so image is the actual package that we saw in one of those previous slides so the application package together with the configuration and the dependencies and all these things this is actually the artifact that is movable around is actually the image container is when I pull that image on my local machine and I actually started so the application inside actually starts that creates the container environment so if it's not running basically it's an image it's just an artifact that's lying around if I start it and actually run it on my machine it is a container so that is the distinction so here it says the active running containers with a container ID image that it's running from and some entry comments that it executed and some other status information so this means that PostgreSQL is now running on my local machine simple as that if I were an out to need let's say another version of Posterous to run at the same time on my local machine I could just go ahead and say let's go back and let's say I want to have nine point six and ten point ten running at the same time on my local machine I would just to run Postgres and run again it doesn't find it locally so it pushes and this is what I actually explained to you earlier because it's the same application but with just a different version some of the layers of the image are the same so I don't have to fetch those again because they are already on my machine and it just fetches the layers that are different so that saves a little bit of time and I think it's it could be actually a good advantage so now we'll wait for other image image layers to load so that we have the second Postgres version running and now you see I have phosphorous 9.6 running in this command line tab and I have Postgres version 10 point and running in the next one so I have two post-crisis with different versions running and I can actually output them here if both of them running and there's no conflict between those two like I can actually run any number of applications with different versions maybe of the same application with no problem at all and we're going to go through the how to use those containers in your application and the port configuration in some of the other configuration stuff later in this tutorial when we do a deep dive but this is just for you to get the first visual image of how docker containers actually work how they look like and how easily you can actually start them on your local machine without having to implement a specific version of post-chorus application and do all the configuration yourself thanks for watching the video I hope it was helpful and if it was don't forget to like it this is a video series so I will create a new one every week so if you want to be notified whenever a new video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in a comment section below and I will try to answer them so thank you and see you in the next video
jPdIRX6q4jA,2019-10-11T13:01:36Z,What is Docker? Docker container concept explained || Docker Tutorial 1,[Music] hello and welcome to kubernetes tutorial kubernetes is an open source technology developed by google in order to make orchestrating containerized applications easier that's why an official definition of kubernetes is actually a container orchestration tool so as you can see containers are the base part of kubernetes technology so if you want to learn kubernetes you first have to understand what a container is and why we use them so in this video series we'll talk about container concepts and one of its most popular implementations which is Ducker so we'll talk about what a container is and what problems it solves we will also look at a container repository which is basically a storage for containers we'll see how container can actually make the development process much easier and more efficient and also how they solve some of the problems that we have in the deployment process of applications so let's dive right into it what a container is a container is a way to package applications with everything they need inside of that package including the dependencies and all the configuration necessary and that package is portable just like any other artifact is and that package can be easily shared and moved around between a development team or development and operations team and that portability of containers Plus everything packaged in one isolated environment gives it some of the advantages that makes development and deployment process more efficient and we'll see some of the examples of how that works in later slides so as I mentioned containers are portable so there must be some kind of a storage for those containers so that you can share them and move them around so containers leave in a container repository this is a specific this is a special type of storage for containers many companies have their own private repositories where they host or the way they store all the containers and this will look something like this where you you can push all of the containers that you have but there is also a public repository for docker containers where you can browse and probably find any application container that you want so let's head over to the browser and see how that looks like so if I here search for a docker hub which is the name of the public repository for Tucker I will see this official website so here if you scroll down you see that there are more than hundred thousand container images of different applications hosted or stored in this docker repository so here you see just some of the examples and for every application there is this official docker container container image but if you are looking for something else you can search it here and I see there's an official image for let's say Jenkins but there's also a lot of non official images or container images that developers or or even from Jenkins itself they actually store it here so public repository is where you usually get started when you're using or when you are starting to use the containers where you can find any application image so now let's see how containers improve the development process by specific examples how did we develop applications before the containers usually when you have a team of developers working on some application you would have to install most of the services on your operating system directly right for example you are developing developing some JavaScript application and you need a post Chris Kuehl and you need Redis for messaging and every developer in the team would then have to go and install the binaries of those services and configure them and run them on their local development environment and depending on which operating system they're using the installation process will look actually different also another thing with installing services like this is that you have multiple steps of installation so you have couple of commands that you have to execute and the chances of something going wrong and error happening is actually pretty high because because of the number of steps required to install each service and this and this approach or this process of setting up a new environment can actually be pretty tedious depending on how complex your application is for example if you have 10 services that your application is using and you would have to do that 10 times on each operating system environment so now let's see how containers solve some of these problems with containers you actually do not have to install any of the services directly on your operating system because the container is its own isolated operating system layer with Linux base image as we saw in the previous slides you have everything packaged in one isolated environment so you have the PostgreSQL with a specific version packaged with the configuration in the start script inside inside of one container so the developer you have to go and look for the binaries to download on your machine but rather you just go ahead and check out the container repository to find that specific container and download on your local machine and the Downloads step is just one docker command which fetches the container and starts it at the same time and regardless of which operating system you're on the command the docker command for starting the container will not be different it will be exactly the same so we have ten applications that your app that your JavaScript application you depends on you would just have to run $10 comments for each container and that will be it which makes the setting up your local development environment actually much easier and much more efficient than the previous version also as we saw in the demonstration before you can actually have different versions of the same application running on your local environment without having any conflict so now let's see how containers can improve the deployment process before the containers a traditional deployment process will look like this developer development team will produce artifacts together with set of instructions of how to actually install and configure those artifacts on the server so you would have a jar file or something similar for your application and in addition you would have some kind of a database service or some other service also with a set of instructions of how to configure and set it up on the server so development team would give those artifacts over to the operations team and the operation team will handle setting up the environments to deploy those applications now the problem with this kind of approach is that first of all you need to configure everything and install everything directly on the operating system which we saw in the previous example that could actually lead to conflicts with dependency versions and and the service is running on the same host another problem that could arise from this kind of process is when there is misunderstanding between the development team and operations because everything is in a textual guide there could be cases where developers miss to mention some important point about configuration and when that fails the operations team have to go back to the developers and ask for more details and this could lead to some back-and-forth communication and the application is successfully deployed on the server with containers this process is actually simplified because now you have the developers and operations working in one team to package the whole configuration dependencies inside the application just as we saw previously and since it's already encapsulated in one environment you don't have to configure any of this directly on the server so the only thing you need to do is run a docker command that pulls that container that you've stored somewhere in the repository and then runs it and that makes the process this is of course a simplify simplified version but then makes exactly the problem that we saw on the previous slide much more easier no environmental configuration needed on the server the only thing of course you need to do is you have to prepare you have to install the and set up the docker runtime on the server before you will be able to run containers there but that's just one time effort and later in this tutorial we will also see how kubernetes actually offers even more abstraction of the deployment environment to make the deploying of super complex applications much much easier than it was possible with the traditional approach thanks for watching the video I hope it was helpful and if it was don't forget to like it this is a video series so I will create a new one every week so if you want to be notified whenever a new video comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in a comment section below and I will try to answer them so thank you and see you in the next video
